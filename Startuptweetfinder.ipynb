{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mZamn1u-BYI",
        "outputId": "1a922d60-cbb7-4913-cec4-7038218f1cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tweety-ns (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# !pip install -q git+https://github.com/mahrtayyab/tweety.git@650981ce9605e6e06f2c5c1fe2db5c01a5270d2e\n",
        "# !pip install -q git+https://github.com/mahrtayyab/tweety.git@c1c060b1b9185ad49abf05b9a0f5eb65b42e5b05\n",
        "!pip install -q git+https://github.com/mahrtayyab/tweety.git@2b0167dc7760c18243c8df4e0f98ccddac96ad38\n",
        "!pip install -q tweepy python-dotenv\n",
        "!pip install -q openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2nb5n6e-D_x",
        "outputId": "89743cf7-9818-4eb8-872a-82f7c63e05e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing .env\n"
          ]
        }
      ],
      "source": [
        "%%writefile .env\n",
        "# OAuth 1.0a Credentials (For Media Upload)\n",
        "API_KEY=\"kbxotZuuqBO8jfm042kiCauyW\"\n",
        "API_SECRET=\"f1dvQQgjhZy8LVbdlD2LiHBBj5J72CcBnXzbqIbCk3pr0lk5DE\"\n",
        "ACCESS_TOKEN=\"1793793907735605248-Xj2yJUzDu5mBBCOegkYzGi6rUk4HZO\"\n",
        "ACCESS_TOKEN_SECRET=\"db1VyIHp4jkqiQ4hTF0Z5cs05Kzf0rZ2mqVaFavYZfO5L\"\n",
        "\n",
        "# OAuth 2.0 Credentials (Keep these separate)\n",
        "TWITTER_CLIENT_ID=\"bEJaQmpSYnhCaDVpUjRQRWRicWE6MTpjaQ\"\n",
        "TWITTER_CLIENT_SECRET=\"7Z3NxOwSKZtYGE_VDnv4G27JesM9aLQKxo7m1ME8nSDDg7InPl\"\n",
        "TWITTER_BEARER_TOKEN=\"AAAAAAAAAAAAAAAAAAAAAOE0wgEAAAAAZj49Kap00mDtalirOUwCSrFUpKQ%3DhWMUv67TBZob2ZN6906wfGh6K59ZfYVrta6D7r4yyea9DaHfsO\"\n",
        "\n",
        "# Deep Infra API\n",
        "DEEP_INFA_API=\"xHXsWME6F8dhMMVbadsbAyeBOUGNZLzY\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from tweety import TwitterAsync\n",
        "from tweety.types import Proxy\n",
        "from tweety.constants import HTTP\n",
        "from tweety.filters import SearchFilters\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import os\n",
        "import traceback\n",
        "from typing import Dict, List, Optional\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class StartupFundingTracker:\n",
        "    def __init__(self):\n",
        "        self.authenticated_clients = {}\n",
        "\n",
        "        # Keywords for searching\n",
        "        self.search_query = 'Series A startup (funding OR raised OR investment)'\n",
        "\n",
        "        # Regular expressions for extracting information\n",
        "        self.money_pattern = r'\\$\\d+(?:\\.\\d+)?(?:M|m|million|Million|MILLION|\\d*K|k|thousand|Thousand|THOUSAND)?'\n",
        "        self.location_pattern = r'(?:based in|from|headquartered in)\\s+([A-Za-z\\s,]+)'\n",
        "\n",
        "        # Major cities and countries for location matching\n",
        "        self.major_cities = {\n",
        "            'new york', 'london', 'paris', 'tokyo', 'singapore', 'dubai', 'hong kong', 'san francisco',\n",
        "            'los angeles', 'chicago', 'boston', 'seattle', 'austin', 'miami', 'toronto', 'vancouver',\n",
        "            'sydney', 'melbourne', 'berlin', 'munich', 'amsterdam', 'tel aviv', 'bangalore', 'mumbai',\n",
        "            'delhi', 'shanghai', 'beijing', 'seoul', 'stockholm', 'oslo', 'copenhagen', 'dublin',\n",
        "            'edinburgh', 'manchester', 'zurich', 'geneva', 'vienna', 'brussels', 'madrid', 'barcelona',\n",
        "            'lisbon', 'milan', 'rome', 'athens', 'warsaw', 'prague', 'budapest', 'helsinki',\n",
        "            'bangkok', 'jakarta', 'kuala lumpur', 'manila', 'mexico city', 'sÃ£o paulo', 'rio de janeiro',\n",
        "            'buenos aires', 'santiago', 'bogota', 'lima', 'johannesburg', 'cape town', 'nairobi',\n",
        "            'dubai', 'abu dhabi', 'doha', 'riyadh', 'istanbul', 'moscow', 'st petersburg',\n",
        "            'denver', 'portland', 'las vegas', 'atlanta', 'dallas', 'houston', 'phoenix',\n",
        "            'philadelphia', 'san diego', 'san jose', 'minneapolis', 'detroit', 'montreal',\n",
        "            'bangalore', 'hyderabad', 'pune', 'chennai', 'kolkata'\n",
        "        }\n",
        "\n",
        "        self.countries = {\n",
        "            'united states', 'usa', 'uk', 'united kingdom', 'france', 'germany', 'italy', 'spain',\n",
        "            'portugal', 'ireland', 'netherlands', 'belgium', 'switzerland', 'austria', 'sweden',\n",
        "            'norway', 'denmark', 'finland', 'russia', 'china', 'japan', 'south korea', 'india',\n",
        "            'australia', 'new zealand', 'canada', 'mexico', 'brazil', 'argentina', 'chile',\n",
        "            'colombia', 'peru', 'south africa', 'kenya', 'nigeria', 'egypt', 'israel', 'turkey',\n",
        "            'saudi arabia', 'uae', 'united arab emirates', 'qatar', 'singapore', 'malaysia',\n",
        "            'indonesia', 'thailand', 'vietnam', 'philippines', 'pakistan', 'bangladesh',\n",
        "            'poland', 'czech republic', 'hungary', 'romania', 'greece', 'ukraine', 'estonia',\n",
        "            'latvia', 'lithuania', 'slovenia', 'croatia', 'serbia', 'bulgaria', 'morocco',\n",
        "            'tunisia', 'ghana', 'ethiopia', 'uganda', 'tanzania', 'zimbabwe', 'botswana',\n",
        "            'iran', 'iraq', 'jordan', 'lebanon', 'kuwait', 'bahrain', 'oman'\n",
        "        }\n",
        "\n",
        "        # Common location prefixes and suffixes\n",
        "        self.location_markers = {\n",
        "            'based in', 'from', 'headquartered in', 'located in', 'operates from',\n",
        "            'founded in', '-based', 'offices in', 'hq in'\n",
        "        }\n",
        "\n",
        "        # Funding rounds dictionary\n",
        "        self.funding_rounds = {\n",
        "            'pre_seed': [\n",
        "                'pre-seed', 'preseed', 'pre seed',\n",
        "                'angel round', 'angel investment',\n",
        "                'friends and family', 'friends & family'\n",
        "            ],\n",
        "            'seed': [\n",
        "                'seed round', 'seed funding', 'seed investment',\n",
        "                'seed capital', 'seed financing', 'seed stage'\n",
        "            ],\n",
        "            'series_a': [\n",
        "                'series a', 'series-a', 'seriesa',\n",
        "                'series a round', 'series a funding'\n",
        "            ],\n",
        "            'series_b': [\n",
        "                'series b', 'series-b', 'seriesb',\n",
        "                'series b round', 'series b funding'\n",
        "            ],\n",
        "            'series_c': [\n",
        "                'series c', 'series-c', 'seriesc',\n",
        "                'series c round', 'series c funding'\n",
        "            ],\n",
        "            'series_d': [\n",
        "                'series d', 'series-d', 'seriesd',\n",
        "                'series d round', 'series d funding'\n",
        "            ],\n",
        "            'series_e': [\n",
        "                'series e', 'series-e', 'seriese',\n",
        "                'series e round', 'series e funding'\n",
        "            ],\n",
        "            'series_f': [\n",
        "                'series f', 'series-f', 'seriesf',\n",
        "                'series f round', 'series f funding'\n",
        "            ],\n",
        "            'series_g': [\n",
        "                'series g', 'series-g', 'seriesg',\n",
        "                'series g round', 'series g funding'\n",
        "            ],\n",
        "            'bridge': [\n",
        "                'bridge round', 'bridge funding',\n",
        "                'bridge financing', 'bridge investment'\n",
        "            ],\n",
        "            'convertible': [\n",
        "                'convertible note', 'convertible debt',\n",
        "                'convertible round', 'convertible financing'\n",
        "            ],\n",
        "            'debt': [\n",
        "                'debt financing', 'debt round',\n",
        "                'venture debt', 'debt funding'\n",
        "            ],\n",
        "            'growth': [\n",
        "                'growth round', 'growth equity',\n",
        "                'growth capital', 'growth financing',\n",
        "                'growth funding', 'late stage'\n",
        "            ],\n",
        "            'ipo': [\n",
        "                'ipo', 'initial public offering',\n",
        "                'public offering', 'going public'\n",
        "            ],\n",
        "            'spac': [\n",
        "                'spac', 'special purpose acquisition company',\n",
        "                'spac merger', 'blank check company'\n",
        "            ],\n",
        "            'private_equity': [\n",
        "                'private equity', 'pe round',\n",
        "                'pe investment', 'pe funding'\n",
        "            ],\n",
        "            'extension': [\n",
        "                'extension round', 'extension funding',\n",
        "                'extension financing', 'round extension'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Human-readable round names for output\n",
        "        self.round_names = {\n",
        "            'pre_seed': 'Pre-Seed',\n",
        "            'seed': 'Seed',\n",
        "            'series_a': 'Series A',\n",
        "            'series_b': 'Series B',\n",
        "            'series_c': 'Series C',\n",
        "            'series_d': 'Series D',\n",
        "            'series_e': 'Series E',\n",
        "            'series_f': 'Series F',\n",
        "            'series_g': 'Series G',\n",
        "            'bridge': 'Bridge',\n",
        "            'convertible': 'Convertible',\n",
        "            'debt': 'Debt',\n",
        "            'growth': 'Growth',\n",
        "            'ipo': 'IPO',\n",
        "            'spac': 'SPAC',\n",
        "            'private_equity': 'Private Equity',\n",
        "            'extension': 'Extension'\n",
        "        }\n",
        "\n",
        "    async def authenticate_accounts(self):\n",
        "        \"\"\"Authenticate all accounts from the CSV.\"\"\"\n",
        "        try:\n",
        "            # Load accounts from CSV\n",
        "            accounts_df = pd.read_csv('tweetfinderfin.csv')\n",
        "            print(\"\\nðŸ”„ Starting authentication process...\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            for _, account in accounts_df.iterrows():\n",
        "                # Skip invalid entries\n",
        "                if (pd.isna(account['username']) or\n",
        "                    pd.isna(account['tweety_or_tweepy']) or\n",
        "                    account['username'].lower() == 'lanadaisyxo'):\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\nProcessing account: {account['username']}\")\n",
        "\n",
        "                # Only handle Tweety authentication\n",
        "                if str(account['tweety_or_tweepy']).lower() == 'tweety':\n",
        "                    try:\n",
        "                        # Create session file\n",
        "                        session_name = f\"session_{account['username']}\"\n",
        "                        session_data = {\n",
        "                            \"cookies\": {\n",
        "                                \"auth_token\": account['auth_token'] if pd.notna(account['auth_token']) else \"\",\n",
        "                                \"ct0\": account['ct0'] if pd.notna(account['ct0']) else \"\"\n",
        "                            }\n",
        "                        }\n",
        "\n",
        "                        # Write session file\n",
        "                        with open(f\"{session_name}.tw_session\", \"w\") as f:\n",
        "                            json.dump(session_data, f)\n",
        "\n",
        "                        # Configure proxy if available\n",
        "                        proxy = None\n",
        "                        if pd.notna(account['host']) and pd.notna(account['port']):\n",
        "                            proxy = Proxy(\n",
        "                                host=str(account['host']),\n",
        "                                port=int(account['port']),\n",
        "                                proxy_type=HTTP,\n",
        "                                username=str(account['proxy_username']) if pd.notna(account['proxy_username']) else None,\n",
        "                                password=str(account['proxy_password']) if pd.notna(account['proxy_password']) else None\n",
        "                            )\n",
        "\n",
        "                        # Initialize Twitter client\n",
        "                        client = TwitterAsync(session_name, proxy=proxy)\n",
        "\n",
        "                        # Load authentication token if available\n",
        "                        if pd.notna(account['auth_token']):\n",
        "                            await client.load_auth_token(account['auth_token'])\n",
        "\n",
        "                        # Verify authentication\n",
        "                        try:\n",
        "                            # Test authentication by getting user info\n",
        "                            await client.get_user_info(account['username'])\n",
        "                            self.authenticated_clients[account['username']] = client\n",
        "                            print(f\"âœ… Authentication successful and verified for {account['username']}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"âŒ Authentication verification failed for {account['username']}: {str(e)}\")\n",
        "                            continue\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"âŒ Authentication failed for {account['username']}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "            print(f\"\\nâœ… Total accounts authenticated: {len(self.authenticated_clients)}\")\n",
        "            return self.authenticated_clients\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error during authentication: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def extract_funding_amount(self, text: str) -> Optional[float]:\n",
        "        \"\"\"Extract funding amount from text and convert to millions.\"\"\"\n",
        "        matches = re.findall(self.money_pattern, text)\n",
        "        if not matches:\n",
        "            return None\n",
        "\n",
        "        amount_str = matches[0].replace('$', '').lower()\n",
        "\n",
        "        try:\n",
        "            if 'm' in amount_str or 'million' in amount_str:\n",
        "                return float(re.findall(r'\\d+(?:\\.\\d+)?', amount_str)[0])\n",
        "            elif 'k' in amount_str or 'thousand' in amount_str:\n",
        "                return float(re.findall(r'\\d+(?:\\.\\d+)?', amount_str)[0]) / 1000\n",
        "            else:\n",
        "                # Assume raw number is in millions\n",
        "                return float(re.findall(r'\\d+(?:\\.\\d+)?', amount_str)[0])\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def extract_location(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Extract location from text using comprehensive city and country lists.\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # First try to find location with common markers\n",
        "        for marker in self.location_markers:\n",
        "            if marker in text_lower:\n",
        "                # Get the text after the marker\n",
        "                marker_index = text_lower.index(marker) + len(marker)\n",
        "                remaining_text = text_lower[marker_index:].strip()\n",
        "\n",
        "                # Look for cities or countries in the next few words\n",
        "                words = remaining_text.split()\n",
        "                for i in range(len(words)):\n",
        "                    for j in range(i + 1, min(i + 4, len(words) + 1)):\n",
        "                        location_candidate = ' '.join(words[i:j])\n",
        "                        if location_candidate in self.major_cities:\n",
        "                            # Find the actual text in original case\n",
        "                            original_case = text[marker_index:].strip().split()\n",
        "                            return ' '.join(original_case[i:j])\n",
        "                        if location_candidate in self.countries:\n",
        "                            original_case = text[marker_index:].strip().split()\n",
        "                            return ' '.join(original_case[i:j])\n",
        "\n",
        "        # If no location found with markers, search entire text for cities and countries\n",
        "        words = text_lower.split()\n",
        "        for i in range(len(words)):\n",
        "            for j in range(i + 1, min(i + 4, len(words) + 1)):\n",
        "                location_candidate = ' '.join(words[i:j])\n",
        "                if location_candidate in self.major_cities:\n",
        "                    original_case = text.split()\n",
        "                    return ' '.join(original_case[i:j])\n",
        "                if location_candidate in self.countries:\n",
        "                    original_case = text.split()\n",
        "                    return ' '.join(original_case[i:j])\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_company_name(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Extract company name using basic heuristics.\"\"\"\n",
        "        patterns = [\n",
        "            r'([A-Z][A-Za-z0-9]+)(?:\\s+has\\s+raised|\\s+raises|\\s+closed|\\s+announces)',\n",
        "            r'([A-Z][A-Za-z0-9]+)(?:\\s+,\\s+a\\s+startup|,\\s+the\\s+startup)',\n",
        "            r'([A-Z][A-Za-z0-9]+)(?:\\s+secures|\\s+receives|\\s+gets)',\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_series_round(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Extract the funding round from text.\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Check each funding round type\n",
        "        for round_type, variations in self.funding_rounds.items():\n",
        "            for variation in variations:\n",
        "                if variation in text_lower:\n",
        "                    return self.round_names[round_type]\n",
        "\n",
        "        # Additional check for generic series mentions (like \"Series H\" or beyond)\n",
        "        series_match = re.search(r'series\\s+([a-z])\\b', text_lower)\n",
        "        if series_match:\n",
        "            series_letter = series_match.group(1).upper()\n",
        "            return f\"Series {series_letter}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def process_tweet(self, tweet) -> Optional[Dict]:\n",
        "        \"\"\"Process a single tweet and extract relevant information.\"\"\"\n",
        "        text = tweet.text\n",
        "\n",
        "        # Extract information\n",
        "        company = self.extract_company_name(text)\n",
        "        amount = self.extract_funding_amount(text)\n",
        "        location = self.extract_location(text)\n",
        "        series = self.extract_series_round(text)\n",
        "\n",
        "        # Only return if we have at least company name and amount\n",
        "        if company and amount:\n",
        "            return {\n",
        "                'company': company,\n",
        "                'location': location,\n",
        "                'raised_millions': amount,\n",
        "                'series': series,\n",
        "                'date': tweet.created_on,\n",
        "                'tweet_text': text,\n",
        "                'tweet_id': tweet.id\n",
        "            }\n",
        "        return None\n",
        "\n",
        "    async def search_funding_tweets(self, client, username: str, pages: int = 150) -> List[Dict]:\n",
        "        \"\"\"Search for startup funding tweets using a specific client.\"\"\"\n",
        "        funding_data = []\n",
        "\n",
        "        try:\n",
        "            print(f\"\\nSearching tweets using account: {username}\")\n",
        "            print(f\"Search query: {self.search_query}\")\n",
        "\n",
        "            # Use search method with await\n",
        "            search_results = await client.search(\n",
        "                self.search_query,\n",
        "                pages=pages,\n",
        "                filter_=SearchFilters.Latest()  # Using Latest() filter for recent tweets\n",
        "            )\n",
        "\n",
        "            # Process tweets from search results\n",
        "            for tweet in search_results:\n",
        "                try:\n",
        "                    processed = self.process_tweet(tweet)\n",
        "                    if processed:\n",
        "                        funding_data.append(processed)\n",
        "                        print(f\"\\nFound funding tweet: {processed['company']} raised ${processed['raised_millions']}M\")\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            return funding_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during search with {username}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def save_to_csv(self, data: List[Dict], filename: str = 'startup_funding_data.csv'):\n",
        "        \"\"\"Save the extracted data to a CSV file and download it.\"\"\"\n",
        "        if data:\n",
        "            df = pd.DataFrame(data)\n",
        "            df.to_csv(filename, index=False)\n",
        "            print(f\"Data saved to {filename}\")\n",
        "\n",
        "            # Download in Colab\n",
        "            try:\n",
        "                from google.colab import files\n",
        "                files.download(filename)\n",
        "                print(\"File download initiated...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading file: {str(e)}\")\n",
        "        else:\n",
        "            print(\"No data to save\")\n",
        "\n",
        "async def main():\n",
        "    try:\n",
        "        # Initialize tracker and authenticate\n",
        "        tracker = StartupFundingTracker()\n",
        "        await tracker.authenticate_accounts()\n",
        "\n",
        "        all_funding_data = []\n",
        "\n",
        "        # Search using each authenticated client\n",
        "        for username, client in tracker.authenticated_clients.items():\n",
        "            funding_data = await tracker.search_funding_tweets(client, username)\n",
        "            all_funding_data.extend(funding_data)\n",
        "\n",
        "        # Remove duplicates based on tweet_id\n",
        "        unique_funding_data = list({d['tweet_id']: d for d in all_funding_data}.values())\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\nFound {len(unique_funding_data)} unique funding announcements\")\n",
        "        for data in unique_funding_data:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(f\"Company: {data['company']}\")\n",
        "            print(f\"Location: {data['location']}\")\n",
        "            print(f\"Raised: ${data['raised_millions']}M\")\n",
        "            print(f\"Series: {data['series']}\")\n",
        "            print(f\"Date: {data['date']}\")\n",
        "            print(f\"Tweet: {data['tweet_text']}\")\n",
        "\n",
        "        # Save to CSV and download\n",
        "        tracker.save_to_csv(unique_funding_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {str(e)}\")\n",
        "        print(traceback.format_exc())\n"
      ],
      "metadata": {
        "id": "9ffgujdH1h8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSd0MzkyuyoY",
        "outputId": "4cd78e12-99b5-4a32-a137-bc31e90f0cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”„ Starting authentication process...\n",
            "==================================================\n",
            "\n",
            "Processing account: lanaforyou1\n",
            "âœ… Authentication successful and verified for lanaforyou1\n",
            "\n",
            "âœ… Total accounts authenticated: 1\n",
            "\n",
            "Searching tweets using account: lanaforyou1\n",
            "Search query: Series A startup (funding OR raised OR investment)\n",
            "Error during search with lanaforyou1: [88] Rate limit exceeded\n",
            "\n",
            "\n",
            "Found 0 unique funding announcements\n",
            "No data to save\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6DFdJqUBvwD",
        "outputId": "0c74227b-029b-4c1d-8130-c8f946102d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from typing import Dict, List, Optional\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "class ProxyCurlEnricher:\n",
        "    def __init__(self, api_key: str):\n",
        "        \"\"\"Initialize the ProxyCurl enrichment client.\"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.headers = {'Authorization': f'Bearer {api_key}'}\n",
        "        self.base_url = 'https://nubela.co/proxycurl/api/v2'\n",
        "        self.requests_per_minute = 10\n",
        "        self.last_request_time = 0\n",
        "\n",
        "    def _rate_limit(self):\n",
        "        \"\"\"Implement rate limiting to avoid API throttling.\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last_request = current_time - self.last_request_time\n",
        "        if time_since_last_request < (60 / self.requests_per_minute):\n",
        "            sleep_time = (60 / self.requests_per_minute) - time_since_last_request\n",
        "            time.sleep(sleep_time)\n",
        "        self.last_request_time = time.time()\n",
        "\n",
        "    def get_company_profile(self, linkedin_url: str) -> Optional[Dict]:\n",
        "        \"\"\"Get company profile using the direct company endpoint.\"\"\"\n",
        "        self._rate_limit()\n",
        "\n",
        "        try:\n",
        "            endpoint = f\"{self.base_url}/linkedin/company\"\n",
        "            params = {\n",
        "                'url': linkedin_url,\n",
        "                'extra': 'include',  # Get additional company details\n",
        "                'funding_data': 'include'  # Include funding information\n",
        "            }\n",
        "\n",
        "            response = requests.get(\n",
        "                endpoint,\n",
        "                params=params,\n",
        "                headers=self.headers\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                print(f\"Error getting company profile: {response.status_code}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception getting company profile: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_person_details(self, linkedin_url: str) -> Optional[Dict]:\n",
        "        \"\"\"Get detailed information about a person.\"\"\"\n",
        "        self._rate_limit()\n",
        "\n",
        "        try:\n",
        "            endpoint = f\"{self.base_url}/linkedin/profile\"\n",
        "            params = {'url': linkedin_url}\n",
        "\n",
        "            response = requests.get(\n",
        "                endpoint,\n",
        "                params=params,\n",
        "                headers=self.headers\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                print(f\"Error getting person details: {response.status_code}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception getting person details: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def identify_key_people(self, employees: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Identify founders and key decision-makers from employee list.\"\"\"\n",
        "        key_titles = {\n",
        "            'founder': 100,\n",
        "            'ceo': 90,\n",
        "            'chief': 80,\n",
        "            'president': 70,\n",
        "            'owner': 60,\n",
        "            'managing director': 50,\n",
        "            'partner': 40,\n",
        "            'director': 30,\n",
        "            'head': 20,\n",
        "            'vp': 10,\n",
        "            'vice president': 10\n",
        "        }\n",
        "\n",
        "        scored_people = []\n",
        "        for employee in employees:\n",
        "            title = employee.get('title', '').lower()\n",
        "            score = 0\n",
        "            for role, priority in key_titles.items():\n",
        "                if role in title:\n",
        "                    score += priority\n",
        "\n",
        "            if score > 0:\n",
        "                scored_people.append({\n",
        "                    'name': employee.get('name'),\n",
        "                    'title': employee.get('title'),\n",
        "                    'linkedin_url': employee.get('linkedin_url'),\n",
        "                    'score': score\n",
        "                })\n",
        "\n",
        "        scored_people.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return scored_people[:3]  # Return top 3 key people\n",
        "\n",
        "    def format_people_data(self, people_details: List[Dict]) -> Dict[str, str]:\n",
        "        \"\"\"Format multiple people's details into consolidated strings.\"\"\"\n",
        "        formatted = {\n",
        "            'key_people_names': [],\n",
        "            'key_people_titles': [],\n",
        "            'key_people_emails': [],\n",
        "            'key_people_phones': [],\n",
        "            'key_people_linkedin': []\n",
        "        }\n",
        "\n",
        "        for person in people_details:\n",
        "            if person.get('name'):\n",
        "                formatted['key_people_names'].append(person['name'])\n",
        "            if person.get('title'):\n",
        "                formatted['key_people_titles'].append(person['title'])\n",
        "            if person.get('email'):\n",
        "                formatted['key_people_emails'].append(person['email'])\n",
        "            if person.get('phone_number'):\n",
        "                formatted['key_people_phones'].append(person['phone_number'])\n",
        "            if person.get('linkedin_url'):\n",
        "                formatted['key_people_linkedin'].append(person['linkedin_url'])\n",
        "\n",
        "        return {k: '; '.join(filter(None, v)) for k, v in formatted.items()}\n",
        "\n",
        "def process_companies(input_file: str, api_key: str):\n",
        "    \"\"\"Process companies from CSV and enrich with LinkedIn data.\"\"\"\n",
        "    enricher = ProxyCurlEnricher(api_key)\n",
        "    df = pd.read_csv(input_file)\n",
        "    enriched_data = []\n",
        "\n",
        "    print(f\"\\nProcessing {len(df)} companies...\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        company_name = row['company']\n",
        "        print(f\"\\nProcessing company {index + 1}/{len(df)}: {company_name}\")\n",
        "\n",
        "        # Get the LinkedIn URL from the user\n",
        "        print(f\"\\nPlease enter the LinkedIn URL for {company_name}\")\n",
        "        print(\"Format: https://www.linkedin.com/company/company-name\")\n",
        "        print(\"Or press Enter to skip this company\")\n",
        "        linkedin_url = input(\"> \").strip()\n",
        "\n",
        "        if not linkedin_url:\n",
        "            print(f\"Skipping {company_name}\")\n",
        "            continue\n",
        "\n",
        "        # Get company profile\n",
        "        company_profile = enricher.get_company_profile(linkedin_url)\n",
        "        if not company_profile:\n",
        "            print(f\"Could not get profile for {company_name}\")\n",
        "            continue\n",
        "\n",
        "        # Get employee information\n",
        "        employees = company_profile.get('employees', [])\n",
        "        key_people = enricher.identify_key_people(employees)\n",
        "\n",
        "        # Get details for key people\n",
        "        people_details = []\n",
        "        for person in key_people:\n",
        "            if person['linkedin_url']:\n",
        "                person_details = enricher.get_person_details(person['linkedin_url'])\n",
        "                if person_details:\n",
        "                    people_details.append(person_details)\n",
        "\n",
        "        # Format people data\n",
        "        people_data = enricher.format_people_data(people_details)\n",
        "\n",
        "        # Combine all data\n",
        "        enriched_entry = {\n",
        "            # Original funding data\n",
        "            'company_name': company_name,\n",
        "            'funding_amount': row['raised_millions'],\n",
        "            'funding_series': row['series'],\n",
        "            'company_location': row['location'],\n",
        "            'funding_date': row['date'],\n",
        "\n",
        "            # Company enrichment data\n",
        "            'company_linkedin': linkedin_url,\n",
        "            'company_size': company_profile.get('company_size_on_linkedin'),\n",
        "            'company_industry': company_profile.get('industry'),\n",
        "            'company_website': company_profile.get('website'),\n",
        "            'company_description': company_profile.get('description'),\n",
        "\n",
        "            # Additional company data\n",
        "            'company_type': company_profile.get('company_type'),\n",
        "            'founded_year': company_profile.get('founded_year'),\n",
        "            'follower_count': company_profile.get('follower_count'),\n",
        "\n",
        "            # Consolidated people data\n",
        "            'key_people_names': people_data.get('key_people_names', ''),\n",
        "            'key_people_titles': people_data.get('key_people_titles', ''),\n",
        "            'key_people_emails': people_data.get('key_people_emails', ''),\n",
        "            'key_people_phones': people_data.get('key_people_phones', ''),\n",
        "            'key_people_linkedin': people_data.get('key_people_linkedin', '')\n",
        "        }\n",
        "\n",
        "        enriched_data.append(enriched_entry)\n",
        "        print(f\"Successfully processed {company_name}\")\n",
        "\n",
        "    # Save enriched data\n",
        "    if enriched_data:\n",
        "        output_df = pd.DataFrame(enriched_data)\n",
        "        output_filename = f'enriched_startup_data_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
        "        output_df.to_csv(output_filename, index=False)\n",
        "        print(f\"\\nEnriched data saved to {output_filename}\")\n",
        "\n",
        "        # Download in Colab\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(output_filename)\n",
        "            print(\"File download initiated...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading file: {str(e)}\")\n",
        "    else:\n",
        "        print(\"\\nNo enriched data to save\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    api_key = input(\"Enter your ProxyCurl API key: \")\n",
        "    process_companies('/content/startup_funding_data.csv', api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hsIuN4tyBEdS",
        "outputId": "22f44c2b-f4d8-4c2b-c772-bb1e32d552e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your ProxyCurl API key: nXGMlBEVeHuSksckp4Lgjw\n",
            "\n",
            "Processing 11 companies...\n",
            "\n",
            "Processing company 1/11: zhipu ai\n",
            "\n",
            "Please enter the LinkedIn URL for zhipu ai\n",
            "Format: https://www.linkedin.com/company/company-name\n",
            "Or press Enter to skip this company\n",
            "> zhipuai\n",
            "Error getting company profile: 404\n",
            "Could not get profile for zhipu ai\n",
            "\n",
            "Processing company 2/11: basis\n",
            "\n",
            "Please enter the LinkedIn URL for basis\n",
            "Format: https://www.linkedin.com/company/company-name\n",
            "Or press Enter to skip this company\n",
            "> https://www.linkedin.com/company/get-basis-ai/\n",
            "Error getting company profile: 404\n",
            "Could not get profile for basis\n",
            "\n",
            "Processing company 3/11: basis ai\n",
            "\n",
            "Please enter the LinkedIn URL for basis ai\n",
            "Format: https://www.linkedin.com/company/company-name\n",
            "Or press Enter to skip this company\n",
            "> https://www.linkedin.com/company/get-basis-ai/\n",
            "Error getting company profile: 404\n",
            "Could not get profile for basis ai\n",
            "\n",
            "Processing company 4/11: orbite\n",
            "\n",
            "Please enter the LinkedIn URL for orbite\n",
            "Format: https://www.linkedin.com/company/company-name\n",
            "Or press Enter to skip this company\n",
            "> https://www.linkedin.com/company/orbite-inc/\n",
            "Error getting company profile: 404\n",
            "Could not get profile for orbite\n",
            "\n",
            "Processing company 5/11: anatomy financial\n",
            "\n",
            "Please enter the LinkedIn URL for anatomy financial\n",
            "Format: https://www.linkedin.com/company/company-name\n",
            "Or press Enter to skip this company\n",
            "> https://www.linkedin.com/company/anatomyfinancial/\n",
            "Error getting company profile: 404\n",
            "Could not get profile for anatomy financial\n",
            "\n",
            "Processing company 6/11: bvnkfinance\n",
            "\n",
            "Please enter the LinkedIn URL for bvnkfinance\n",
            "Format: https://www.linkedin.com/company/company-name\n",
            "Or press Enter to skip this company\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-16247680ff73>\u001b[0m in \u001b[0;36m<cell line: 230>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your ProxyCurl API key: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mprocess_companies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/startup_funding_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-16247680ff73>\u001b[0m in \u001b[0;36mprocess_companies\u001b[0;34m(input_file, api_key)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Format: https://www.linkedin.com/company/company-name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Or press Enter to skip this company\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mlinkedin_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlinkedin_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest-asyncio proxycurl-py[asyncio]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl-_qRh9F78F",
        "outputId": "ebfb77dd-ac17-470e-feb4-2a69e7957f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: proxycurl-py[asyncio] in /usr/local/lib/python3.10/dist-packages (0.1.0.post2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from proxycurl-py[asyncio]) (3.11.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4->proxycurl-py[asyncio]) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4->proxycurl-py[asyncio]) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4->proxycurl-py[asyncio]) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4->proxycurl-py[asyncio]) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4->proxycurl-py[asyncio]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4->proxycurl-py[asyncio]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4->proxycurl-py[asyncio]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4->proxycurl-py[asyncio]) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp<4.0.0,>=3.7.4->proxycurl-py[asyncio]) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp<4.0.0,>=3.7.4->proxycurl-py[asyncio]) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Set up the authentication\n",
        "api_key = \"P8Lz2OgMswmkHs2qOtAc-w\"\n",
        "headers = {'Authorization': f'Bearer {api_key}'}\n",
        "\n",
        "# The endpoint for company employees is structured differently from the main company endpoint\n",
        "api_endpoint = 'https://nubela.co/proxycurl/api/linkedin/company/employees'\n",
        "\n",
        "# These parameters help us get exactly the data we want\n",
        "params = {\n",
        "    'url': 'https://www.linkedin.com/company/microsoft',\n",
        "    'coy_name_match': 'include',     # This helps ensure we get the right company\n",
        "    'use_cache': 'if-present',       # Use cached data when available to save credits\n",
        "    'country': 'us',                 # Focus on US-based employees\n",
        "    'enrich_profiles': 'enrich',     # Get additional profile information\n",
        "    'role_search': '(co)?-?founder', # Look for founders specifically\n",
        "    'page_size': '10',               # Limit results to 10 employees\n",
        "    'employment_status': 'current',   # Only currently employed people\n",
        "    'sort_by': 'recently-joined',    # Sort by most recent employees first\n",
        "    'resolve_numeric_id': 'false'    # Don't resolve numeric LinkedIn IDs\n",
        "}\n",
        "\n",
        "print(\"Testing API connection...\")\n",
        "print(f\"Using endpoint: {api_endpoint}\")\n",
        "print(f\"With parameters: {params}\")\n",
        "print(f\"Headers: {headers}\")\n",
        "\n",
        "# Make the request\n",
        "response = requests.get(\n",
        "    api_endpoint,\n",
        "    params=params,\n",
        "    headers=headers\n",
        ")\n",
        "\n",
        "# Print the full response information for debugging\n",
        "print(\"\\nResponse Status:\", response.status_code)\n",
        "print(\"\\nResponse Headers:\", dict(response.headers))\n",
        "print(\"\\nResponse Content Preview:\")\n",
        "print(response.text[:500])  # First 500 characters of the response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sozqiXuuGiGk",
        "outputId": "1eea5052-55f8-44a2-b86a-39329b80ef3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing API connection...\n",
            "Using endpoint: https://nubela.co/proxycurl/api/linkedin/company/employees\n",
            "With parameters: {'url': 'https://www.linkedin.com/company/microsoft', 'coy_name_match': 'include', 'use_cache': 'if-present', 'country': 'us', 'enrich_profiles': 'enrich', 'role_search': '(co)?-?founder', 'page_size': '10', 'employment_status': 'current', 'sort_by': 'recently-joined', 'resolve_numeric_id': 'false'}\n",
            "Headers: {'Authorization': 'Bearer P8Lz2OgMswmkHs2qOtAc-w'}\n",
            "\n",
            "Response Status: 403\n",
            "\n",
            "Response Headers: {'Date': 'Wed, 01 Jan 2025 04:31:56 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'access-control-max-age': '3600', 'x-proxycurl-credit-cost': '0', 'set-cookie': 'PC_USER_INFO=eyJlbWFpbCI6ICJlZGVuemFrYXJkMTJAZ21haWwuY29tIiwgInJlZmVycmFsX3Rva2VuIjogImV5SmhiR2NpT2lKSVV6VXhNaUlzSW5SNWNDSTZJa3BYVkNKOS5leUp3Y205a2RXTjBTV1FpT2lKdWRXSmxiR0V1WTI4aUxDSndjbTlrZFdOMFZYTmxja2xrSWpvaVFVaEdlSGh6TjFaSVpVVTBiMUExWTJaaVdqVklSeUo5LkZvSDh6XzRfaEc5LVZTYnVBM19KQUtqeVZRLVV4ZGpLMVVuQW1DNjlSWlllemV2Z3lQYUcyQXdCTUUteC1KSy16LWFnZUFnc1dxb0I2VXpXTU5wRDNnIn0=; Path=/; SameSite=Lax', 'vary': 'Cookie', 'access-control-allow-origin': 'https://sapiengraph.com', 'access-control-allow-methods': 'GET, POST, PUT, DELETE, OPTIONS', 'access-control-allow-headers': 'Content-Type, Authorization, X-Requested-With, CF-Access-Client-Id, CF-Access-Client-Secret', 'x-envoy-upstream-service-time': '20276', 'cf-cache-status': 'DYNAMIC', 'Report-To': '{\"endpoints\":[{\"url\":\"https:\\\\/\\\\/a.nel.cloudflare.com\\\\/report\\\\/v4?s=Jy6iIz%2FiELxCm4pE4EcgeMWHFRQ2EC4EM75THZvYUr3UfbFMDpTd4g%2FXXfrdlmrhT2A7zbrrbxdSbKQC5sKuzoU91ZY4ysJ2ZrKzTyJLGF9clfmhEKQeduKsiYQ%3D\"}],\"group\":\"cf-nel\",\"max_age\":604800}', 'NEL': '{\"success_fraction\":0,\"report_to\":\"cf-nel\",\"max_age\":604800}', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'X-Content-Type-Options': 'nosniff', 'Server': 'cloudflare', 'CF-RAY': '8fafc7db7fb6cb7d-LAX', 'Content-Encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400', 'server-timing': 'cfL4;desc=\"?proto=TCP&rtt=9162&min_rtt=8898&rtt_var=3525&sent=4&recv=6&lost=0&retrans=0&sent_bytes=2832&recv_bytes=1103&delivery_rate=325466&cwnd=169&unsent_bytes=0&cid=9ae5e5b34f5630e1&ts=20501&x=0\"'}\n",
            "\n",
            "Response Content Preview:\n",
            "{\"code\":403,\"description\":\"Insufficient credits. You need 260 credits for this request. Please top-up your credits or modify your request.\",\"name\":\"Forbidden\"}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Set up the authentication with your ProxyCurl API key\n",
        "api_key = \"P8Lz2OgMswmkHs2qOtAc-w\"\n",
        "headers = {'Authorization': f'Bearer {api_key}'}\n",
        "\n",
        "# Define the API endpoint for accessing company employee data\n",
        "# This endpoint specifically allows us to search through company employees\n",
        "api_endpoint = 'https://nubela.co/proxycurl/api/linkedin/company/employees'\n",
        "\n",
        "# Configure the search parameters for ZhipuAI\n",
        "# Each parameter helps us narrow down the exact data we want to retrieve\n",
        "params = {\n",
        "    # The LinkedIn URL for ZhipuAI's company page\n",
        "    'url': 'https://www.linkedin.com/company/zhipuai/',\n",
        "\n",
        "    # Include company name matching to ensure we're getting the right company\n",
        "    'coy_name_match': 'include',\n",
        "\n",
        "    # Use cached data if available to save on API credits\n",
        "    'use_cache': 'if-present',\n",
        "\n",
        "    # Since ZhipuAI is a Chinese company, we might want to remove the US filter\n",
        "    # to ensure we don't miss key employees\n",
        "    # 'country': 'us',  # Commenting this out to get all locations\n",
        "\n",
        "    # Get enriched profile information for each employee\n",
        "    'enrich_profiles': 'enrich',\n",
        "\n",
        "    # Search pattern to find founders and co-founders\n",
        "    # The pattern (co)?-?founder matches: founder, co-founder, cofounder\n",
        "    'role_search': '(co)?-?founder',\n",
        "\n",
        "    # Limit to 10 results to manage API credits while getting key personnel\n",
        "    'page_size': '10',\n",
        "\n",
        "    # Only show current employees\n",
        "    'employment_status': 'current',\n",
        "\n",
        "    # Sort by recently joined to get the most up-to-date employee list\n",
        "    'sort_by': 'recently-joined',\n",
        "\n",
        "    # Don't resolve numeric LinkedIn IDs to save on processing\n",
        "    'resolve_numeric_id': 'false'\n",
        "}\n",
        "\n",
        "# Print out our request details for debugging\n",
        "print(\"Testing API connection to ZhipuAI's employee data...\")\n",
        "print(f\"Using endpoint: {api_endpoint}\")\n",
        "print(f\"With parameters: {params}\")\n",
        "print(f\"Headers: {headers}\")\n",
        "\n",
        "# Make the API request\n",
        "response = requests.get(\n",
        "    api_endpoint,\n",
        "    params=params,\n",
        "    headers=headers\n",
        ")\n",
        "\n",
        "# Print out the complete response information\n",
        "print(\"\\nResponse Status:\", response.status_code)\n",
        "print(\"\\nResponse Headers:\", dict(response.headers))\n",
        "print(\"\\nResponse Content Preview:\")\n",
        "print(response.text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Hw0CmvGtYO",
        "outputId": "462a8fc1-6fa9-4a1a-c3ff-0673d6891a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing API connection to ZhipuAI's employee data...\n",
            "Using endpoint: https://nubela.co/proxycurl/api/linkedin/company/employees\n",
            "With parameters: {'url': 'https://www.linkedin.com/company/zhipuai/', 'coy_name_match': 'include', 'use_cache': 'if-present', 'enrich_profiles': 'enrich', 'role_search': '(co)?-?founder', 'page_size': '10', 'employment_status': 'current', 'sort_by': 'recently-joined', 'resolve_numeric_id': 'false'}\n",
            "Headers: {'Authorization': 'Bearer P8Lz2OgMswmkHs2qOtAc-w'}\n",
            "\n",
            "Response Status: 200\n",
            "\n",
            "Response Headers: {'Date': 'Wed, 01 Jan 2025 04:33:07 GMT', 'Content-Type': 'application/json', 'Content-Length': '36', 'Connection': 'keep-alive', 'access-control-max-age': '3600', 'x-proxycurl-credit-cost': '0', 'set-cookie': 'PC_USER_INFO=eyJlbWFpbCI6ICJlZGVuemFrYXJkMTJAZ21haWwuY29tIiwgInJlZmVycmFsX3Rva2VuIjogImV5SmhiR2NpT2lKSVV6VXhNaUlzSW5SNWNDSTZJa3BYVkNKOS5leUp3Y205a2RXTjBTV1FpT2lKdWRXSmxiR0V1WTI4aUxDSndjbTlrZFdOMFZYTmxja2xrSWpvaVFVaEdlSGh6TjFaSVpVVTBiMUExWTJaaVdqVklSeUo5LkZvSDh6XzRfaEc5LVZTYnVBM19KQUtqeVZRLVV4ZGpLMVVuQW1DNjlSWlllemV2Z3lQYUcyQXdCTUUteC1KSy16LWFnZUFnc1dxb0I2VXpXTU5wRDNnIn0=; Path=/; SameSite=Lax', 'vary': 'Cookie', 'access-control-allow-origin': 'https://sapiengraph.com', 'access-control-allow-methods': 'GET, POST, PUT, DELETE, OPTIONS', 'access-control-allow-headers': 'Content-Type, Authorization, X-Requested-With, CF-Access-Client-Id, CF-Access-Client-Secret', 'x-envoy-upstream-service-time': '263', 'cf-cache-status': 'DYNAMIC', 'Report-To': '{\"endpoints\":[{\"url\":\"https:\\\\/\\\\/a.nel.cloudflare.com\\\\/report\\\\/v4?s=v8%2FXE9yuzVo2jlrM1r78lgKtWQyDrlIFfcOiWM2Ej8F1zWiDwy8B8kxiWaUyDzbLeiVDRTPcgDPUVLrH5ah1yi79O4RFBd5up1DUatJezPDlzQiN2Pl62ZXDjvs%3D\"}],\"group\":\"cf-nel\",\"max_age\":604800}', 'NEL': '{\"success_fraction\":0,\"report_to\":\"cf-nel\",\"max_age\":604800}', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'X-Content-Type-Options': 'nosniff', 'Server': 'cloudflare', 'CF-RAY': '8fafca151d417cec-LAX', 'alt-svc': 'h3=\":443\"; ma=86400', 'server-timing': 'cfL4;desc=\"?proto=TCP&rtt=9115&min_rtt=8981&rtt_var=3464&sent=4&recv=6&lost=0&retrans=0&sent_bytes=2831&recv_bytes=1093&delivery_rate=322458&cwnd=146&unsent_bytes=0&cid=f7ae30cbc7bfb33e&ts=499&x=0\"'}\n",
            "\n",
            "Response Content Preview:\n",
            "{\"employees\": [], \"next_page\": null}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "api_key = \"P8Lz2OgMswmkHs2qOtAc-w\"\n",
        "headers = {'Authorization': f'Bearer {api_key}'}\n",
        "api_endpoint = 'https://nubela.co/proxycurl/api/linkedin/company/employees'\n",
        "\n",
        "# Modified parameters to get a broader range of employees\n",
        "params = {\n",
        "    'url': 'https://www.linkedin.com/company/get-basis-ai/',\n",
        "    'coy_name_match': 'include',\n",
        "    'use_cache': 'if-present',\n",
        "    'enrich_profiles': 'enrich',\n",
        "    # Modified role search to include more leadership positions\n",
        "    'role_search': '(founder|ceo|chief|director|president)',\n",
        "    'page_size': '10',\n",
        "    'employment_status': 'current',\n",
        "    'sort_by': 'recently-joined',\n",
        "    'resolve_numeric_id': 'false'\n",
        "}\n",
        "\n",
        "print(\"Testing API connection to ZhipuAI's employee data with broader search...\")\n",
        "print(f\"Using endpoint: {api_endpoint}\")\n",
        "print(f\"With parameters: {params}\")\n",
        "print(f\"Headers: {headers}\")\n",
        "\n",
        "response = requests.get(\n",
        "    api_endpoint,\n",
        "    params=params,\n",
        "    headers=headers\n",
        ")\n",
        "\n",
        "print(\"\\nResponse Status:\", response.status_code)\n",
        "print(\"\\nResponse Headers:\", dict(response.headers))\n",
        "print(\"\\nResponse Content Preview:\")\n",
        "print(response.text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXKigJUaJPed",
        "outputId": "f6bb392a-f409-4172-84ab-d7195f9831ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing API connection to ZhipuAI's employee data with broader search...\n",
            "Using endpoint: https://nubela.co/proxycurl/api/linkedin/company/employees\n",
            "With parameters: {'url': 'https://www.linkedin.com/company/get-basis-ai/', 'coy_name_match': 'include', 'use_cache': 'if-present', 'enrich_profiles': 'enrich', 'role_search': '(founder|ceo|chief|director|president)', 'page_size': '10', 'employment_status': 'current', 'sort_by': 'recently-joined', 'resolve_numeric_id': 'false'}\n",
            "Headers: {'Authorization': 'Bearer P8Lz2OgMswmkHs2qOtAc-w'}\n",
            "\n",
            "Response Status: 403\n",
            "\n",
            "Response Headers: {'Date': 'Wed, 01 Jan 2025 04:35:20 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'access-control-max-age': '3600', 'x-proxycurl-credit-cost': '0', 'set-cookie': 'PC_USER_INFO=eyJlbWFpbCI6ICJlZGVuemFrYXJkMTJAZ21haWwuY29tIiwgInJlZmVycmFsX3Rva2VuIjogImV5SmhiR2NpT2lKSVV6VXhNaUlzSW5SNWNDSTZJa3BYVkNKOS5leUp3Y205a2RXTjBTV1FpT2lKdWRXSmxiR0V1WTI4aUxDSndjbTlrZFdOMFZYTmxja2xrSWpvaVFVaEdlSGh6TjFaSVpVVTBiMUExWTJaaVdqVklSeUo5LkZvSDh6XzRfaEc5LVZTYnVBM19KQUtqeVZRLVV4ZGpLMVVuQW1DNjlSWlllemV2Z3lQYUcyQXdCTUUteC1KSy16LWFnZUFnc1dxb0I2VXpXTU5wRDNnIn0=; Path=/; SameSite=Lax', 'vary': 'Cookie', 'access-control-allow-origin': 'https://sapiengraph.com', 'access-control-allow-methods': 'GET, POST, PUT, DELETE, OPTIONS', 'access-control-allow-headers': 'Content-Type, Authorization, X-Requested-With, CF-Access-Client-Id, CF-Access-Client-Secret', 'x-envoy-upstream-service-time': '500', 'cf-cache-status': 'DYNAMIC', 'Report-To': '{\"endpoints\":[{\"url\":\"https:\\\\/\\\\/a.nel.cloudflare.com\\\\/report\\\\/v4?s=9w9p31K49TRpYkxsFr9f2KPQFD9EOHXMZElWgV2j2LrEreAhq8WTyumw1SB1JiT3raD3SutgQ526V9M%2BCkl2e9jVztWIuKJ6WRr8hAMeKdh78ORqnialcZ0tS%2B8%3D\"}],\"group\":\"cf-nel\",\"max_age\":604800}', 'NEL': '{\"success_fraction\":0,\"report_to\":\"cf-nel\",\"max_age\":604800}', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'X-Content-Type-Options': 'nosniff', 'Server': 'cloudflare', 'CF-RAY': '8fafcd517c2f08d5-LAX', 'Content-Encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400', 'server-timing': 'cfL4;desc=\"?proto=TCP&rtt=7886&min_rtt=7879&rtt_var=2969&sent=4&recv=6&lost=0&retrans=0&sent_bytes=2832&recv_bytes=1126&delivery_rate=364827&cwnd=32&unsent_bytes=0&cid=94b7ee873ca7e3eb&ts=740&x=0\"'}\n",
            "\n",
            "Response Content Preview:\n",
            "{\"code\":403,\"description\":\"Insufficient credits. You need 162 credits for this request. Please top-up your credits or modify your request.\",\"name\":\"Forbidden\"}\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}