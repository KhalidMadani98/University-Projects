{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz1gKSj3XuXi"
      },
      "source": [
        "## GCN CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NquQJyTgXwhG"
      },
      "outputs": [],
      "source": [
        "!pip -q install rdkit\n",
        "!pip -q install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.1.0+cu121.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qGfgC8mZ3ya"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdchem, rdmolops, Descriptors, rdMolDescriptors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def atom_features(atom):\n",
        "    \"\"\"Create an atom feature vector.\"\"\"\n",
        "    return [\n",
        "        atom.GetAtomicNum(),\n",
        "        atom.GetDegree(),\n",
        "        atom.GetFormalCharge(),\n",
        "        atom.GetHybridization().real,\n",
        "        atom.GetIsAromatic(),\n",
        "        # Add more atom-level features here.\n",
        "    ]\n",
        "\n",
        "def bond_features(bond):\n",
        "    \"\"\"Create a bond feature vector.\"\"\"\n",
        "    bt = bond.GetBondType()\n",
        "    return [\n",
        "        bt == rdchem.BondType.SINGLE,\n",
        "        bt == rdchem.BondType.DOUBLE,\n",
        "        bt == rdchem.BondType.TRIPLE,\n",
        "        bt == rdchem.BondType.AROMATIC,\n",
        "        bond.GetIsConjugated(),\n",
        "        bond.IsInRing(),\n",
        "        # Add more bond-level features here.\n",
        "    ]\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    \"\"\"\n",
        "    Convert a SMILES string to a graph with nodes as atoms and edges as bonds,\n",
        "    including additional features for atoms and bonds.\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    mol = Chem.AddHs(mol)  # Add hydrogens to the molecule.\n",
        "\n",
        "    # Get atom features\n",
        "    atom_features_list = [atom_features(atom) for atom in mol.GetAtoms()]\n",
        "\n",
        "    # Get bond features (as a matrix)\n",
        "    num_atoms = mol.GetNumAtoms()\n",
        "    bond_features_matrix = np.zeros((num_atoms, num_atoms, len(bond_features(mol.GetBonds()[0]))))\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        idx = bond.GetBeginAtomIdx()\n",
        "        jdx = bond.GetEndAtomIdx()\n",
        "        bond_feat = bond_features(bond)\n",
        "\n",
        "        bond_features_matrix[idx, jdx, :] = bond_feat\n",
        "        bond_features_matrix[jdx, idx, :] = bond_feat  # Bond is undirected\n",
        "\n",
        "    # Create adjacency matrix where entries are 1 for bonded atom pairs\n",
        "    adjacency_matrix = rdmolops.GetAdjacencyMatrix(mol)\n",
        "\n",
        "    graph = {\n",
        "        \"atom_features\": atom_features_list,\n",
        "        \"bond_features\": bond_features_matrix,\n",
        "        \"adjacency_matrix\": adjacency_matrix\n",
        "        # Add more graph-level features if needed\n",
        "    }\n",
        "\n",
        "    return graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVv55pwkanat"
      },
      "outputs": [],
      "source": [
        "def read_and_process_dataset(file_path, target_columns):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    smiles_column = 'Smiles'\n",
        "    graphs = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        graph = smiles_to_graph(row[smiles_column])\n",
        "        labels.append([row[col] for col in target_columns])\n",
        "        graphs.append(graph)\n",
        "\n",
        "    return graphs, np.array(labels)\n",
        "\n",
        "# Replace with the path to your  dataset and target columns\n",
        "file_path = '/content/LCIA_DATASET_CLEANED_V3.csv'\n",
        "target_columns = ['HTP']  # Replace with target column names\n",
        "\n",
        "graphs, labels = read_and_process_dataset(file_path, target_columns)\n",
        "\n",
        "# Standardize target columns\n",
        "scaler = StandardScaler()\n",
        "labels_scaled = scaler.fit_transform(labels)\n",
        "\n",
        "# Convert graphs to PyG data objects\n",
        "pyg_graphs = [\n",
        "    Data(\n",
        "        x=torch.tensor(graph['atom_features'], dtype=torch.float),\n",
        "        edge_index=torch.tensor(np.nonzero(graph['adjacency_matrix'])).type(torch.long),\n",
        "        y=torch.tensor([labels_scaled[i]], dtype=torch.float) # GWP value for each graph\n",
        "      )\n",
        "for i, graph in enumerate(graphs)\n",
        "]\n",
        "\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(pyg_graphs, labels_scaled, train_size=0.6, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBGgKthuh86O"
      },
      "outputs": [],
      "source": [
        "num_node_features = pyg_graphs[0].num_node_features\n",
        "#num_classes = labels_scaled.shape[1]  # This is the number of target columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx-Be9oCiFzB"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_node_features):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, 64)\n",
        "        self.conv2 = GCNConv(64, 32)\n",
        "        self.fc = torch.nn.Linear(32, 1)  # Output one value per graph\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = global_mean_pool(x, batch)  # Pool node features to get graph-level features\n",
        "        x = self.fc(x)  # Output one value per graph\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = GCN(num_node_features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOvepqFDjLxe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Convert the lists of PyG Data objects into DataLoader objects\n",
        "train_loader = DataLoader(X_train, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(X_val, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(X_test, batch_size=8, shuffle=False)\n",
        "\n",
        "# Instantiate the model\n",
        "model = GCN(num_node_features)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the loss function\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)\n",
        "\n",
        "# Early stopping parameters\n",
        "early_stopping_patience = 25\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96K43u3vko6f",
        "outputId": "1f8513e3-4a0f-4975-9259-c45774be6162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300, Train Loss: 1.9190, Val Loss: 0.0187\n",
            "Epoch 2/300, Train Loss: 1.7026, Val Loss: 0.0334\n",
            "Epoch 3/300, Train Loss: 1.7240, Val Loss: 0.0241\n",
            "Epoch 4/300, Train Loss: 1.7723, Val Loss: 0.0284\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.7499, Val Loss: 0.0269\n",
            "Epoch 6/300, Train Loss: 1.7191, Val Loss: 0.0493\n",
            "Epoch 7/300, Train Loss: 1.7216, Val Loss: 0.0666\n",
            "Epoch 8/300, Train Loss: 1.7282, Val Loss: 0.0214\n",
            "Epoch 9/300, Train Loss: 1.7228, Val Loss: 0.0127\n",
            "Epoch 10/300, Train Loss: 1.7075, Val Loss: 0.0186\n",
            "Epoch 11/300, Train Loss: 1.6676, Val Loss: 0.0197\n",
            "Epoch 12/300, Train Loss: 1.6871, Val Loss: 0.0177\n",
            "Epoch 00013: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 13/300, Train Loss: 1.7049, Val Loss: 0.0209\n",
            "Epoch 14/300, Train Loss: 1.6853, Val Loss: 0.0245\n",
            "Epoch 15/300, Train Loss: 1.6842, Val Loss: 0.0250\n",
            "Epoch 16/300, Train Loss: 1.6665, Val Loss: 0.0299\n",
            "Epoch 00017: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 17/300, Train Loss: 1.6906, Val Loss: 0.0248\n",
            "Epoch 18/300, Train Loss: 1.6834, Val Loss: 0.0232\n",
            "Epoch 19/300, Train Loss: 1.6877, Val Loss: 0.0225\n",
            "Epoch 20/300, Train Loss: 1.6931, Val Loss: 0.0233\n",
            "Epoch 00021: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 21/300, Train Loss: 1.6782, Val Loss: 0.0251\n",
            "Epoch 22/300, Train Loss: 1.6351, Val Loss: 0.0199\n",
            "Epoch 23/300, Train Loss: 1.6892, Val Loss: 0.0244\n",
            "Epoch 24/300, Train Loss: 1.6623, Val Loss: 0.0223\n",
            "Epoch 00025: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 25/300, Train Loss: 1.7053, Val Loss: 0.0224\n",
            "Epoch 26/300, Train Loss: 1.6752, Val Loss: 0.0233\n",
            "Epoch 27/300, Train Loss: 1.6887, Val Loss: 0.0214\n",
            "Epoch 28/300, Train Loss: 1.6546, Val Loss: 0.0195\n",
            "Epoch 00029: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 29/300, Train Loss: 1.6885, Val Loss: 0.0198\n",
            "Epoch 30/300, Train Loss: 1.7064, Val Loss: 0.0196\n",
            "Epoch 31/300, Train Loss: 1.6827, Val Loss: 0.0218\n",
            "Epoch 32/300, Train Loss: 1.6765, Val Loss: 0.0190\n",
            "Epoch 00033: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 33/300, Train Loss: 1.6764, Val Loss: 0.0188\n",
            "Epoch 34/300, Train Loss: 1.6358, Val Loss: 0.0196\n",
            "Early stopping triggered after 34 epochs!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, criterion, scheduler, epochs, early_stopping_patience):\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                output = model(data)\n",
        "                loss = criterion(output, data.y)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        # Scheduler step (for learning rate decay)\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == early_stopping_patience:\n",
        "                print(f'Early stopping triggered after {epoch+1} epochs!')\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Number of epochs\n",
        "epochs = 300\n",
        "\n",
        "# Early stopping patience\n",
        "early_stopping_patience = 25\n",
        "\n",
        "# Train the model\n",
        "trained_model = train(model, train_loader, val_loader, optimizer, loss_func, scheduler, epochs, early_stopping_patience)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDmaBE1uko_l"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            output = model(data)\n",
        "            predictions.append(output.numpy())\n",
        "            actuals.append(data.y.numpy())\n",
        "\n",
        "    predictions = np.vstack(predictions)\n",
        "    actuals = np.vstack(actuals)\n",
        "\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    r2 = r2_score(actuals, predictions)\n",
        "\n",
        "    return mae, mse, r2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx4RAyeruWo5",
        "outputId": "e6beae24-0046-4a89-8687-aee456068273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation MAE: 0.13007131218910217\n",
            "Validation MSE: 0.02008882164955139\n",
            "Validation R²: -7.2505609208639985\n",
            "Test MAE: 0.12080761045217514\n",
            "Test MSE: 0.017602864652872086\n",
            "Test R²: -5.423544480582923\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on the validation set\n",
        "val_mae, val_mse, val_r2 = evaluate_model(trained_model, val_loader)\n",
        "print(f'Validation MAE: {val_mae}')\n",
        "print(f'Validation MSE: {val_mse}')\n",
        "print(f'Validation R²: {val_r2}')\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_mae, test_mse, test_r2 = evaluate_model(trained_model, test_loader)\n",
        "print(f'Test MAE: {test_mae}')\n",
        "print(f'Test MSE: {test_mse}')\n",
        "print(f'Test R²: {test_r2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1YxQOiscPaH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Check the shapes of the tensors\n",
        "# for i, data in enumerate(pyg_graphs[:5]):  # Check the first 5 graphs as an example\n",
        "#     print(f\"Graph {i}:\")\n",
        "#     print(f\"Number of nodes: {data.num_nodes}\")\n",
        "#     print(f\"Shape of node features (x): {data.x.shape}\")\n",
        "#     print(f\"Shape of edge_index: {data.edge_index.shape}\")\n",
        "#     print(\"Sample node features:\", data.x[:5])  # Print the features of the first 5 nodes\n",
        "#     print(\"Sample edges:\", data.edge_index[:, :5])  # Print the first 5 edges\n",
        "#     print(\"\\n\")\n",
        "\n",
        "#check against a known molecule\n",
        "# known_smiles = \"CCO\"  # Example SMILES for ethanol\n",
        "# known_graph = smiles_to_graph(known_smiles)\n",
        "# ethanol_data = Data(\n",
        "#     x=torch.tensor(np.array(known_graph['atom_features']), dtype=torch.float),\n",
        "#     edge_index=torch.tensor(np.nonzero(known_graph['adjacency_matrix'])).type(torch.long)\n",
        "# )\n",
        "\n",
        "# print(\"Known molecule - Ethanol:\")\n",
        "# print(\"Number of nodes:\", ethanol_data.num_nodes)\n",
        "# print(\"Shape of node features (x):\", ethanol_data.x.shape)\n",
        "# print(\"Shape of edge_index:\", ethanol_data.edge_index.shape)\n",
        "# print(\"Node features:\", ethanol_data.x)\n",
        "# print(\"Edges:\", ethanol_data.edge_index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPJd9yVpv_BK"
      },
      "source": [
        "## Ensamble method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySwPEJFKwtf1",
        "outputId": "f0de8371-61ae-4d84-d468-5fe74a968b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300, Train Loss: 1.4457, Val Loss: 0.0109\n",
            "Epoch 2/300, Train Loss: 1.2838, Val Loss: 0.0015\n",
            "Epoch 3/300, Train Loss: 1.2612, Val Loss: 0.0017\n",
            "Epoch 4/300, Train Loss: 1.2745, Val Loss: 0.0026\n",
            "Epoch 5/300, Train Loss: 1.2873, Val Loss: 0.0025\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2496, Val Loss: 0.0032\n",
            "Epoch 7/300, Train Loss: 1.2371, Val Loss: 0.0035\n",
            "Epoch 8/300, Train Loss: 1.2445, Val Loss: 0.0080\n",
            "Epoch 9/300, Train Loss: 1.2608, Val Loss: 0.0050\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2177, Val Loss: 0.0055\n",
            "Epoch 11/300, Train Loss: 1.2722, Val Loss: 0.0085\n",
            "Epoch 12/300, Train Loss: 1.2286, Val Loss: 0.0096\n",
            "Epoch 13/300, Train Loss: 1.2452, Val Loss: 0.0109\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 14/300, Train Loss: 1.2067, Val Loss: 0.0093\n",
            "Epoch 15/300, Train Loss: 1.2260, Val Loss: 0.0096\n",
            "Epoch 16/300, Train Loss: 1.2145, Val Loss: 0.0098\n",
            "Epoch 17/300, Train Loss: 1.2316, Val Loss: 0.0074\n",
            "Epoch 00018: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 18/300, Train Loss: 1.2454, Val Loss: 0.0094\n",
            "Epoch 19/300, Train Loss: 1.2215, Val Loss: 0.0078\n",
            "Epoch 20/300, Train Loss: 1.2519, Val Loss: 0.0049\n",
            "Epoch 21/300, Train Loss: 1.2369, Val Loss: 0.0092\n",
            "Epoch 00022: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 22/300, Train Loss: 1.2706, Val Loss: 0.0096\n",
            "Epoch 23/300, Train Loss: 1.9984, Val Loss: 0.0080\n",
            "Epoch 24/300, Train Loss: 1.2305, Val Loss: 0.0132\n",
            "Epoch 25/300, Train Loss: 1.9892, Val Loss: 0.0086\n",
            "Epoch 00026: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 26/300, Train Loss: 1.2452, Val Loss: 0.0133\n",
            "Epoch 27/300, Train Loss: 1.2404, Val Loss: 0.0115\n",
            "Early stopping triggered after 27 epochs!\n",
            "Epoch 1/300, Train Loss: 2.1062, Val Loss: 0.0198\n",
            "Epoch 2/300, Train Loss: 1.2350, Val Loss: 0.0037\n",
            "Epoch 3/300, Train Loss: 1.2544, Val Loss: 0.0052\n",
            "Epoch 4/300, Train Loss: 1.2635, Val Loss: 0.0031\n",
            "Epoch 5/300, Train Loss: 1.2323, Val Loss: 0.0094\n",
            "Epoch 6/300, Train Loss: 1.2368, Val Loss: 0.0026\n",
            "Epoch 7/300, Train Loss: 1.2624, Val Loss: 0.0065\n",
            "Epoch 8/300, Train Loss: 1.2356, Val Loss: 0.0035\n",
            "Epoch 9/300, Train Loss: 1.2641, Val Loss: 0.0027\n",
            "Epoch 00010: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2148, Val Loss: 0.0112\n",
            "Epoch 11/300, Train Loss: 1.2552, Val Loss: 0.0096\n",
            "Epoch 12/300, Train Loss: 1.2515, Val Loss: 0.0070\n",
            "Epoch 13/300, Train Loss: 1.2291, Val Loss: 0.0040\n",
            "Epoch 00014: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 14/300, Train Loss: 1.2189, Val Loss: 0.0048\n",
            "Epoch 15/300, Train Loss: 1.2343, Val Loss: 0.0083\n",
            "Epoch 16/300, Train Loss: 1.2288, Val Loss: 0.0059\n",
            "Epoch 17/300, Train Loss: 1.2519, Val Loss: 0.0063\n",
            "Epoch 00018: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 18/300, Train Loss: 1.2347, Val Loss: 0.0061\n",
            "Epoch 19/300, Train Loss: 1.2159, Val Loss: 0.0075\n",
            "Epoch 20/300, Train Loss: 1.2279, Val Loss: 0.0050\n",
            "Epoch 21/300, Train Loss: 1.2239, Val Loss: 0.0078\n",
            "Epoch 00022: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 22/300, Train Loss: 1.2309, Val Loss: 0.0122\n",
            "Epoch 23/300, Train Loss: 1.2161, Val Loss: 0.0109\n",
            "Epoch 24/300, Train Loss: 1.2488, Val Loss: 0.0090\n",
            "Epoch 25/300, Train Loss: 1.2264, Val Loss: 0.0115\n",
            "Epoch 00026: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 26/300, Train Loss: 1.2254, Val Loss: 0.0119\n",
            "Epoch 27/300, Train Loss: 1.2169, Val Loss: 0.0101\n",
            "Epoch 28/300, Train Loss: 1.2366, Val Loss: 0.0140\n",
            "Epoch 29/300, Train Loss: 1.2319, Val Loss: 0.0099\n",
            "Epoch 00030: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 30/300, Train Loss: 1.2321, Val Loss: 0.0126\n",
            "Epoch 31/300, Train Loss: 1.2292, Val Loss: 0.0102\n",
            "Early stopping triggered after 31 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3229, Val Loss: 0.0052\n",
            "Epoch 2/300, Train Loss: 1.3143, Val Loss: 0.0050\n",
            "Epoch 3/300, Train Loss: 1.2946, Val Loss: 0.0196\n",
            "Epoch 4/300, Train Loss: 1.2999, Val Loss: 0.0200\n",
            "Epoch 5/300, Train Loss: 1.3086, Val Loss: 0.0096\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2483, Val Loss: 0.0064\n",
            "Epoch 7/300, Train Loss: 1.2652, Val Loss: 0.0099\n",
            "Epoch 8/300, Train Loss: 1.2446, Val Loss: 0.0112\n",
            "Epoch 9/300, Train Loss: 1.2193, Val Loss: 0.0111\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2512, Val Loss: 0.0097\n",
            "Epoch 11/300, Train Loss: 1.2108, Val Loss: 0.0091\n",
            "Epoch 12/300, Train Loss: 1.2511, Val Loss: 0.0118\n",
            "Epoch 13/300, Train Loss: 1.2549, Val Loss: 0.0099\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 14/300, Train Loss: 1.2432, Val Loss: 0.0102\n",
            "Epoch 15/300, Train Loss: 1.2164, Val Loss: 0.0106\n",
            "Epoch 16/300, Train Loss: 1.2249, Val Loss: 0.0161\n",
            "Epoch 17/300, Train Loss: 1.2242, Val Loss: 0.0100\n",
            "Epoch 00018: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 18/300, Train Loss: 1.2641, Val Loss: 0.0106\n",
            "Epoch 19/300, Train Loss: 1.2394, Val Loss: 0.0091\n",
            "Epoch 20/300, Train Loss: 1.9844, Val Loss: 0.0088\n",
            "Epoch 21/300, Train Loss: 1.2331, Val Loss: 0.0142\n",
            "Epoch 00022: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 22/300, Train Loss: 1.2532, Val Loss: 0.0091\n",
            "Epoch 23/300, Train Loss: 1.2035, Val Loss: 0.0098\n",
            "Epoch 24/300, Train Loss: 1.2201, Val Loss: 0.0112\n",
            "Epoch 25/300, Train Loss: 1.2347, Val Loss: 0.0114\n",
            "Epoch 00026: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 26/300, Train Loss: 1.1904, Val Loss: 0.0107\n",
            "Epoch 27/300, Train Loss: 1.2096, Val Loss: 0.0104\n",
            "Early stopping triggered after 27 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2776, Val Loss: 0.0117\n",
            "Epoch 2/300, Train Loss: 1.2565, Val Loss: 0.0104\n",
            "Epoch 3/300, Train Loss: 1.2702, Val Loss: 0.0101\n",
            "Epoch 4/300, Train Loss: 1.2275, Val Loss: 0.0161\n",
            "Epoch 5/300, Train Loss: 1.2327, Val Loss: 0.0115\n",
            "Epoch 6/300, Train Loss: 1.2267, Val Loss: 0.0116\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2283, Val Loss: 0.0131\n",
            "Epoch 8/300, Train Loss: 1.2380, Val Loss: 0.0213\n",
            "Epoch 9/300, Train Loss: 1.2439, Val Loss: 0.0169\n",
            "Epoch 10/300, Train Loss: 1.9766, Val Loss: 0.0119\n",
            "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2136, Val Loss: 0.0297\n",
            "Epoch 12/300, Train Loss: 1.2360, Val Loss: 0.0229\n",
            "Epoch 13/300, Train Loss: 1.2260, Val Loss: 0.0200\n",
            "Epoch 14/300, Train Loss: 1.2196, Val Loss: 0.0156\n",
            "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 15/300, Train Loss: 1.2332, Val Loss: 0.0182\n",
            "Epoch 16/300, Train Loss: 1.2348, Val Loss: 0.0163\n",
            "Epoch 17/300, Train Loss: 1.2421, Val Loss: 0.0189\n",
            "Epoch 18/300, Train Loss: 1.2201, Val Loss: 0.0215\n",
            "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 19/300, Train Loss: 1.9615, Val Loss: 0.0145\n",
            "Epoch 20/300, Train Loss: 1.2490, Val Loss: 0.0205\n",
            "Epoch 21/300, Train Loss: 1.2338, Val Loss: 0.0186\n",
            "Epoch 22/300, Train Loss: 1.2301, Val Loss: 0.0166\n",
            "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 23/300, Train Loss: 1.2228, Val Loss: 0.0143\n",
            "Epoch 24/300, Train Loss: 1.2215, Val Loss: 0.0154\n",
            "Epoch 25/300, Train Loss: 1.2252, Val Loss: 0.0150\n",
            "Epoch 26/300, Train Loss: 1.2378, Val Loss: 0.0143\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 27/300, Train Loss: 1.2408, Val Loss: 0.0150\n",
            "Epoch 28/300, Train Loss: 1.2356, Val Loss: 0.0139\n",
            "Early stopping triggered after 28 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2617, Val Loss: 0.0034\n",
            "Epoch 2/300, Train Loss: 1.2668, Val Loss: 0.0034\n",
            "Epoch 3/300, Train Loss: 1.2405, Val Loss: 0.0041\n",
            "Epoch 4/300, Train Loss: 1.2165, Val Loss: 0.0035\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.2509, Val Loss: 0.0057\n",
            "Epoch 6/300, Train Loss: 1.2335, Val Loss: 0.0058\n",
            "Epoch 7/300, Train Loss: 1.2389, Val Loss: 0.0073\n",
            "Epoch 8/300, Train Loss: 1.2322, Val Loss: 0.0097\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 1.2172, Val Loss: 0.0168\n",
            "Epoch 10/300, Train Loss: 1.2176, Val Loss: 0.0167\n",
            "Epoch 11/300, Train Loss: 1.2470, Val Loss: 0.0143\n",
            "Epoch 12/300, Train Loss: 1.2369, Val Loss: 0.0056\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 1.2367, Val Loss: 0.0124\n",
            "Epoch 14/300, Train Loss: 1.2153, Val Loss: 0.0084\n",
            "Epoch 15/300, Train Loss: 1.2385, Val Loss: 0.0074\n",
            "Epoch 16/300, Train Loss: 1.2096, Val Loss: 0.0143\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 1.2409, Val Loss: 0.0144\n",
            "Epoch 18/300, Train Loss: 1.2178, Val Loss: 0.0124\n",
            "Epoch 19/300, Train Loss: 1.2354, Val Loss: 0.0079\n",
            "Epoch 20/300, Train Loss: 1.2065, Val Loss: 0.0113\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 1.9805, Val Loss: 0.0059\n",
            "Epoch 22/300, Train Loss: 1.2345, Val Loss: 0.0125\n",
            "Epoch 23/300, Train Loss: 1.2151, Val Loss: 0.0133\n",
            "Epoch 24/300, Train Loss: 1.2293, Val Loss: 0.0094\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 1.9404, Val Loss: 0.0078\n",
            "Epoch 26/300, Train Loss: 1.2317, Val Loss: 0.0146\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3854, Val Loss: 0.0125\n",
            "Epoch 2/300, Train Loss: 1.2494, Val Loss: 0.0217\n",
            "Epoch 3/300, Train Loss: 1.2641, Val Loss: 0.0105\n",
            "Epoch 4/300, Train Loss: 1.3124, Val Loss: 0.0089\n",
            "Epoch 5/300, Train Loss: 1.2674, Val Loss: 0.0237\n",
            "Epoch 6/300, Train Loss: 1.2396, Val Loss: 0.0137\n",
            "Epoch 7/300, Train Loss: 1.2484, Val Loss: 0.0138\n",
            "Epoch 00008: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 8/300, Train Loss: 1.2501, Val Loss: 0.0107\n",
            "Epoch 9/300, Train Loss: 1.2295, Val Loss: 0.0164\n",
            "Epoch 10/300, Train Loss: 1.2262, Val Loss: 0.0110\n",
            "Epoch 11/300, Train Loss: 1.2478, Val Loss: 0.0107\n",
            "Epoch 00012: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 12/300, Train Loss: 1.2493, Val Loss: 0.0104\n",
            "Epoch 13/300, Train Loss: 1.2232, Val Loss: 0.0102\n",
            "Epoch 14/300, Train Loss: 1.2453, Val Loss: 0.0168\n",
            "Epoch 15/300, Train Loss: 1.2426, Val Loss: 0.0124\n",
            "Epoch 00016: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 16/300, Train Loss: 1.2425, Val Loss: 0.0109\n",
            "Epoch 17/300, Train Loss: 1.2388, Val Loss: 0.0096\n",
            "Epoch 18/300, Train Loss: 1.9242, Val Loss: 0.0113\n",
            "Epoch 19/300, Train Loss: 1.2466, Val Loss: 0.0250\n",
            "Epoch 00020: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 20/300, Train Loss: 1.2556, Val Loss: 0.0140\n",
            "Epoch 21/300, Train Loss: 1.2395, Val Loss: 0.0148\n",
            "Epoch 22/300, Train Loss: 1.2215, Val Loss: 0.0126\n",
            "Epoch 23/300, Train Loss: 1.2626, Val Loss: 0.0113\n",
            "Epoch 00024: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 24/300, Train Loss: 1.2416, Val Loss: 0.0107\n",
            "Epoch 25/300, Train Loss: 1.2258, Val Loss: 0.0144\n",
            "Epoch 26/300, Train Loss: 1.2533, Val Loss: 0.0125\n",
            "Epoch 27/300, Train Loss: 1.2337, Val Loss: 0.0120\n",
            "Epoch 00028: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 28/300, Train Loss: 1.2406, Val Loss: 0.0124\n",
            "Epoch 29/300, Train Loss: 1.2238, Val Loss: 0.0117\n",
            "Early stopping triggered after 29 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3307, Val Loss: 0.0189\n",
            "Epoch 2/300, Train Loss: 1.2540, Val Loss: 0.0082\n",
            "Epoch 3/300, Train Loss: 1.2567, Val Loss: 0.0056\n",
            "Epoch 4/300, Train Loss: 1.2285, Val Loss: 0.0107\n",
            "Epoch 5/300, Train Loss: 1.2631, Val Loss: 0.0047\n",
            "Epoch 6/300, Train Loss: 1.2473, Val Loss: 0.0128\n",
            "Epoch 7/300, Train Loss: 1.2300, Val Loss: 0.0091\n",
            "Epoch 8/300, Train Loss: 1.9482, Val Loss: 0.0043\n",
            "Epoch 9/300, Train Loss: 1.2299, Val Loss: 0.0230\n",
            "Epoch 10/300, Train Loss: 1.2506, Val Loss: 0.0166\n",
            "Epoch 11/300, Train Loss: 1.2416, Val Loss: 0.0096\n",
            "Epoch 00012: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 12/300, Train Loss: 1.2334, Val Loss: 0.0119\n",
            "Epoch 13/300, Train Loss: 1.2500, Val Loss: 0.0198\n",
            "Epoch 14/300, Train Loss: 1.9613, Val Loss: 0.0064\n",
            "Epoch 15/300, Train Loss: 1.2294, Val Loss: 0.0105\n",
            "Epoch 00016: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 16/300, Train Loss: 1.2678, Val Loss: 0.0119\n",
            "Epoch 17/300, Train Loss: 1.9725, Val Loss: 0.0093\n",
            "Epoch 18/300, Train Loss: 1.2519, Val Loss: 0.0195\n",
            "Epoch 19/300, Train Loss: 1.2271, Val Loss: 0.0163\n",
            "Epoch 00020: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 20/300, Train Loss: 1.2124, Val Loss: 0.0125\n",
            "Epoch 21/300, Train Loss: 1.2242, Val Loss: 0.0134\n",
            "Epoch 22/300, Train Loss: 1.9691, Val Loss: 0.0120\n",
            "Epoch 23/300, Train Loss: 1.2436, Val Loss: 0.0194\n",
            "Epoch 00024: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 24/300, Train Loss: 1.2248, Val Loss: 0.0151\n",
            "Epoch 25/300, Train Loss: 1.2253, Val Loss: 0.0164\n",
            "Epoch 26/300, Train Loss: 1.2348, Val Loss: 0.0166\n",
            "Epoch 27/300, Train Loss: 1.2191, Val Loss: 0.0157\n",
            "Epoch 00028: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 28/300, Train Loss: 1.2354, Val Loss: 0.0125\n",
            "Epoch 29/300, Train Loss: 1.2385, Val Loss: 0.0138\n",
            "Epoch 30/300, Train Loss: 1.2274, Val Loss: 0.0154\n",
            "Epoch 31/300, Train Loss: 1.2313, Val Loss: 0.0157\n",
            "Epoch 00032: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 32/300, Train Loss: 1.2227, Val Loss: 0.0157\n",
            "Epoch 33/300, Train Loss: 1.2268, Val Loss: 0.0138\n",
            "Early stopping triggered after 33 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3318, Val Loss: 0.0153\n",
            "Epoch 2/300, Train Loss: 1.2631, Val Loss: 0.0035\n",
            "Epoch 3/300, Train Loss: 1.3066, Val Loss: 0.0147\n",
            "Epoch 4/300, Train Loss: 1.2751, Val Loss: 0.0051\n",
            "Epoch 5/300, Train Loss: 1.2572, Val Loss: 0.0067\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2575, Val Loss: 0.0081\n",
            "Epoch 7/300, Train Loss: 1.2730, Val Loss: 0.0149\n",
            "Epoch 8/300, Train Loss: 1.2634, Val Loss: 0.0090\n",
            "Epoch 9/300, Train Loss: 1.2575, Val Loss: 0.0057\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2309, Val Loss: 0.0118\n",
            "Epoch 11/300, Train Loss: 1.2416, Val Loss: 0.0067\n",
            "Epoch 12/300, Train Loss: 1.2160, Val Loss: 0.0077\n",
            "Epoch 13/300, Train Loss: 1.2258, Val Loss: 0.0082\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 14/300, Train Loss: 1.2379, Val Loss: 0.0087\n",
            "Epoch 15/300, Train Loss: 1.2283, Val Loss: 0.0104\n",
            "Epoch 16/300, Train Loss: 1.2444, Val Loss: 0.0098\n",
            "Epoch 17/300, Train Loss: 1.2332, Val Loss: 0.0084\n",
            "Epoch 00018: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 18/300, Train Loss: 1.9702, Val Loss: 0.0072\n",
            "Epoch 19/300, Train Loss: 1.2518, Val Loss: 0.0142\n",
            "Epoch 20/300, Train Loss: 1.2637, Val Loss: 0.0109\n",
            "Epoch 21/300, Train Loss: 1.2559, Val Loss: 0.0092\n",
            "Epoch 00022: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 22/300, Train Loss: 1.2231, Val Loss: 0.0089\n",
            "Epoch 23/300, Train Loss: 1.2422, Val Loss: 0.0088\n",
            "Epoch 24/300, Train Loss: 1.2180, Val Loss: 0.0093\n",
            "Epoch 25/300, Train Loss: 1.2209, Val Loss: 0.0092\n",
            "Epoch 00026: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 26/300, Train Loss: 1.2306, Val Loss: 0.0094\n",
            "Epoch 27/300, Train Loss: 1.2365, Val Loss: 0.0096\n",
            "Early stopping triggered after 27 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2469, Val Loss: 0.0044\n",
            "Epoch 2/300, Train Loss: 1.2445, Val Loss: 0.0177\n",
            "Epoch 3/300, Train Loss: 1.2477, Val Loss: 0.0098\n",
            "Epoch 4/300, Train Loss: 1.2740, Val Loss: 0.0062\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.2341, Val Loss: 0.0056\n",
            "Epoch 6/300, Train Loss: 1.2289, Val Loss: 0.0082\n",
            "Epoch 7/300, Train Loss: 1.2621, Val Loss: 0.0041\n",
            "Epoch 8/300, Train Loss: 1.2345, Val Loss: 0.0043\n",
            "Epoch 9/300, Train Loss: 1.2384, Val Loss: 0.0054\n",
            "Epoch 10/300, Train Loss: 1.2354, Val Loss: 0.0071\n",
            "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2167, Val Loss: 0.0063\n",
            "Epoch 12/300, Train Loss: 1.2346, Val Loss: 0.0056\n",
            "Epoch 13/300, Train Loss: 1.2305, Val Loss: 0.0062\n",
            "Epoch 14/300, Train Loss: 1.2414, Val Loss: 0.0076\n",
            "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 15/300, Train Loss: 1.2245, Val Loss: 0.0069\n",
            "Epoch 16/300, Train Loss: 1.2130, Val Loss: 0.0076\n",
            "Epoch 17/300, Train Loss: 1.2317, Val Loss: 0.0068\n",
            "Epoch 18/300, Train Loss: 1.2386, Val Loss: 0.0068\n",
            "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 19/300, Train Loss: 1.2341, Val Loss: 0.0064\n",
            "Epoch 20/300, Train Loss: 1.2257, Val Loss: 0.0075\n",
            "Epoch 21/300, Train Loss: 1.2516, Val Loss: 0.0064\n",
            "Epoch 22/300, Train Loss: 1.2259, Val Loss: 0.0060\n",
            "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 23/300, Train Loss: 1.2302, Val Loss: 0.0078\n",
            "Epoch 24/300, Train Loss: 1.2408, Val Loss: 0.0073\n",
            "Epoch 25/300, Train Loss: 1.2308, Val Loss: 0.0070\n",
            "Epoch 26/300, Train Loss: 1.2268, Val Loss: 0.0074\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 27/300, Train Loss: 1.2372, Val Loss: 0.0072\n",
            "Epoch 28/300, Train Loss: 1.2340, Val Loss: 0.0075\n",
            "Epoch 29/300, Train Loss: 1.2085, Val Loss: 0.0073\n",
            "Epoch 30/300, Train Loss: 1.2285, Val Loss: 0.0064\n",
            "Epoch 00031: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 31/300, Train Loss: 1.2201, Val Loss: 0.0071\n",
            "Epoch 32/300, Train Loss: 1.2226, Val Loss: 0.0073\n",
            "Early stopping triggered after 32 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3016, Val Loss: 0.0105\n",
            "Epoch 2/300, Train Loss: 1.3030, Val Loss: 0.0227\n",
            "Epoch 3/300, Train Loss: 1.2758, Val Loss: 0.0124\n",
            "Epoch 4/300, Train Loss: 1.2364, Val Loss: 0.0408\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.2632, Val Loss: 0.0154\n",
            "Epoch 6/300, Train Loss: 1.2275, Val Loss: 0.0251\n",
            "Epoch 7/300, Train Loss: 1.2355, Val Loss: 0.0256\n",
            "Epoch 8/300, Train Loss: 1.2268, Val Loss: 0.0197\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 1.2365, Val Loss: 0.0167\n",
            "Epoch 10/300, Train Loss: 1.2416, Val Loss: 0.0189\n",
            "Epoch 11/300, Train Loss: 1.2133, Val Loss: 0.0131\n",
            "Epoch 12/300, Train Loss: 1.2292, Val Loss: 0.0127\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 1.2447, Val Loss: 0.0147\n",
            "Epoch 14/300, Train Loss: 1.2303, Val Loss: 0.0173\n",
            "Epoch 15/300, Train Loss: 1.2360, Val Loss: 0.0147\n",
            "Epoch 16/300, Train Loss: 1.2289, Val Loss: 0.0150\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 1.2361, Val Loss: 0.0123\n",
            "Epoch 18/300, Train Loss: 1.2236, Val Loss: 0.0178\n",
            "Epoch 19/300, Train Loss: 1.2336, Val Loss: 0.0161\n",
            "Epoch 20/300, Train Loss: 1.2318, Val Loss: 0.0144\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 1.2350, Val Loss: 0.0135\n",
            "Epoch 22/300, Train Loss: 1.2523, Val Loss: 0.0172\n",
            "Epoch 23/300, Train Loss: 1.2394, Val Loss: 0.0149\n",
            "Epoch 24/300, Train Loss: 1.2406, Val Loss: 0.0154\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 1.2354, Val Loss: 0.0140\n",
            "Epoch 26/300, Train Loss: 1.2189, Val Loss: 0.0147\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 2.1176, Val Loss: 0.0127\n",
            "Epoch 2/300, Train Loss: 1.2512, Val Loss: 0.0160\n",
            "Epoch 3/300, Train Loss: 1.2521, Val Loss: 0.0047\n",
            "Epoch 4/300, Train Loss: 1.2516, Val Loss: 0.0056\n",
            "Epoch 5/300, Train Loss: 1.2426, Val Loss: 0.0104\n",
            "Epoch 6/300, Train Loss: 1.2586, Val Loss: 0.0159\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2572, Val Loss: 0.0075\n",
            "Epoch 8/300, Train Loss: 1.2396, Val Loss: 0.0156\n",
            "Epoch 9/300, Train Loss: 1.2392, Val Loss: 0.0168\n",
            "Epoch 10/300, Train Loss: 1.2698, Val Loss: 0.0084\n",
            "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2159, Val Loss: 0.0187\n",
            "Epoch 12/300, Train Loss: 1.2249, Val Loss: 0.0158\n",
            "Epoch 13/300, Train Loss: 1.2469, Val Loss: 0.0107\n",
            "Epoch 14/300, Train Loss: 1.2579, Val Loss: 0.0129\n",
            "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 15/300, Train Loss: 1.2282, Val Loss: 0.0125\n",
            "Epoch 16/300, Train Loss: 1.2445, Val Loss: 0.0096\n",
            "Epoch 17/300, Train Loss: 1.2261, Val Loss: 0.0098\n",
            "Epoch 18/300, Train Loss: 1.2323, Val Loss: 0.0107\n",
            "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 19/300, Train Loss: 1.2374, Val Loss: 0.0119\n",
            "Epoch 20/300, Train Loss: 1.2169, Val Loss: 0.0108\n",
            "Epoch 21/300, Train Loss: 1.2255, Val Loss: 0.0103\n",
            "Epoch 22/300, Train Loss: 1.2346, Val Loss: 0.0098\n",
            "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 23/300, Train Loss: 1.1945, Val Loss: 0.0116\n",
            "Epoch 24/300, Train Loss: 1.2101, Val Loss: 0.0117\n",
            "Epoch 25/300, Train Loss: 1.2377, Val Loss: 0.0122\n",
            "Epoch 26/300, Train Loss: 1.2372, Val Loss: 0.0114\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 27/300, Train Loss: 1.2363, Val Loss: 0.0115\n",
            "Epoch 28/300, Train Loss: 1.2201, Val Loss: 0.0112\n",
            "Early stopping triggered after 28 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3163, Val Loss: 0.0175\n",
            "Epoch 2/300, Train Loss: 1.2334, Val Loss: 0.0051\n",
            "Epoch 3/300, Train Loss: 1.9532, Val Loss: 0.0182\n",
            "Epoch 4/300, Train Loss: 1.2665, Val Loss: 0.0271\n",
            "Epoch 5/300, Train Loss: 1.2583, Val Loss: 0.0058\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2203, Val Loss: 0.0103\n",
            "Epoch 7/300, Train Loss: 1.2349, Val Loss: 0.0126\n",
            "Epoch 8/300, Train Loss: 1.2532, Val Loss: 0.0091\n",
            "Epoch 9/300, Train Loss: 1.2588, Val Loss: 0.0071\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2259, Val Loss: 0.0141\n",
            "Epoch 11/300, Train Loss: 1.2295, Val Loss: 0.0067\n",
            "Epoch 12/300, Train Loss: 1.2356, Val Loss: 0.0107\n",
            "Epoch 13/300, Train Loss: 1.2580, Val Loss: 0.0145\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 14/300, Train Loss: 1.2256, Val Loss: 0.0103\n",
            "Epoch 15/300, Train Loss: 1.2337, Val Loss: 0.0101\n",
            "Epoch 16/300, Train Loss: 1.2433, Val Loss: 0.0086\n",
            "Epoch 17/300, Train Loss: 1.2340, Val Loss: 0.0131\n",
            "Epoch 00018: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 18/300, Train Loss: 1.2315, Val Loss: 0.0095\n",
            "Epoch 19/300, Train Loss: 1.2157, Val Loss: 0.0136\n",
            "Epoch 20/300, Train Loss: 1.2268, Val Loss: 0.0114\n",
            "Epoch 21/300, Train Loss: 1.2143, Val Loss: 0.0143\n",
            "Epoch 00022: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 22/300, Train Loss: 1.2166, Val Loss: 0.0186\n",
            "Epoch 23/300, Train Loss: 1.2467, Val Loss: 0.0171\n",
            "Epoch 24/300, Train Loss: 1.2146, Val Loss: 0.0155\n",
            "Epoch 25/300, Train Loss: 1.2162, Val Loss: 0.0156\n",
            "Epoch 00026: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 26/300, Train Loss: 1.2114, Val Loss: 0.0155\n",
            "Epoch 27/300, Train Loss: 1.2485, Val Loss: 0.0137\n",
            "Early stopping triggered after 27 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2483, Val Loss: 0.0029\n",
            "Epoch 2/300, Train Loss: 1.2472, Val Loss: 0.0090\n",
            "Epoch 3/300, Train Loss: 1.2465, Val Loss: 0.0053\n",
            "Epoch 4/300, Train Loss: 1.2437, Val Loss: 0.0050\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.2266, Val Loss: 0.0080\n",
            "Epoch 6/300, Train Loss: 1.2221, Val Loss: 0.0044\n",
            "Epoch 7/300, Train Loss: 1.2479, Val Loss: 0.0083\n",
            "Epoch 8/300, Train Loss: 1.2476, Val Loss: 0.0091\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 1.2390, Val Loss: 0.0135\n",
            "Epoch 10/300, Train Loss: 1.2398, Val Loss: 0.0058\n",
            "Epoch 11/300, Train Loss: 1.2218, Val Loss: 0.0113\n",
            "Epoch 12/300, Train Loss: 1.2482, Val Loss: 0.0099\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 1.2452, Val Loss: 0.0081\n",
            "Epoch 14/300, Train Loss: 1.2124, Val Loss: 0.0125\n",
            "Epoch 15/300, Train Loss: 1.2175, Val Loss: 0.0146\n",
            "Epoch 16/300, Train Loss: 1.2137, Val Loss: 0.0125\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 1.2312, Val Loss: 0.0145\n",
            "Epoch 18/300, Train Loss: 1.2473, Val Loss: 0.0138\n",
            "Epoch 19/300, Train Loss: 1.2325, Val Loss: 0.0140\n",
            "Epoch 20/300, Train Loss: 1.2263, Val Loss: 0.0142\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 1.2199, Val Loss: 0.0139\n",
            "Epoch 22/300, Train Loss: 1.2275, Val Loss: 0.0123\n",
            "Epoch 23/300, Train Loss: 1.2304, Val Loss: 0.0116\n",
            "Epoch 24/300, Train Loss: 1.2204, Val Loss: 0.0112\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 1.2173, Val Loss: 0.0111\n",
            "Epoch 26/300, Train Loss: 1.2180, Val Loss: 0.0129\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 2.0481, Val Loss: 0.0065\n",
            "Epoch 2/300, Train Loss: 1.2858, Val Loss: 0.0135\n",
            "Epoch 3/300, Train Loss: 1.2427, Val Loss: 0.0104\n",
            "Epoch 4/300, Train Loss: 1.2583, Val Loss: 0.0134\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.2634, Val Loss: 0.0177\n",
            "Epoch 6/300, Train Loss: 1.2461, Val Loss: 0.0127\n",
            "Epoch 7/300, Train Loss: 1.2317, Val Loss: 0.0152\n",
            "Epoch 8/300, Train Loss: 1.2245, Val Loss: 0.0179\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 1.2183, Val Loss: 0.0165\n",
            "Epoch 10/300, Train Loss: 1.2334, Val Loss: 0.0166\n",
            "Epoch 11/300, Train Loss: 1.2422, Val Loss: 0.0191\n",
            "Epoch 12/300, Train Loss: 1.2270, Val Loss: 0.0144\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 1.2311, Val Loss: 0.0146\n",
            "Epoch 14/300, Train Loss: 1.2287, Val Loss: 0.0136\n",
            "Epoch 15/300, Train Loss: 1.2443, Val Loss: 0.0174\n",
            "Epoch 16/300, Train Loss: 1.2328, Val Loss: 0.0172\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 1.2407, Val Loss: 0.0183\n",
            "Epoch 18/300, Train Loss: 1.2371, Val Loss: 0.0189\n",
            "Epoch 19/300, Train Loss: 1.2320, Val Loss: 0.0157\n",
            "Epoch 20/300, Train Loss: 1.2260, Val Loss: 0.0183\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 1.2297, Val Loss: 0.0181\n",
            "Epoch 22/300, Train Loss: 1.2308, Val Loss: 0.0170\n",
            "Epoch 23/300, Train Loss: 1.2346, Val Loss: 0.0184\n",
            "Epoch 24/300, Train Loss: 1.2176, Val Loss: 0.0213\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 1.2229, Val Loss: 0.0223\n",
            "Epoch 26/300, Train Loss: 1.2100, Val Loss: 0.0199\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 0.0279, Val Loss: 0.0043\n",
            "Epoch 2/300, Train Loss: 0.0191, Val Loss: 0.0027\n",
            "Epoch 3/300, Train Loss: 0.0120, Val Loss: 0.0029\n",
            "Epoch 4/300, Train Loss: 0.0108, Val Loss: 0.0038\n",
            "Epoch 5/300, Train Loss: 0.0104, Val Loss: 0.0031\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 0.0098, Val Loss: 0.0061\n",
            "Epoch 7/300, Train Loss: 0.0077, Val Loss: 0.0023\n",
            "Epoch 8/300, Train Loss: 0.0083, Val Loss: 0.0036\n",
            "Epoch 9/300, Train Loss: 0.0078, Val Loss: 0.0025\n",
            "Epoch 10/300, Train Loss: 0.0072, Val Loss: 0.0022\n",
            "Epoch 11/300, Train Loss: 0.0055, Val Loss: 0.0018\n",
            "Epoch 12/300, Train Loss: 0.0061, Val Loss: 0.0019\n",
            "Epoch 13/300, Train Loss: 0.0064, Val Loss: 0.0020\n",
            "Epoch 14/300, Train Loss: 0.0057, Val Loss: 0.0018\n",
            "Epoch 00015: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 15/300, Train Loss: 0.0062, Val Loss: 0.0018\n",
            "Epoch 16/300, Train Loss: 0.0067, Val Loss: 0.0018\n",
            "Epoch 17/300, Train Loss: 0.0050, Val Loss: 0.0018\n",
            "Epoch 18/300, Train Loss: 0.0044, Val Loss: 0.0017\n",
            "Epoch 19/300, Train Loss: 0.0048, Val Loss: 0.0017\n",
            "Epoch 20/300, Train Loss: 0.0046, Val Loss: 0.0017\n",
            "Epoch 21/300, Train Loss: 0.0054, Val Loss: 0.0019\n",
            "Epoch 00022: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 22/300, Train Loss: 0.0043, Val Loss: 0.0019\n",
            "Epoch 23/300, Train Loss: 0.0047, Val Loss: 0.0018\n",
            "Epoch 24/300, Train Loss: 0.0045, Val Loss: 0.0017\n",
            "Epoch 25/300, Train Loss: 0.0041, Val Loss: 0.0017\n",
            "Epoch 26/300, Train Loss: 0.0049, Val Loss: 0.0017\n",
            "Epoch 27/300, Train Loss: 0.0043, Val Loss: 0.0018\n",
            "Epoch 28/300, Train Loss: 0.0041, Val Loss: 0.0016\n",
            "Epoch 29/300, Train Loss: 0.0042, Val Loss: 0.0016\n",
            "Epoch 30/300, Train Loss: 0.0038, Val Loss: 0.0016\n",
            "Epoch 31/300, Train Loss: 0.0050, Val Loss: 0.0016\n",
            "Epoch 32/300, Train Loss: 0.0038, Val Loss: 0.0016\n",
            "Epoch 33/300, Train Loss: 0.0037, Val Loss: 0.0016\n",
            "Epoch 34/300, Train Loss: 0.0045, Val Loss: 0.0017\n",
            "Epoch 35/300, Train Loss: 0.0041, Val Loss: 0.0016\n",
            "Epoch 36/300, Train Loss: 0.0039, Val Loss: 0.0016\n",
            "Epoch 00037: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 37/300, Train Loss: 0.0046, Val Loss: 0.0016\n",
            "Epoch 38/300, Train Loss: 0.0036, Val Loss: 0.0017\n",
            "Epoch 39/300, Train Loss: 0.0040, Val Loss: 0.0017\n",
            "Epoch 40/300, Train Loss: 0.0039, Val Loss: 0.0017\n",
            "Epoch 00041: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 41/300, Train Loss: 0.0037, Val Loss: 0.0016\n",
            "Epoch 42/300, Train Loss: 0.0039, Val Loss: 0.0016\n",
            "Epoch 43/300, Train Loss: 0.0035, Val Loss: 0.0017\n",
            "Epoch 44/300, Train Loss: 0.0038, Val Loss: 0.0016\n",
            "Epoch 00045: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 45/300, Train Loss: 0.0037, Val Loss: 0.0016\n",
            "Epoch 46/300, Train Loss: 0.0040, Val Loss: 0.0016\n",
            "Epoch 47/300, Train Loss: 0.0040, Val Loss: 0.0016\n",
            "Epoch 48/300, Train Loss: 0.0039, Val Loss: 0.0017\n",
            "Epoch 00049: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 49/300, Train Loss: 0.0037, Val Loss: 0.0016\n",
            "Epoch 50/300, Train Loss: 0.0044, Val Loss: 0.0016\n",
            "Epoch 51/300, Train Loss: 0.0038, Val Loss: 0.0017\n",
            "Epoch 52/300, Train Loss: 0.0037, Val Loss: 0.0016\n",
            "Epoch 00053: reducing learning rate of group 0 to 1.6777e-04.\n",
            "Epoch 53/300, Train Loss: 0.0043, Val Loss: 0.0016\n",
            "Epoch 54/300, Train Loss: 0.0035, Val Loss: 0.0016\n",
            "Epoch 55/300, Train Loss: 0.0036, Val Loss: 0.0016\n",
            "Epoch 56/300, Train Loss: 0.0040, Val Loss: 0.0016\n",
            "Epoch 00057: reducing learning rate of group 0 to 1.3422e-04.\n",
            "Epoch 57/300, Train Loss: 0.0038, Val Loss: 0.0016\n",
            "Epoch 58/300, Train Loss: 0.0035, Val Loss: 0.0016\n",
            "Early stopping triggered after 58 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2923, Val Loss: 0.0068\n",
            "Epoch 2/300, Train Loss: 1.2644, Val Loss: 0.0044\n",
            "Epoch 3/300, Train Loss: 1.2605, Val Loss: 0.0052\n",
            "Epoch 4/300, Train Loss: 1.2370, Val Loss: 0.0113\n",
            "Epoch 5/300, Train Loss: 1.2705, Val Loss: 0.0107\n",
            "Epoch 6/300, Train Loss: 1.2327, Val Loss: 0.0038\n",
            "Epoch 7/300, Train Loss: 1.2361, Val Loss: 0.0058\n",
            "Epoch 8/300, Train Loss: 1.2318, Val Loss: 0.0122\n",
            "Epoch 9/300, Train Loss: 1.2278, Val Loss: 0.0037\n",
            "Epoch 10/300, Train Loss: 1.2468, Val Loss: 0.0039\n",
            "Epoch 11/300, Train Loss: 1.2332, Val Loss: 0.0120\n",
            "Epoch 12/300, Train Loss: 1.2372, Val Loss: 0.0061\n",
            "Epoch 00013: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 13/300, Train Loss: 1.2629, Val Loss: 0.0139\n",
            "Epoch 14/300, Train Loss: 1.2279, Val Loss: 0.0077\n",
            "Epoch 15/300, Train Loss: 1.2275, Val Loss: 0.0048\n",
            "Epoch 16/300, Train Loss: 1.2319, Val Loss: 0.0058\n",
            "Epoch 00017: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 17/300, Train Loss: 1.2283, Val Loss: 0.0074\n",
            "Epoch 18/300, Train Loss: 1.2387, Val Loss: 0.0062\n",
            "Epoch 19/300, Train Loss: 1.2383, Val Loss: 0.0047\n",
            "Epoch 20/300, Train Loss: 1.2246, Val Loss: 0.0043\n",
            "Epoch 00021: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 21/300, Train Loss: 1.2373, Val Loss: 0.0041\n",
            "Epoch 22/300, Train Loss: 1.2304, Val Loss: 0.0071\n",
            "Epoch 23/300, Train Loss: 1.2317, Val Loss: 0.0069\n",
            "Epoch 24/300, Train Loss: 1.2328, Val Loss: 0.0064\n",
            "Epoch 00025: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 25/300, Train Loss: 1.9739, Val Loss: 0.0045\n",
            "Epoch 26/300, Train Loss: 1.2144, Val Loss: 0.0067\n",
            "Epoch 27/300, Train Loss: 1.2251, Val Loss: 0.0066\n",
            "Epoch 28/300, Train Loss: 1.2232, Val Loss: 0.0058\n",
            "Epoch 00029: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 29/300, Train Loss: 1.2171, Val Loss: 0.0066\n",
            "Epoch 30/300, Train Loss: 1.2268, Val Loss: 0.0064\n",
            "Epoch 31/300, Train Loss: 1.2244, Val Loss: 0.0060\n",
            "Epoch 32/300, Train Loss: 1.2244, Val Loss: 0.0061\n",
            "Epoch 00033: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 33/300, Train Loss: 1.9438, Val Loss: 0.0054\n",
            "Epoch 34/300, Train Loss: 1.2309, Val Loss: 0.0061\n",
            "Early stopping triggered after 34 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2829, Val Loss: 0.0118\n",
            "Epoch 2/300, Train Loss: 1.2539, Val Loss: 0.0070\n",
            "Epoch 3/300, Train Loss: 1.2619, Val Loss: 0.0062\n",
            "Epoch 4/300, Train Loss: 1.2239, Val Loss: 0.0094\n",
            "Epoch 5/300, Train Loss: 1.2301, Val Loss: 0.0080\n",
            "Epoch 6/300, Train Loss: 1.2608, Val Loss: 0.0129\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2309, Val Loss: 0.0082\n",
            "Epoch 8/300, Train Loss: 1.2491, Val Loss: 0.0050\n",
            "Epoch 9/300, Train Loss: 1.2245, Val Loss: 0.0043\n",
            "Epoch 10/300, Train Loss: 1.2270, Val Loss: 0.0043\n",
            "Epoch 11/300, Train Loss: 1.2394, Val Loss: 0.0098\n",
            "Epoch 12/300, Train Loss: 1.2323, Val Loss: 0.0070\n",
            "Epoch 13/300, Train Loss: 1.2485, Val Loss: 0.0034\n",
            "Epoch 14/300, Train Loss: 1.2323, Val Loss: 0.0078\n",
            "Epoch 15/300, Train Loss: 1.2335, Val Loss: 0.0113\n",
            "Epoch 16/300, Train Loss: 1.2267, Val Loss: 0.0069\n",
            "Epoch 00017: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 17/300, Train Loss: 1.2313, Val Loss: 0.0044\n",
            "Epoch 18/300, Train Loss: 1.2414, Val Loss: 0.0057\n",
            "Epoch 19/300, Train Loss: 1.2336, Val Loss: 0.0114\n",
            "Epoch 20/300, Train Loss: 1.2338, Val Loss: 0.0049\n",
            "Epoch 00021: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 21/300, Train Loss: 1.2173, Val Loss: 0.0067\n",
            "Epoch 22/300, Train Loss: 1.2132, Val Loss: 0.0074\n",
            "Epoch 23/300, Train Loss: 1.2161, Val Loss: 0.0052\n",
            "Epoch 24/300, Train Loss: 1.2242, Val Loss: 0.0084\n",
            "Epoch 00025: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 25/300, Train Loss: 1.2325, Val Loss: 0.0079\n",
            "Epoch 26/300, Train Loss: 1.2169, Val Loss: 0.0083\n",
            "Epoch 27/300, Train Loss: 1.2084, Val Loss: 0.0086\n",
            "Epoch 28/300, Train Loss: 1.2197, Val Loss: 0.0086\n",
            "Epoch 00029: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 29/300, Train Loss: 1.2170, Val Loss: 0.0103\n",
            "Epoch 30/300, Train Loss: 1.2208, Val Loss: 0.0079\n",
            "Epoch 31/300, Train Loss: 1.2026, Val Loss: 0.0072\n",
            "Epoch 32/300, Train Loss: 1.2173, Val Loss: 0.0095\n",
            "Epoch 00033: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 33/300, Train Loss: 1.2297, Val Loss: 0.0104\n",
            "Epoch 34/300, Train Loss: 1.2152, Val Loss: 0.0085\n",
            "Epoch 35/300, Train Loss: 1.9461, Val Loss: 0.0068\n",
            "Epoch 36/300, Train Loss: 1.2203, Val Loss: 0.0115\n",
            "Epoch 00037: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 37/300, Train Loss: 1.2211, Val Loss: 0.0093\n",
            "Epoch 38/300, Train Loss: 1.2259, Val Loss: 0.0093\n",
            "Early stopping triggered after 38 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2941, Val Loss: 0.0307\n",
            "Epoch 2/300, Train Loss: 1.2705, Val Loss: 0.0112\n",
            "Epoch 3/300, Train Loss: 1.2608, Val Loss: 0.0171\n",
            "Epoch 4/300, Train Loss: 1.2588, Val Loss: 0.0237\n",
            "Epoch 5/300, Train Loss: 1.2476, Val Loss: 0.0186\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2315, Val Loss: 0.0151\n",
            "Epoch 7/300, Train Loss: 1.2362, Val Loss: 0.0145\n",
            "Epoch 8/300, Train Loss: 1.2432, Val Loss: 0.0167\n",
            "Epoch 9/300, Train Loss: 1.2476, Val Loss: 0.0172\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2353, Val Loss: 0.0186\n",
            "Epoch 11/300, Train Loss: 1.2438, Val Loss: 0.0205\n",
            "Epoch 12/300, Train Loss: 1.2301, Val Loss: 0.0116\n",
            "Epoch 13/300, Train Loss: 1.2467, Val Loss: 0.0144\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 14/300, Train Loss: 1.2414, Val Loss: 0.0151\n",
            "Epoch 15/300, Train Loss: 1.2365, Val Loss: 0.0169\n",
            "Epoch 16/300, Train Loss: 1.2328, Val Loss: 0.0211\n",
            "Epoch 17/300, Train Loss: 1.2374, Val Loss: 0.0145\n",
            "Epoch 00018: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 18/300, Train Loss: 1.2409, Val Loss: 0.0169\n",
            "Epoch 19/300, Train Loss: 1.9671, Val Loss: 0.0114\n",
            "Epoch 20/300, Train Loss: 1.2179, Val Loss: 0.0175\n",
            "Epoch 21/300, Train Loss: 1.9494, Val Loss: 0.0131\n",
            "Epoch 00022: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 22/300, Train Loss: 1.2311, Val Loss: 0.0165\n",
            "Epoch 23/300, Train Loss: 1.2169, Val Loss: 0.0156\n",
            "Epoch 24/300, Train Loss: 1.2259, Val Loss: 0.0170\n",
            "Epoch 25/300, Train Loss: 1.9549, Val Loss: 0.0131\n",
            "Epoch 00026: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 26/300, Train Loss: 1.2335, Val Loss: 0.0161\n",
            "Epoch 27/300, Train Loss: 1.2134, Val Loss: 0.0161\n",
            "Early stopping triggered after 27 epochs!\n",
            "Epoch 1/300, Train Loss: 0.0608, Val Loss: 0.0026\n",
            "Epoch 2/300, Train Loss: 0.0185, Val Loss: 0.0189\n",
            "Epoch 3/300, Train Loss: 0.0172, Val Loss: 0.0046\n",
            "Epoch 4/300, Train Loss: 0.0136, Val Loss: 0.0089\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 0.0128, Val Loss: 0.0079\n",
            "Epoch 6/300, Train Loss: 0.0136, Val Loss: 0.0074\n",
            "Epoch 7/300, Train Loss: 0.0095, Val Loss: 0.0100\n",
            "Epoch 8/300, Train Loss: 0.0085, Val Loss: 0.0077\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 0.0070, Val Loss: 0.0069\n",
            "Epoch 10/300, Train Loss: 0.0073, Val Loss: 0.0056\n",
            "Epoch 11/300, Train Loss: 0.0064, Val Loss: 0.0056\n",
            "Epoch 12/300, Train Loss: 0.0059, Val Loss: 0.0059\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 0.0072, Val Loss: 0.0055\n",
            "Epoch 14/300, Train Loss: 0.0062, Val Loss: 0.0056\n",
            "Epoch 15/300, Train Loss: 0.0093, Val Loss: 0.0067\n",
            "Epoch 16/300, Train Loss: 0.0061, Val Loss: 0.0063\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 0.0052, Val Loss: 0.0057\n",
            "Epoch 18/300, Train Loss: 0.0049, Val Loss: 0.0059\n",
            "Epoch 19/300, Train Loss: 0.0053, Val Loss: 0.0058\n",
            "Epoch 20/300, Train Loss: 0.0058, Val Loss: 0.0064\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 0.0054, Val Loss: 0.0062\n",
            "Epoch 22/300, Train Loss: 0.0056, Val Loss: 0.0059\n",
            "Epoch 23/300, Train Loss: 0.0054, Val Loss: 0.0055\n",
            "Epoch 24/300, Train Loss: 0.0046, Val Loss: 0.0053\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 0.0045, Val Loss: 0.0049\n",
            "Epoch 26/300, Train Loss: 0.0051, Val Loss: 0.0052\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 0.0958, Val Loss: 7.9805\n",
            "Epoch 2/300, Train Loss: 0.0590, Val Loss: 7.7756\n",
            "Epoch 3/300, Train Loss: 0.0360, Val Loss: 7.7963\n",
            "Epoch 4/300, Train Loss: 0.0159, Val Loss: 7.8002\n",
            "Epoch 5/300, Train Loss: 0.0212, Val Loss: 7.8225\n",
            "Epoch 6/300, Train Loss: 0.0122, Val Loss: 7.7630\n",
            "Epoch 7/300, Train Loss: 0.0184, Val Loss: 7.8108\n",
            "Epoch 8/300, Train Loss: 0.0143, Val Loss: 7.7764\n",
            "Epoch 9/300, Train Loss: 0.0087, Val Loss: 7.7914\n",
            "Epoch 00010: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 10/300, Train Loss: 0.0097, Val Loss: 7.7830\n",
            "Epoch 11/300, Train Loss: 0.0091, Val Loss: 7.7824\n",
            "Epoch 12/300, Train Loss: 0.0099, Val Loss: 7.7972\n",
            "Epoch 13/300, Train Loss: 0.0070, Val Loss: 7.7923\n",
            "Epoch 00014: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 14/300, Train Loss: 0.0063, Val Loss: 7.7842\n",
            "Epoch 15/300, Train Loss: 0.0073, Val Loss: 7.7816\n",
            "Epoch 16/300, Train Loss: 0.0068, Val Loss: 7.8038\n",
            "Epoch 17/300, Train Loss: 0.0056, Val Loss: 7.7819\n",
            "Epoch 00018: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 18/300, Train Loss: 0.0063, Val Loss: 7.7848\n",
            "Epoch 19/300, Train Loss: 0.0067, Val Loss: 7.7916\n",
            "Epoch 20/300, Train Loss: 0.0066, Val Loss: 7.7816\n",
            "Epoch 21/300, Train Loss: 0.0055, Val Loss: 7.7811\n",
            "Epoch 00022: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 22/300, Train Loss: 0.0047, Val Loss: 7.7858\n",
            "Epoch 23/300, Train Loss: 0.0055, Val Loss: 7.7830\n",
            "Epoch 24/300, Train Loss: 0.0053, Val Loss: 7.7812\n",
            "Epoch 25/300, Train Loss: 0.0057, Val Loss: 7.7776\n",
            "Epoch 00026: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 26/300, Train Loss: 0.0052, Val Loss: 7.7782\n",
            "Epoch 27/300, Train Loss: 0.0073, Val Loss: 7.7770\n",
            "Epoch 28/300, Train Loss: 0.0057, Val Loss: 7.7787\n",
            "Epoch 29/300, Train Loss: 0.0050, Val Loss: 7.7779\n",
            "Epoch 00030: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 30/300, Train Loss: 0.0051, Val Loss: 7.7667\n",
            "Epoch 31/300, Train Loss: 0.0048, Val Loss: 7.7725\n",
            "Early stopping triggered after 31 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2533, Val Loss: 0.0117\n",
            "Epoch 2/300, Train Loss: 1.2439, Val Loss: 0.0053\n",
            "Epoch 3/300, Train Loss: 1.2420, Val Loss: 0.0081\n",
            "Epoch 4/300, Train Loss: 1.2386, Val Loss: 0.0039\n",
            "Epoch 5/300, Train Loss: 1.2344, Val Loss: 0.0172\n",
            "Epoch 6/300, Train Loss: 1.2257, Val Loss: 0.0171\n",
            "Epoch 7/300, Train Loss: 1.2370, Val Loss: 0.0145\n",
            "Epoch 00008: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 8/300, Train Loss: 1.2338, Val Loss: 0.0135\n",
            "Epoch 9/300, Train Loss: 1.2365, Val Loss: 0.0146\n",
            "Epoch 10/300, Train Loss: 1.2323, Val Loss: 0.0209\n",
            "Epoch 11/300, Train Loss: 1.2379, Val Loss: 0.0093\n",
            "Epoch 00012: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 12/300, Train Loss: 1.2252, Val Loss: 0.0112\n",
            "Epoch 13/300, Train Loss: 1.2289, Val Loss: 0.0116\n",
            "Epoch 14/300, Train Loss: 1.9406, Val Loss: 0.0070\n",
            "Epoch 15/300, Train Loss: 1.2361, Val Loss: 0.0214\n",
            "Epoch 00016: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 16/300, Train Loss: 1.2175, Val Loss: 0.0134\n",
            "Epoch 17/300, Train Loss: 1.2307, Val Loss: 0.0165\n",
            "Epoch 18/300, Train Loss: 1.2215, Val Loss: 0.0169\n",
            "Epoch 19/300, Train Loss: 1.2260, Val Loss: 0.0128\n",
            "Epoch 00020: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 20/300, Train Loss: 1.2246, Val Loss: 0.0116\n",
            "Epoch 21/300, Train Loss: 1.2178, Val Loss: 0.0172\n",
            "Epoch 22/300, Train Loss: 1.2251, Val Loss: 0.0194\n",
            "Epoch 23/300, Train Loss: 1.9515, Val Loss: 0.0124\n",
            "Epoch 00024: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 24/300, Train Loss: 1.2115, Val Loss: 0.0239\n",
            "Epoch 25/300, Train Loss: 1.2196, Val Loss: 0.0214\n",
            "Epoch 26/300, Train Loss: 1.2106, Val Loss: 0.0192\n",
            "Epoch 27/300, Train Loss: 1.9637, Val Loss: 0.0173\n",
            "Epoch 00028: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 28/300, Train Loss: 1.2218, Val Loss: 0.0226\n",
            "Epoch 29/300, Train Loss: 1.2278, Val Loss: 0.0198\n",
            "Early stopping triggered after 29 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3755, Val Loss: 0.0554\n",
            "Epoch 2/300, Train Loss: 1.2883, Val Loss: 0.0059\n",
            "Epoch 3/300, Train Loss: 1.2631, Val Loss: 0.0039\n",
            "Epoch 4/300, Train Loss: 1.2278, Val Loss: 0.0059\n",
            "Epoch 5/300, Train Loss: 1.2474, Val Loss: 0.0143\n",
            "Epoch 6/300, Train Loss: 1.2428, Val Loss: 0.0047\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2328, Val Loss: 0.0049\n",
            "Epoch 8/300, Train Loss: 1.2307, Val Loss: 0.0206\n",
            "Epoch 9/300, Train Loss: 1.2258, Val Loss: 0.0102\n",
            "Epoch 10/300, Train Loss: 1.2569, Val Loss: 0.0074\n",
            "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2421, Val Loss: 0.0067\n",
            "Epoch 12/300, Train Loss: 1.2198, Val Loss: 0.0117\n",
            "Epoch 13/300, Train Loss: 1.2377, Val Loss: 0.0069\n",
            "Epoch 14/300, Train Loss: 1.2278, Val Loss: 0.0110\n",
            "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 15/300, Train Loss: 1.2516, Val Loss: 0.0124\n",
            "Epoch 16/300, Train Loss: 1.2254, Val Loss: 0.0094\n",
            "Epoch 17/300, Train Loss: 1.2496, Val Loss: 0.0125\n",
            "Epoch 18/300, Train Loss: 1.9626, Val Loss: 0.0092\n",
            "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 19/300, Train Loss: 1.2426, Val Loss: 0.0132\n",
            "Epoch 20/300, Train Loss: 1.2420, Val Loss: 0.0125\n",
            "Epoch 21/300, Train Loss: 1.2240, Val Loss: 0.0116\n",
            "Epoch 22/300, Train Loss: 1.2215, Val Loss: 0.0138\n",
            "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 23/300, Train Loss: 1.2556, Val Loss: 0.0125\n",
            "Epoch 24/300, Train Loss: 1.2372, Val Loss: 0.0114\n",
            "Epoch 25/300, Train Loss: 1.2252, Val Loss: 0.0124\n",
            "Epoch 26/300, Train Loss: 1.2404, Val Loss: 0.0125\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 27/300, Train Loss: 1.2397, Val Loss: 0.0113\n",
            "Epoch 28/300, Train Loss: 1.2289, Val Loss: 0.0121\n",
            "Early stopping triggered after 28 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2639, Val Loss: 0.0173\n",
            "Epoch 2/300, Train Loss: 1.2203, Val Loss: 0.0119\n",
            "Epoch 3/300, Train Loss: 1.2386, Val Loss: 0.0142\n",
            "Epoch 4/300, Train Loss: 1.2267, Val Loss: 0.0110\n",
            "Epoch 5/300, Train Loss: 1.2411, Val Loss: 0.0132\n",
            "Epoch 6/300, Train Loss: 1.2632, Val Loss: 0.0110\n",
            "Epoch 7/300, Train Loss: 1.2189, Val Loss: 0.0266\n",
            "Epoch 8/300, Train Loss: 1.2244, Val Loss: 0.0136\n",
            "Epoch 9/300, Train Loss: 1.2489, Val Loss: 0.0121\n",
            "Epoch 00010: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2399, Val Loss: 0.0114\n",
            "Epoch 11/300, Train Loss: 1.2376, Val Loss: 0.0276\n",
            "Epoch 12/300, Train Loss: 1.2247, Val Loss: 0.0257\n",
            "Epoch 13/300, Train Loss: 1.2373, Val Loss: 0.0085\n",
            "Epoch 14/300, Train Loss: 1.2300, Val Loss: 0.0207\n",
            "Epoch 15/300, Train Loss: 1.2260, Val Loss: 0.0175\n",
            "Epoch 16/300, Train Loss: 1.2443, Val Loss: 0.0383\n",
            "Epoch 00017: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 17/300, Train Loss: 1.2416, Val Loss: 0.0256\n",
            "Epoch 18/300, Train Loss: 1.2357, Val Loss: 0.0182\n",
            "Epoch 19/300, Train Loss: 1.9633, Val Loss: 0.0175\n",
            "Epoch 20/300, Train Loss: 1.2211, Val Loss: 0.0332\n",
            "Epoch 00021: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 21/300, Train Loss: 1.2385, Val Loss: 0.0179\n",
            "Epoch 22/300, Train Loss: 1.1998, Val Loss: 0.0188\n",
            "Epoch 23/300, Train Loss: 1.2405, Val Loss: 0.0227\n",
            "Epoch 24/300, Train Loss: 1.2339, Val Loss: 0.0238\n",
            "Epoch 00025: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 25/300, Train Loss: 1.2344, Val Loss: 0.0201\n",
            "Epoch 26/300, Train Loss: 1.2204, Val Loss: 0.0168\n",
            "Epoch 27/300, Train Loss: 1.2377, Val Loss: 0.0197\n",
            "Epoch 28/300, Train Loss: 1.2179, Val Loss: 0.0172\n",
            "Epoch 00029: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 29/300, Train Loss: 1.2230, Val Loss: 0.0152\n",
            "Epoch 30/300, Train Loss: 1.2428, Val Loss: 0.0153\n",
            "Epoch 31/300, Train Loss: 1.2309, Val Loss: 0.0163\n",
            "Epoch 32/300, Train Loss: 1.2256, Val Loss: 0.0145\n",
            "Epoch 00033: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 33/300, Train Loss: 1.2195, Val Loss: 0.0132\n",
            "Epoch 34/300, Train Loss: 1.2347, Val Loss: 0.0146\n",
            "Epoch 35/300, Train Loss: 1.2371, Val Loss: 0.0143\n",
            "Epoch 36/300, Train Loss: 1.2251, Val Loss: 0.0140\n",
            "Epoch 00037: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 37/300, Train Loss: 1.2253, Val Loss: 0.0132\n",
            "Epoch 38/300, Train Loss: 1.2219, Val Loss: 0.0150\n",
            "Early stopping triggered after 38 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2923, Val Loss: 0.0197\n",
            "Epoch 2/300, Train Loss: 1.2864, Val Loss: 0.0048\n",
            "Epoch 3/300, Train Loss: 1.2337, Val Loss: 0.0124\n",
            "Epoch 4/300, Train Loss: 1.2531, Val Loss: 0.0115\n",
            "Epoch 5/300, Train Loss: 1.2550, Val Loss: 0.0161\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2445, Val Loss: 0.0171\n",
            "Epoch 7/300, Train Loss: 1.2675, Val Loss: 0.0165\n",
            "Epoch 8/300, Train Loss: 1.2515, Val Loss: 0.0219\n",
            "Epoch 9/300, Train Loss: 1.2218, Val Loss: 0.0155\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2349, Val Loss: 0.0220\n",
            "Epoch 11/300, Train Loss: 1.2219, Val Loss: 0.0218\n",
            "Epoch 12/300, Train Loss: 1.2321, Val Loss: 0.0207\n",
            "Epoch 13/300, Train Loss: 1.2238, Val Loss: 0.0146\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 14/300, Train Loss: 1.2251, Val Loss: 0.0267\n",
            "Epoch 15/300, Train Loss: 1.2231, Val Loss: 0.0302\n",
            "Epoch 16/300, Train Loss: 1.2339, Val Loss: 0.0210\n",
            "Epoch 17/300, Train Loss: 1.2407, Val Loss: 0.0256\n",
            "Epoch 00018: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 18/300, Train Loss: 1.2523, Val Loss: 0.0268\n",
            "Epoch 19/300, Train Loss: 1.2149, Val Loss: 0.0238\n",
            "Epoch 20/300, Train Loss: 1.2189, Val Loss: 0.0261\n",
            "Epoch 21/300, Train Loss: 1.2158, Val Loss: 0.0279\n",
            "Epoch 00022: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 22/300, Train Loss: 1.2164, Val Loss: 0.0276\n",
            "Epoch 23/300, Train Loss: 1.2176, Val Loss: 0.0313\n",
            "Epoch 24/300, Train Loss: 1.2200, Val Loss: 0.0250\n",
            "Epoch 25/300, Train Loss: 1.9383, Val Loss: 0.0239\n",
            "Epoch 00026: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 26/300, Train Loss: 1.2290, Val Loss: 0.0293\n",
            "Epoch 27/300, Train Loss: 1.2163, Val Loss: 0.0240\n",
            "Early stopping triggered after 27 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2920, Val Loss: 0.0149\n",
            "Epoch 2/300, Train Loss: 1.2634, Val Loss: 0.0033\n",
            "Epoch 3/300, Train Loss: 1.2519, Val Loss: 0.0170\n",
            "Epoch 4/300, Train Loss: 1.2691, Val Loss: 0.0187\n",
            "Epoch 5/300, Train Loss: 1.2467, Val Loss: 0.0079\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2349, Val Loss: 0.0157\n",
            "Epoch 7/300, Train Loss: 1.2289, Val Loss: 0.0241\n",
            "Epoch 8/300, Train Loss: 1.2276, Val Loss: 0.0114\n",
            "Epoch 9/300, Train Loss: 1.2391, Val Loss: 0.0176\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2418, Val Loss: 0.0213\n",
            "Epoch 11/300, Train Loss: 1.2455, Val Loss: 0.0173\n",
            "Epoch 12/300, Train Loss: 1.2278, Val Loss: 0.0261\n",
            "Epoch 13/300, Train Loss: 1.9687, Val Loss: 0.0147\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 14/300, Train Loss: 1.2285, Val Loss: 0.0233\n",
            "Epoch 15/300, Train Loss: 1.2304, Val Loss: 0.0207\n",
            "Epoch 16/300, Train Loss: 1.2425, Val Loss: 0.0159\n",
            "Epoch 17/300, Train Loss: 1.2249, Val Loss: 0.0145\n",
            "Epoch 00018: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 18/300, Train Loss: 1.2243, Val Loss: 0.0137\n",
            "Epoch 19/300, Train Loss: 1.2248, Val Loss: 0.0126\n",
            "Epoch 20/300, Train Loss: 1.2262, Val Loss: 0.0155\n",
            "Epoch 21/300, Train Loss: 1.2319, Val Loss: 0.0131\n",
            "Epoch 00022: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 22/300, Train Loss: 1.2260, Val Loss: 0.0141\n",
            "Epoch 23/300, Train Loss: 1.2346, Val Loss: 0.0102\n",
            "Epoch 24/300, Train Loss: 1.2200, Val Loss: 0.0116\n",
            "Epoch 25/300, Train Loss: 1.2239, Val Loss: 0.0117\n",
            "Epoch 00026: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 26/300, Train Loss: 1.2516, Val Loss: 0.0154\n",
            "Epoch 27/300, Train Loss: 1.2355, Val Loss: 0.0155\n",
            "Early stopping triggered after 27 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2732, Val Loss: 0.0090\n",
            "Epoch 2/300, Train Loss: 1.2397, Val Loss: 0.0177\n",
            "Epoch 3/300, Train Loss: 1.2549, Val Loss: 0.0066\n",
            "Epoch 4/300, Train Loss: 1.2441, Val Loss: 0.0144\n",
            "Epoch 5/300, Train Loss: 1.2538, Val Loss: 0.0102\n",
            "Epoch 6/300, Train Loss: 1.2443, Val Loss: 0.0084\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2544, Val Loss: 0.0073\n",
            "Epoch 8/300, Train Loss: 1.2320, Val Loss: 0.0047\n",
            "Epoch 9/300, Train Loss: 1.2490, Val Loss: 0.0053\n",
            "Epoch 10/300, Train Loss: 1.2255, Val Loss: 0.0050\n",
            "Epoch 11/300, Train Loss: 1.2290, Val Loss: 0.0028\n",
            "Epoch 12/300, Train Loss: 1.2370, Val Loss: 0.0067\n",
            "Epoch 13/300, Train Loss: 1.2326, Val Loss: 0.0088\n",
            "Epoch 14/300, Train Loss: 1.2216, Val Loss: 0.0060\n",
            "Epoch 00015: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 15/300, Train Loss: 1.2200, Val Loss: 0.0042\n",
            "Epoch 16/300, Train Loss: 1.2180, Val Loss: 0.0061\n",
            "Epoch 17/300, Train Loss: 1.2312, Val Loss: 0.0088\n",
            "Epoch 18/300, Train Loss: 1.2262, Val Loss: 0.0077\n",
            "Epoch 00019: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 19/300, Train Loss: 1.2422, Val Loss: 0.0078\n",
            "Epoch 20/300, Train Loss: 1.2240, Val Loss: 0.0047\n",
            "Epoch 21/300, Train Loss: 1.2337, Val Loss: 0.0084\n",
            "Epoch 22/300, Train Loss: 1.2290, Val Loss: 0.0089\n",
            "Epoch 00023: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 23/300, Train Loss: 1.2288, Val Loss: 0.0094\n",
            "Epoch 24/300, Train Loss: 1.2273, Val Loss: 0.0089\n",
            "Epoch 25/300, Train Loss: 1.2161, Val Loss: 0.0090\n",
            "Epoch 26/300, Train Loss: 1.2267, Val Loss: 0.0077\n",
            "Epoch 00027: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 27/300, Train Loss: 1.2274, Val Loss: 0.0075\n",
            "Epoch 28/300, Train Loss: 1.2295, Val Loss: 0.0095\n",
            "Epoch 29/300, Train Loss: 1.2329, Val Loss: 0.0090\n",
            "Epoch 30/300, Train Loss: 1.2344, Val Loss: 0.0098\n",
            "Epoch 00031: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 31/300, Train Loss: 1.2254, Val Loss: 0.0095\n",
            "Epoch 32/300, Train Loss: 1.2288, Val Loss: 0.0081\n",
            "Epoch 33/300, Train Loss: 1.2337, Val Loss: 0.0096\n",
            "Epoch 34/300, Train Loss: 1.2378, Val Loss: 0.0083\n",
            "Epoch 00035: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 35/300, Train Loss: 1.2277, Val Loss: 0.0102\n",
            "Epoch 36/300, Train Loss: 1.2357, Val Loss: 0.0092\n",
            "Early stopping triggered after 36 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2759, Val Loss: 0.0065\n",
            "Epoch 2/300, Train Loss: 1.2634, Val Loss: 0.0048\n",
            "Epoch 3/300, Train Loss: 1.3074, Val Loss: 0.0037\n",
            "Epoch 4/300, Train Loss: 1.2213, Val Loss: 0.0039\n",
            "Epoch 5/300, Train Loss: 1.2764, Val Loss: 0.0042\n",
            "Epoch 6/300, Train Loss: 1.2512, Val Loss: 0.0081\n",
            "Epoch 7/300, Train Loss: 1.2299, Val Loss: 0.0033\n",
            "Epoch 8/300, Train Loss: 1.2374, Val Loss: 0.0047\n",
            "Epoch 9/300, Train Loss: 1.2427, Val Loss: 0.0049\n",
            "Epoch 10/300, Train Loss: 1.2530, Val Loss: 0.0105\n",
            "Epoch 00011: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2307, Val Loss: 0.0075\n",
            "Epoch 12/300, Train Loss: 1.2278, Val Loss: 0.0053\n",
            "Epoch 13/300, Train Loss: 1.2434, Val Loss: 0.0134\n",
            "Epoch 14/300, Train Loss: 1.2373, Val Loss: 0.0089\n",
            "Epoch 00015: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 15/300, Train Loss: 1.2354, Val Loss: 0.0064\n",
            "Epoch 16/300, Train Loss: 1.2272, Val Loss: 0.0039\n",
            "Epoch 17/300, Train Loss: 1.2305, Val Loss: 0.0075\n",
            "Epoch 18/300, Train Loss: 1.2536, Val Loss: 0.0111\n",
            "Epoch 00019: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 19/300, Train Loss: 1.2410, Val Loss: 0.0089\n",
            "Epoch 20/300, Train Loss: 1.2163, Val Loss: 0.0057\n",
            "Epoch 21/300, Train Loss: 1.9703, Val Loss: 0.0060\n",
            "Epoch 22/300, Train Loss: 1.2437, Val Loss: 0.0092\n",
            "Epoch 00023: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 23/300, Train Loss: 1.2367, Val Loss: 0.0090\n",
            "Epoch 24/300, Train Loss: 1.2227, Val Loss: 0.0074\n",
            "Epoch 25/300, Train Loss: 1.2274, Val Loss: 0.0067\n",
            "Epoch 26/300, Train Loss: 1.9489, Val Loss: 0.0049\n",
            "Epoch 00027: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 27/300, Train Loss: 1.2427, Val Loss: 0.0113\n",
            "Epoch 28/300, Train Loss: 1.2256, Val Loss: 0.0092\n",
            "Epoch 29/300, Train Loss: 1.2329, Val Loss: 0.0094\n",
            "Epoch 30/300, Train Loss: 1.2158, Val Loss: 0.0100\n",
            "Epoch 00031: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 31/300, Train Loss: 1.2283, Val Loss: 0.0085\n",
            "Epoch 32/300, Train Loss: 1.2425, Val Loss: 0.0101\n",
            "Early stopping triggered after 32 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2636, Val Loss: 0.0179\n",
            "Epoch 2/300, Train Loss: 1.2442, Val Loss: 0.0178\n",
            "Epoch 3/300, Train Loss: 1.2055, Val Loss: 0.0249\n",
            "Epoch 4/300, Train Loss: 1.2428, Val Loss: 0.0226\n",
            "Epoch 5/300, Train Loss: 1.2498, Val Loss: 0.0276\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2354, Val Loss: 0.0211\n",
            "Epoch 7/300, Train Loss: 1.2188, Val Loss: 0.0203\n",
            "Epoch 8/300, Train Loss: 1.2339, Val Loss: 0.0212\n",
            "Epoch 9/300, Train Loss: 1.2360, Val Loss: 0.0178\n",
            "Epoch 10/300, Train Loss: 1.2164, Val Loss: 0.0248\n",
            "Epoch 11/300, Train Loss: 1.2589, Val Loss: 0.0176\n",
            "Epoch 12/300, Train Loss: 1.2319, Val Loss: 0.0221\n",
            "Epoch 13/300, Train Loss: 1.2448, Val Loss: 0.0180\n",
            "Epoch 14/300, Train Loss: 1.2266, Val Loss: 0.0160\n",
            "Epoch 15/300, Train Loss: 1.9594, Val Loss: 0.0139\n",
            "Epoch 16/300, Train Loss: 1.2293, Val Loss: 0.0239\n",
            "Epoch 17/300, Train Loss: 1.2305, Val Loss: 0.0175\n",
            "Epoch 18/300, Train Loss: 1.2195, Val Loss: 0.0154\n",
            "Epoch 00019: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 19/300, Train Loss: 1.2311, Val Loss: 0.0189\n",
            "Epoch 20/300, Train Loss: 1.2318, Val Loss: 0.0177\n",
            "Epoch 21/300, Train Loss: 1.2275, Val Loss: 0.0184\n",
            "Epoch 22/300, Train Loss: 1.2387, Val Loss: 0.0159\n",
            "Epoch 00023: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 23/300, Train Loss: 1.2389, Val Loss: 0.0192\n",
            "Epoch 24/300, Train Loss: 1.2243, Val Loss: 0.0179\n",
            "Epoch 25/300, Train Loss: 1.2169, Val Loss: 0.0155\n",
            "Epoch 26/300, Train Loss: 1.2330, Val Loss: 0.0179\n",
            "Epoch 00027: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 27/300, Train Loss: 1.2223, Val Loss: 0.0166\n",
            "Epoch 28/300, Train Loss: 1.2228, Val Loss: 0.0154\n",
            "Epoch 29/300, Train Loss: 1.2137, Val Loss: 0.0157\n",
            "Epoch 30/300, Train Loss: 1.2288, Val Loss: 0.0176\n",
            "Epoch 00031: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 31/300, Train Loss: 1.2322, Val Loss: 0.0165\n",
            "Epoch 32/300, Train Loss: 1.2164, Val Loss: 0.0170\n",
            "Epoch 33/300, Train Loss: 1.2247, Val Loss: 0.0176\n",
            "Epoch 34/300, Train Loss: 1.2276, Val Loss: 0.0169\n",
            "Epoch 00035: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 35/300, Train Loss: 1.2079, Val Loss: 0.0162\n",
            "Epoch 36/300, Train Loss: 1.2191, Val Loss: 0.0170\n",
            "Epoch 37/300, Train Loss: 1.2084, Val Loss: 0.0185\n",
            "Epoch 38/300, Train Loss: 1.2144, Val Loss: 0.0190\n",
            "Epoch 00039: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 39/300, Train Loss: 1.2203, Val Loss: 0.0182\n",
            "Epoch 40/300, Train Loss: 1.2284, Val Loss: 0.0180\n",
            "Early stopping triggered after 40 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3065, Val Loss: 0.0153\n",
            "Epoch 2/300, Train Loss: 1.2891, Val Loss: 0.0145\n",
            "Epoch 3/300, Train Loss: 1.2462, Val Loss: 0.0123\n",
            "Epoch 4/300, Train Loss: 1.2422, Val Loss: 0.0126\n",
            "Epoch 5/300, Train Loss: 1.2744, Val Loss: 0.0245\n",
            "Epoch 6/300, Train Loss: 1.2867, Val Loss: 0.0081\n",
            "Epoch 7/300, Train Loss: 1.2383, Val Loss: 0.0060\n",
            "Epoch 8/300, Train Loss: 1.2411, Val Loss: 0.0035\n",
            "Epoch 9/300, Train Loss: 1.2235, Val Loss: 0.0096\n",
            "Epoch 10/300, Train Loss: 1.2515, Val Loss: 0.0152\n",
            "Epoch 11/300, Train Loss: 2.0003, Val Loss: 0.0026\n",
            "Epoch 12/300, Train Loss: 1.2384, Val Loss: 0.0166\n",
            "Epoch 13/300, Train Loss: 1.2320, Val Loss: 0.0131\n",
            "Epoch 14/300, Train Loss: 1.2139, Val Loss: 0.0133\n",
            "Epoch 00015: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 15/300, Train Loss: 1.2097, Val Loss: 0.0079\n",
            "Epoch 16/300, Train Loss: 1.2362, Val Loss: 0.0054\n",
            "Epoch 17/300, Train Loss: 1.2208, Val Loss: 0.0102\n",
            "Epoch 18/300, Train Loss: 1.2281, Val Loss: 0.0094\n",
            "Epoch 00019: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 19/300, Train Loss: 1.2516, Val Loss: 0.0036\n",
            "Epoch 20/300, Train Loss: 1.2388, Val Loss: 0.0075\n",
            "Epoch 21/300, Train Loss: 1.9639, Val Loss: 0.0063\n",
            "Epoch 22/300, Train Loss: 1.2335, Val Loss: 0.0135\n",
            "Epoch 00023: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 23/300, Train Loss: 1.2042, Val Loss: 0.0112\n",
            "Epoch 24/300, Train Loss: 1.2374, Val Loss: 0.0084\n",
            "Epoch 25/300, Train Loss: 1.2366, Val Loss: 0.0154\n",
            "Epoch 26/300, Train Loss: 1.2123, Val Loss: 0.0135\n",
            "Epoch 00027: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 27/300, Train Loss: 1.2304, Val Loss: 0.0111\n",
            "Epoch 28/300, Train Loss: 1.2362, Val Loss: 0.0108\n",
            "Epoch 29/300, Train Loss: 1.2324, Val Loss: 0.0098\n",
            "Epoch 30/300, Train Loss: 1.2252, Val Loss: 0.0110\n",
            "Epoch 00031: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 31/300, Train Loss: 1.2326, Val Loss: 0.0100\n",
            "Epoch 32/300, Train Loss: 1.2125, Val Loss: 0.0116\n",
            "Epoch 33/300, Train Loss: 1.2275, Val Loss: 0.0121\n",
            "Epoch 34/300, Train Loss: 1.2225, Val Loss: 0.0078\n",
            "Epoch 00035: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 35/300, Train Loss: 1.2283, Val Loss: 0.0086\n",
            "Epoch 36/300, Train Loss: 1.2220, Val Loss: 0.0092\n",
            "Early stopping triggered after 36 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2397, Val Loss: 0.0092\n",
            "Epoch 2/300, Train Loss: 1.2608, Val Loss: 0.0169\n",
            "Epoch 3/300, Train Loss: 1.2395, Val Loss: 0.0088\n",
            "Epoch 4/300, Train Loss: 1.2453, Val Loss: 0.0155\n",
            "Epoch 5/300, Train Loss: 1.2305, Val Loss: 0.0146\n",
            "Epoch 6/300, Train Loss: 1.2271, Val Loss: 0.0125\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2238, Val Loss: 0.0122\n",
            "Epoch 8/300, Train Loss: 1.2237, Val Loss: 0.0136\n",
            "Epoch 9/300, Train Loss: 1.2306, Val Loss: 0.0111\n",
            "Epoch 10/300, Train Loss: 1.2325, Val Loss: 0.0151\n",
            "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2148, Val Loss: 0.0118\n",
            "Epoch 12/300, Train Loss: 1.2200, Val Loss: 0.0158\n",
            "Epoch 13/300, Train Loss: 1.9492, Val Loss: 0.0123\n",
            "Epoch 14/300, Train Loss: 1.2392, Val Loss: 0.0216\n",
            "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 15/300, Train Loss: 1.2280, Val Loss: 0.0169\n",
            "Epoch 16/300, Train Loss: 1.2340, Val Loss: 0.0175\n",
            "Epoch 17/300, Train Loss: 1.2399, Val Loss: 0.0213\n",
            "Epoch 18/300, Train Loss: 1.2313, Val Loss: 0.0236\n",
            "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 19/300, Train Loss: 1.2264, Val Loss: 0.0209\n",
            "Epoch 20/300, Train Loss: 1.2131, Val Loss: 0.0182\n",
            "Epoch 21/300, Train Loss: 1.2151, Val Loss: 0.0194\n",
            "Epoch 22/300, Train Loss: 1.2162, Val Loss: 0.0192\n",
            "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 23/300, Train Loss: 1.2215, Val Loss: 0.0179\n",
            "Epoch 24/300, Train Loss: 1.2195, Val Loss: 0.0217\n",
            "Epoch 25/300, Train Loss: 1.2134, Val Loss: 0.0251\n",
            "Epoch 26/300, Train Loss: 1.2180, Val Loss: 0.0223\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 27/300, Train Loss: 1.2242, Val Loss: 0.0216\n",
            "Epoch 28/300, Train Loss: 1.2171, Val Loss: 0.0212\n",
            "Early stopping triggered after 28 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2587, Val Loss: 0.0085\n",
            "Epoch 2/300, Train Loss: 1.2446, Val Loss: 0.0053\n",
            "Epoch 3/300, Train Loss: 1.2687, Val Loss: 0.0051\n",
            "Epoch 4/300, Train Loss: 1.2455, Val Loss: 0.0078\n",
            "Epoch 5/300, Train Loss: 1.2194, Val Loss: 0.0075\n",
            "Epoch 6/300, Train Loss: 1.2314, Val Loss: 0.0077\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2230, Val Loss: 0.0074\n",
            "Epoch 8/300, Train Loss: 1.2178, Val Loss: 0.0093\n",
            "Epoch 9/300, Train Loss: 1.2446, Val Loss: 0.0079\n",
            "Epoch 10/300, Train Loss: 1.2344, Val Loss: 0.0125\n",
            "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2258, Val Loss: 0.0084\n",
            "Epoch 12/300, Train Loss: 1.2300, Val Loss: 0.0085\n",
            "Epoch 13/300, Train Loss: 1.2255, Val Loss: 0.0083\n",
            "Epoch 14/300, Train Loss: 1.2336, Val Loss: 0.0081\n",
            "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 15/300, Train Loss: 1.2121, Val Loss: 0.0097\n",
            "Epoch 16/300, Train Loss: 1.2159, Val Loss: 0.0098\n",
            "Epoch 17/300, Train Loss: 1.2291, Val Loss: 0.0079\n",
            "Epoch 18/300, Train Loss: 1.2396, Val Loss: 0.0075\n",
            "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 19/300, Train Loss: 1.9499, Val Loss: 0.0068\n",
            "Epoch 20/300, Train Loss: 1.2274, Val Loss: 0.0108\n",
            "Epoch 21/300, Train Loss: 1.9393, Val Loss: 0.0082\n",
            "Epoch 22/300, Train Loss: 1.2467, Val Loss: 0.0096\n",
            "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 23/300, Train Loss: 1.2115, Val Loss: 0.0074\n",
            "Epoch 24/300, Train Loss: 1.2264, Val Loss: 0.0081\n",
            "Epoch 25/300, Train Loss: 1.2391, Val Loss: 0.0077\n",
            "Epoch 26/300, Train Loss: 1.2393, Val Loss: 0.0073\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 27/300, Train Loss: 1.2185, Val Loss: 0.0076\n",
            "Epoch 28/300, Train Loss: 1.2039, Val Loss: 0.0079\n",
            "Early stopping triggered after 28 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2953, Val Loss: 0.0068\n",
            "Epoch 2/300, Train Loss: 1.3033, Val Loss: 0.0038\n",
            "Epoch 3/300, Train Loss: 1.2761, Val Loss: 0.0055\n",
            "Epoch 4/300, Train Loss: 1.2877, Val Loss: 0.0184\n",
            "Epoch 5/300, Train Loss: 1.2424, Val Loss: 0.0085\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2630, Val Loss: 0.0048\n",
            "Epoch 7/300, Train Loss: 1.2391, Val Loss: 0.0092\n",
            "Epoch 8/300, Train Loss: 1.2491, Val Loss: 0.0079\n",
            "Epoch 9/300, Train Loss: 1.2205, Val Loss: 0.0145\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2345, Val Loss: 0.0090\n",
            "Epoch 11/300, Train Loss: 1.2122, Val Loss: 0.0108\n",
            "Epoch 12/300, Train Loss: 1.2260, Val Loss: 0.0109\n",
            "Epoch 13/300, Train Loss: 1.9807, Val Loss: 0.0043\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 14/300, Train Loss: 1.2206, Val Loss: 0.0175\n",
            "Epoch 15/300, Train Loss: 1.2299, Val Loss: 0.0143\n",
            "Epoch 16/300, Train Loss: 1.2228, Val Loss: 0.0113\n",
            "Epoch 17/300, Train Loss: 1.2219, Val Loss: 0.0104\n",
            "Epoch 00018: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 18/300, Train Loss: 1.2307, Val Loss: 0.0108\n",
            "Epoch 19/300, Train Loss: 1.2168, Val Loss: 0.0096\n",
            "Epoch 20/300, Train Loss: 1.2205, Val Loss: 0.0120\n",
            "Epoch 21/300, Train Loss: 1.2475, Val Loss: 0.0133\n",
            "Epoch 00022: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 22/300, Train Loss: 1.2280, Val Loss: 0.0104\n",
            "Epoch 23/300, Train Loss: 1.2375, Val Loss: 0.0107\n",
            "Epoch 24/300, Train Loss: 1.2406, Val Loss: 0.0085\n",
            "Epoch 25/300, Train Loss: 1.2447, Val Loss: 0.0116\n",
            "Epoch 00026: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 26/300, Train Loss: 1.2207, Val Loss: 0.0127\n",
            "Epoch 27/300, Train Loss: 1.2282, Val Loss: 0.0135\n",
            "Early stopping triggered after 27 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2459, Val Loss: 0.0046\n",
            "Epoch 2/300, Train Loss: 1.2474, Val Loss: 0.0117\n",
            "Epoch 3/300, Train Loss: 1.2333, Val Loss: 0.0106\n",
            "Epoch 4/300, Train Loss: 1.2302, Val Loss: 0.0155\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.2048, Val Loss: 0.0280\n",
            "Epoch 6/300, Train Loss: 1.2319, Val Loss: 0.0083\n",
            "Epoch 7/300, Train Loss: 1.2310, Val Loss: 0.0170\n",
            "Epoch 8/300, Train Loss: 1.2340, Val Loss: 0.0103\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 1.2266, Val Loss: 0.0074\n",
            "Epoch 10/300, Train Loss: 1.2244, Val Loss: 0.0279\n",
            "Epoch 11/300, Train Loss: 1.2180, Val Loss: 0.0128\n",
            "Epoch 12/300, Train Loss: 1.2438, Val Loss: 0.0138\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 1.2378, Val Loss: 0.0115\n",
            "Epoch 14/300, Train Loss: 1.2211, Val Loss: 0.0118\n",
            "Epoch 15/300, Train Loss: 1.2423, Val Loss: 0.0111\n",
            "Epoch 16/300, Train Loss: 1.2501, Val Loss: 0.0102\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 1.2316, Val Loss: 0.0107\n",
            "Epoch 18/300, Train Loss: 1.9588, Val Loss: 0.0063\n",
            "Epoch 19/300, Train Loss: 1.2196, Val Loss: 0.0138\n",
            "Epoch 20/300, Train Loss: 1.2298, Val Loss: 0.0129\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 1.2220, Val Loss: 0.0081\n",
            "Epoch 22/300, Train Loss: 1.9530, Val Loss: 0.0070\n",
            "Epoch 23/300, Train Loss: 1.2357, Val Loss: 0.0124\n",
            "Epoch 24/300, Train Loss: 1.2230, Val Loss: 0.0112\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 1.2137, Val Loss: 0.0102\n",
            "Epoch 26/300, Train Loss: 1.2254, Val Loss: 0.0108\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 0.0401, Val Loss: 7.8538\n",
            "Epoch 2/300, Train Loss: 0.0263, Val Loss: 7.8442\n",
            "Epoch 3/300, Train Loss: 0.0158, Val Loss: 7.8148\n",
            "Epoch 4/300, Train Loss: 0.0151, Val Loss: 7.8163\n",
            "Epoch 5/300, Train Loss: 0.0145, Val Loss: 7.8059\n",
            "Epoch 6/300, Train Loss: 0.0093, Val Loss: 7.8000\n",
            "Epoch 7/300, Train Loss: 0.0102, Val Loss: 7.8011\n",
            "Epoch 8/300, Train Loss: 0.0112, Val Loss: 7.8110\n",
            "Epoch 9/300, Train Loss: 0.0096, Val Loss: 7.8083\n",
            "Epoch 00010: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 10/300, Train Loss: 0.0076, Val Loss: 7.8014\n",
            "Epoch 11/300, Train Loss: 0.0073, Val Loss: 7.7996\n",
            "Epoch 12/300, Train Loss: 0.0067, Val Loss: 7.8027\n",
            "Epoch 13/300, Train Loss: 0.0069, Val Loss: 7.8021\n",
            "Epoch 14/300, Train Loss: 0.0052, Val Loss: 7.7959\n",
            "Epoch 15/300, Train Loss: 0.0056, Val Loss: 7.7876\n",
            "Epoch 16/300, Train Loss: 0.0052, Val Loss: 7.7856\n",
            "Epoch 17/300, Train Loss: 0.0053, Val Loss: 7.7912\n",
            "Epoch 18/300, Train Loss: 0.0059, Val Loss: 7.7893\n",
            "Epoch 19/300, Train Loss: 0.0052, Val Loss: 7.7970\n",
            "Epoch 00020: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 20/300, Train Loss: 0.0051, Val Loss: 7.8045\n",
            "Epoch 21/300, Train Loss: 0.0053, Val Loss: 7.8002\n",
            "Epoch 22/300, Train Loss: 0.0049, Val Loss: 7.7934\n",
            "Epoch 23/300, Train Loss: 0.0046, Val Loss: 7.8000\n",
            "Epoch 00024: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 24/300, Train Loss: 0.0045, Val Loss: 7.8028\n",
            "Epoch 25/300, Train Loss: 0.0053, Val Loss: 7.8031\n",
            "Epoch 26/300, Train Loss: 0.0044, Val Loss: 7.8007\n",
            "Epoch 27/300, Train Loss: 0.0049, Val Loss: 7.8075\n",
            "Epoch 00028: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 28/300, Train Loss: 0.0041, Val Loss: 7.8047\n",
            "Epoch 29/300, Train Loss: 0.0037, Val Loss: 7.8073\n",
            "Epoch 30/300, Train Loss: 0.0048, Val Loss: 7.8088\n",
            "Epoch 31/300, Train Loss: 0.0039, Val Loss: 7.8048\n",
            "Epoch 00032: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 32/300, Train Loss: 0.0041, Val Loss: 7.8065\n",
            "Epoch 33/300, Train Loss: 0.0046, Val Loss: 7.8040\n",
            "Epoch 34/300, Train Loss: 0.0040, Val Loss: 7.8051\n",
            "Epoch 35/300, Train Loss: 0.0038, Val Loss: 7.8060\n",
            "Epoch 00036: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 36/300, Train Loss: 0.0039, Val Loss: 7.8061\n",
            "Epoch 37/300, Train Loss: 0.0042, Val Loss: 7.8039\n",
            "Epoch 38/300, Train Loss: 0.0040, Val Loss: 7.8032\n",
            "Epoch 39/300, Train Loss: 0.0038, Val Loss: 7.8053\n",
            "Epoch 00040: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 40/300, Train Loss: 0.0043, Val Loss: 7.8047\n",
            "Epoch 41/300, Train Loss: 0.0040, Val Loss: 7.8054\n",
            "Early stopping triggered after 41 epochs!\n",
            "Epoch 1/300, Train Loss: 0.0415, Val Loss: 0.0021\n",
            "Epoch 2/300, Train Loss: 0.0322, Val Loss: 0.0023\n",
            "Epoch 3/300, Train Loss: 0.0280, Val Loss: 0.0045\n",
            "Epoch 4/300, Train Loss: 0.0175, Val Loss: 0.0031\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 0.0135, Val Loss: 0.0066\n",
            "Epoch 6/300, Train Loss: 0.0164, Val Loss: 0.0025\n",
            "Epoch 7/300, Train Loss: 0.0115, Val Loss: 0.0028\n",
            "Epoch 8/300, Train Loss: 0.0090, Val Loss: 0.0038\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 0.0069, Val Loss: 0.0026\n",
            "Epoch 10/300, Train Loss: 0.0102, Val Loss: 0.0024\n",
            "Epoch 11/300, Train Loss: 0.0088, Val Loss: 0.0024\n",
            "Epoch 12/300, Train Loss: 0.0068, Val Loss: 0.0024\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 0.0071, Val Loss: 0.0024\n",
            "Epoch 14/300, Train Loss: 0.0071, Val Loss: 0.0023\n",
            "Epoch 15/300, Train Loss: 0.0080, Val Loss: 0.0024\n",
            "Epoch 16/300, Train Loss: 0.0082, Val Loss: 0.0024\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 0.0070, Val Loss: 0.0026\n",
            "Epoch 18/300, Train Loss: 0.0077, Val Loss: 0.0025\n",
            "Epoch 19/300, Train Loss: 0.0072, Val Loss: 0.0024\n",
            "Epoch 20/300, Train Loss: 0.0063, Val Loss: 0.0027\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 0.0065, Val Loss: 0.0026\n",
            "Epoch 22/300, Train Loss: 0.0080, Val Loss: 0.0025\n",
            "Epoch 23/300, Train Loss: 0.0057, Val Loss: 0.0025\n",
            "Epoch 24/300, Train Loss: 0.0062, Val Loss: 0.0024\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 0.0057, Val Loss: 0.0024\n",
            "Epoch 26/300, Train Loss: 0.0062, Val Loss: 0.0024\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 0.0410, Val Loss: 7.8049\n",
            "Epoch 2/300, Train Loss: 0.0195, Val Loss: 7.8388\n",
            "Epoch 3/300, Train Loss: 0.0200, Val Loss: 7.8645\n",
            "Epoch 4/300, Train Loss: 0.0129, Val Loss: 7.7870\n",
            "Epoch 5/300, Train Loss: 0.0115, Val Loss: 7.7686\n",
            "Epoch 6/300, Train Loss: 0.0157, Val Loss: 7.7486\n",
            "Epoch 7/300, Train Loss: 0.0107, Val Loss: 7.7569\n",
            "Epoch 8/300, Train Loss: 0.0080, Val Loss: 7.7494\n",
            "Epoch 9/300, Train Loss: 0.0056, Val Loss: 7.7713\n",
            "Epoch 00010: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 10/300, Train Loss: 0.0085, Val Loss: 7.7565\n",
            "Epoch 11/300, Train Loss: 0.0051, Val Loss: 7.7504\n",
            "Epoch 12/300, Train Loss: 0.0083, Val Loss: 7.7548\n",
            "Epoch 13/300, Train Loss: 0.0048, Val Loss: 7.7649\n",
            "Epoch 00014: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 14/300, Train Loss: 0.0046, Val Loss: 7.7707\n",
            "Epoch 15/300, Train Loss: 0.0039, Val Loss: 7.7754\n",
            "Epoch 16/300, Train Loss: 0.0067, Val Loss: 7.7647\n",
            "Epoch 17/300, Train Loss: 0.0042, Val Loss: 7.7664\n",
            "Epoch 00018: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 18/300, Train Loss: 0.0035, Val Loss: 7.7623\n",
            "Epoch 19/300, Train Loss: 0.0039, Val Loss: 7.7648\n",
            "Epoch 20/300, Train Loss: 0.0033, Val Loss: 7.7651\n",
            "Epoch 21/300, Train Loss: 0.0033, Val Loss: 7.7624\n",
            "Epoch 00022: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 22/300, Train Loss: 0.0039, Val Loss: 7.7668\n",
            "Epoch 23/300, Train Loss: 0.0036, Val Loss: 7.7591\n",
            "Epoch 24/300, Train Loss: 0.0043, Val Loss: 7.7617\n",
            "Epoch 25/300, Train Loss: 0.0034, Val Loss: 7.7604\n",
            "Epoch 00026: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 26/300, Train Loss: 0.0039, Val Loss: 7.7674\n",
            "Epoch 27/300, Train Loss: 0.0037, Val Loss: 7.7617\n",
            "Epoch 28/300, Train Loss: 0.0044, Val Loss: 7.7630\n",
            "Epoch 29/300, Train Loss: 0.0048, Val Loss: 7.7548\n",
            "Epoch 00030: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 30/300, Train Loss: 0.0034, Val Loss: 7.7535\n",
            "Epoch 31/300, Train Loss: 0.0041, Val Loss: 7.7577\n",
            "Early stopping triggered after 31 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3401, Val Loss: 0.0125\n",
            "Epoch 2/300, Train Loss: 1.2515, Val Loss: 0.0097\n",
            "Epoch 3/300, Train Loss: 1.2483, Val Loss: 0.0028\n",
            "Epoch 4/300, Train Loss: 1.2470, Val Loss: 0.0071\n",
            "Epoch 5/300, Train Loss: 1.2320, Val Loss: 0.0135\n",
            "Epoch 6/300, Train Loss: 1.2422, Val Loss: 0.0066\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2576, Val Loss: 0.0097\n",
            "Epoch 8/300, Train Loss: 1.2361, Val Loss: 0.0149\n",
            "Epoch 9/300, Train Loss: 1.2517, Val Loss: 0.0219\n",
            "Epoch 10/300, Train Loss: 1.2380, Val Loss: 0.0189\n",
            "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2347, Val Loss: 0.0165\n",
            "Epoch 12/300, Train Loss: 1.2262, Val Loss: 0.0162\n",
            "Epoch 13/300, Train Loss: 1.2429, Val Loss: 0.0172\n",
            "Epoch 14/300, Train Loss: 1.2327, Val Loss: 0.0146\n",
            "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 15/300, Train Loss: 1.2381, Val Loss: 0.0134\n",
            "Epoch 16/300, Train Loss: 1.2263, Val Loss: 0.0112\n",
            "Epoch 17/300, Train Loss: 1.2290, Val Loss: 0.0156\n",
            "Epoch 18/300, Train Loss: 1.2346, Val Loss: 0.0133\n",
            "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 19/300, Train Loss: 1.2245, Val Loss: 0.0183\n",
            "Epoch 20/300, Train Loss: 1.2308, Val Loss: 0.0141\n",
            "Epoch 21/300, Train Loss: 1.2267, Val Loss: 0.0142\n",
            "Epoch 22/300, Train Loss: 1.2062, Val Loss: 0.0168\n",
            "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 23/300, Train Loss: 1.2496, Val Loss: 0.0176\n",
            "Epoch 24/300, Train Loss: 1.2298, Val Loss: 0.0150\n",
            "Epoch 25/300, Train Loss: 1.2058, Val Loss: 0.0144\n",
            "Epoch 26/300, Train Loss: 1.2319, Val Loss: 0.0146\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 27/300, Train Loss: 1.2223, Val Loss: 0.0156\n",
            "Epoch 28/300, Train Loss: 1.2303, Val Loss: 0.0151\n",
            "Early stopping triggered after 28 epochs!\n",
            "Epoch 1/300, Train Loss: 0.0218, Val Loss: 0.0034\n",
            "Epoch 2/300, Train Loss: 0.0163, Val Loss: 0.0036\n",
            "Epoch 3/300, Train Loss: 0.0146, Val Loss: 0.0036\n",
            "Epoch 4/300, Train Loss: 0.0146, Val Loss: 0.0041\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 0.0102, Val Loss: 0.0035\n",
            "Epoch 6/300, Train Loss: 0.0097, Val Loss: 0.0036\n",
            "Epoch 7/300, Train Loss: 0.0082, Val Loss: 0.0038\n",
            "Epoch 8/300, Train Loss: 0.0111, Val Loss: 0.0034\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 0.0090, Val Loss: 0.0043\n",
            "Epoch 10/300, Train Loss: 0.0072, Val Loss: 0.0035\n",
            "Epoch 11/300, Train Loss: 0.0064, Val Loss: 0.0038\n",
            "Epoch 12/300, Train Loss: 0.0057, Val Loss: 0.0034\n",
            "Epoch 13/300, Train Loss: 0.0073, Val Loss: 0.0048\n",
            "Epoch 14/300, Train Loss: 0.0076, Val Loss: 0.0036\n",
            "Epoch 15/300, Train Loss: 0.0045, Val Loss: 0.0043\n",
            "Epoch 16/300, Train Loss: 0.0055, Val Loss: 0.0031\n",
            "Epoch 17/300, Train Loss: 0.0058, Val Loss: 0.0046\n",
            "Epoch 18/300, Train Loss: 0.0052, Val Loss: 0.0038\n",
            "Epoch 19/300, Train Loss: 0.0056, Val Loss: 0.0047\n",
            "Epoch 00020: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 20/300, Train Loss: 0.0053, Val Loss: 0.0039\n",
            "Epoch 21/300, Train Loss: 0.0047, Val Loss: 0.0033\n",
            "Epoch 22/300, Train Loss: 0.0040, Val Loss: 0.0042\n",
            "Epoch 23/300, Train Loss: 0.0053, Val Loss: 0.0041\n",
            "Epoch 00024: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 24/300, Train Loss: 0.0034, Val Loss: 0.0043\n",
            "Epoch 25/300, Train Loss: 0.0059, Val Loss: 0.0044\n",
            "Epoch 26/300, Train Loss: 0.0048, Val Loss: 0.0045\n",
            "Epoch 27/300, Train Loss: 0.0045, Val Loss: 0.0046\n",
            "Epoch 00028: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 28/300, Train Loss: 0.0043, Val Loss: 0.0048\n",
            "Epoch 29/300, Train Loss: 0.0044, Val Loss: 0.0044\n",
            "Epoch 30/300, Train Loss: 0.0041, Val Loss: 0.0043\n",
            "Epoch 31/300, Train Loss: 0.0042, Val Loss: 0.0045\n",
            "Epoch 00032: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 32/300, Train Loss: 0.0047, Val Loss: 0.0042\n",
            "Epoch 33/300, Train Loss: 0.0039, Val Loss: 0.0044\n",
            "Epoch 34/300, Train Loss: 0.0044, Val Loss: 0.0046\n",
            "Epoch 35/300, Train Loss: 0.0040, Val Loss: 0.0042\n",
            "Epoch 00036: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 36/300, Train Loss: 0.0042, Val Loss: 0.0038\n",
            "Epoch 37/300, Train Loss: 0.0038, Val Loss: 0.0038\n",
            "Epoch 38/300, Train Loss: 0.0041, Val Loss: 0.0038\n",
            "Epoch 39/300, Train Loss: 0.0048, Val Loss: 0.0043\n",
            "Epoch 00040: reducing learning rate of group 0 to 1.6777e-04.\n",
            "Epoch 40/300, Train Loss: 0.0045, Val Loss: 0.0043\n",
            "Epoch 41/300, Train Loss: 0.0044, Val Loss: 0.0042\n",
            "Early stopping triggered after 41 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3235, Val Loss: 0.0754\n",
            "Epoch 2/300, Train Loss: 1.2670, Val Loss: 0.0190\n",
            "Epoch 3/300, Train Loss: 1.2638, Val Loss: 0.0181\n",
            "Epoch 4/300, Train Loss: 1.2499, Val Loss: 0.0298\n",
            "Epoch 5/300, Train Loss: 1.2443, Val Loss: 0.0095\n",
            "Epoch 6/300, Train Loss: 1.2601, Val Loss: 0.0143\n",
            "Epoch 7/300, Train Loss: 1.2606, Val Loss: 0.0065\n",
            "Epoch 8/300, Train Loss: 1.2527, Val Loss: 0.0317\n",
            "Epoch 9/300, Train Loss: 1.9687, Val Loss: 0.0141\n",
            "Epoch 10/300, Train Loss: 1.2479, Val Loss: 0.0390\n",
            "Epoch 00011: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2343, Val Loss: 0.0200\n",
            "Epoch 12/300, Train Loss: 1.2276, Val Loss: 0.0088\n",
            "Epoch 13/300, Train Loss: 1.9746, Val Loss: 0.0175\n",
            "Epoch 14/300, Train Loss: 1.2528, Val Loss: 0.0324\n",
            "Epoch 00015: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 15/300, Train Loss: 1.9716, Val Loss: 0.0077\n",
            "Epoch 16/300, Train Loss: 1.2314, Val Loss: 0.0110\n",
            "Epoch 17/300, Train Loss: 1.2459, Val Loss: 0.0096\n",
            "Epoch 18/300, Train Loss: 1.2377, Val Loss: 0.0084\n",
            "Epoch 00019: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 19/300, Train Loss: 1.2329, Val Loss: 0.0121\n",
            "Epoch 20/300, Train Loss: 1.2224, Val Loss: 0.0150\n",
            "Epoch 21/300, Train Loss: 1.2310, Val Loss: 0.0111\n",
            "Epoch 22/300, Train Loss: 1.2265, Val Loss: 0.0086\n",
            "Epoch 23/300, Train Loss: 1.2385, Val Loss: 0.0065\n",
            "Epoch 24/300, Train Loss: 1.2325, Val Loss: 0.0060\n",
            "Epoch 25/300, Train Loss: 1.2304, Val Loss: 0.0067\n",
            "Epoch 26/300, Train Loss: 1.2282, Val Loss: 0.0094\n",
            "Epoch 27/300, Train Loss: 1.2381, Val Loss: 0.0065\n",
            "Epoch 00028: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 28/300, Train Loss: 1.2436, Val Loss: 0.0084\n",
            "Epoch 29/300, Train Loss: 1.2323, Val Loss: 0.0079\n",
            "Epoch 30/300, Train Loss: 1.2328, Val Loss: 0.0076\n",
            "Epoch 31/300, Train Loss: 1.2145, Val Loss: 0.0079\n",
            "Epoch 00032: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 32/300, Train Loss: 1.2432, Val Loss: 0.0108\n",
            "Epoch 33/300, Train Loss: 1.2261, Val Loss: 0.0084\n",
            "Epoch 34/300, Train Loss: 1.2246, Val Loss: 0.0106\n",
            "Epoch 35/300, Train Loss: 1.2297, Val Loss: 0.0109\n",
            "Epoch 00036: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 36/300, Train Loss: 1.2275, Val Loss: 0.0117\n",
            "Epoch 37/300, Train Loss: 1.2299, Val Loss: 0.0105\n",
            "Epoch 38/300, Train Loss: 1.2460, Val Loss: 0.0089\n",
            "Epoch 39/300, Train Loss: 1.2244, Val Loss: 0.0090\n",
            "Epoch 00040: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 40/300, Train Loss: 1.9626, Val Loss: 0.0075\n",
            "Epoch 41/300, Train Loss: 1.2196, Val Loss: 0.0100\n",
            "Epoch 42/300, Train Loss: 1.2246, Val Loss: 0.0104\n",
            "Epoch 43/300, Train Loss: 1.2315, Val Loss: 0.0088\n",
            "Epoch 00044: reducing learning rate of group 0 to 1.6777e-04.\n",
            "Epoch 44/300, Train Loss: 1.2306, Val Loss: 0.0101\n",
            "Epoch 45/300, Train Loss: 1.2315, Val Loss: 0.0090\n",
            "Epoch 46/300, Train Loss: 1.2331, Val Loss: 0.0095\n",
            "Epoch 47/300, Train Loss: 1.2310, Val Loss: 0.0094\n",
            "Epoch 00048: reducing learning rate of group 0 to 1.3422e-04.\n",
            "Epoch 48/300, Train Loss: 1.2099, Val Loss: 0.0093\n",
            "Epoch 49/300, Train Loss: 1.2290, Val Loss: 0.0105\n",
            "Early stopping triggered after 49 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2748, Val Loss: 0.0039\n",
            "Epoch 2/300, Train Loss: 1.2446, Val Loss: 0.0055\n",
            "Epoch 3/300, Train Loss: 1.2236, Val Loss: 0.0099\n",
            "Epoch 4/300, Train Loss: 1.2293, Val Loss: 0.0056\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.2227, Val Loss: 0.0061\n",
            "Epoch 6/300, Train Loss: 1.2234, Val Loss: 0.0059\n",
            "Epoch 7/300, Train Loss: 1.2187, Val Loss: 0.0139\n",
            "Epoch 8/300, Train Loss: 1.2378, Val Loss: 0.0060\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 1.2366, Val Loss: 0.0064\n",
            "Epoch 10/300, Train Loss: 1.2234, Val Loss: 0.0063\n",
            "Epoch 11/300, Train Loss: 1.2360, Val Loss: 0.0105\n",
            "Epoch 12/300, Train Loss: 1.2378, Val Loss: 0.0063\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 1.2468, Val Loss: 0.0079\n",
            "Epoch 14/300, Train Loss: 1.2220, Val Loss: 0.0102\n",
            "Epoch 15/300, Train Loss: 1.2272, Val Loss: 0.0091\n",
            "Epoch 16/300, Train Loss: 1.2236, Val Loss: 0.0102\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 1.2360, Val Loss: 0.0076\n",
            "Epoch 18/300, Train Loss: 1.2428, Val Loss: 0.0101\n",
            "Epoch 19/300, Train Loss: 1.2169, Val Loss: 0.0103\n",
            "Epoch 20/300, Train Loss: 1.2172, Val Loss: 0.0099\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 1.2377, Val Loss: 0.0080\n",
            "Epoch 22/300, Train Loss: 1.2352, Val Loss: 0.0088\n",
            "Epoch 23/300, Train Loss: 1.2403, Val Loss: 0.0120\n",
            "Epoch 24/300, Train Loss: 1.9419, Val Loss: 0.0081\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 1.2128, Val Loss: 0.0122\n",
            "Epoch 26/300, Train Loss: 1.2404, Val Loss: 0.0118\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 0.0504, Val Loss: 0.0107\n",
            "Epoch 2/300, Train Loss: 0.0252, Val Loss: 0.0031\n",
            "Epoch 3/300, Train Loss: 0.0323, Val Loss: 0.0039\n",
            "Epoch 4/300, Train Loss: 0.0240, Val Loss: 0.0064\n",
            "Epoch 5/300, Train Loss: 0.0184, Val Loss: 0.0041\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 0.0151, Val Loss: 0.0086\n",
            "Epoch 7/300, Train Loss: 0.0129, Val Loss: 0.0042\n",
            "Epoch 8/300, Train Loss: 0.0109, Val Loss: 0.0046\n",
            "Epoch 9/300, Train Loss: 0.0104, Val Loss: 0.0039\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 0.0136, Val Loss: 0.0034\n",
            "Epoch 11/300, Train Loss: 0.0082, Val Loss: 0.0035\n",
            "Epoch 12/300, Train Loss: 0.0083, Val Loss: 0.0030\n",
            "Epoch 13/300, Train Loss: 0.0085, Val Loss: 0.0034\n",
            "Epoch 14/300, Train Loss: 0.0113, Val Loss: 0.0037\n",
            "Epoch 15/300, Train Loss: 0.0075, Val Loss: 0.0034\n",
            "Epoch 00016: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 16/300, Train Loss: 0.0084, Val Loss: 0.0042\n",
            "Epoch 17/300, Train Loss: 0.0100, Val Loss: 0.0035\n",
            "Epoch 18/300, Train Loss: 0.0087, Val Loss: 0.0033\n",
            "Epoch 19/300, Train Loss: 0.0092, Val Loss: 0.0034\n",
            "Epoch 00020: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 20/300, Train Loss: 0.0077, Val Loss: 0.0041\n",
            "Epoch 21/300, Train Loss: 0.0067, Val Loss: 0.0036\n",
            "Epoch 22/300, Train Loss: 0.0082, Val Loss: 0.0031\n",
            "Epoch 23/300, Train Loss: 0.0054, Val Loss: 0.0032\n",
            "Epoch 00024: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 24/300, Train Loss: 0.0058, Val Loss: 0.0034\n",
            "Epoch 25/300, Train Loss: 0.0062, Val Loss: 0.0036\n",
            "Epoch 26/300, Train Loss: 0.0061, Val Loss: 0.0039\n",
            "Epoch 27/300, Train Loss: 0.0078, Val Loss: 0.0034\n",
            "Epoch 00028: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 28/300, Train Loss: 0.0065, Val Loss: 0.0039\n",
            "Epoch 29/300, Train Loss: 0.0051, Val Loss: 0.0035\n",
            "Epoch 30/300, Train Loss: 0.0054, Val Loss: 0.0031\n",
            "Epoch 31/300, Train Loss: 0.0057, Val Loss: 0.0034\n",
            "Epoch 00032: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 32/300, Train Loss: 0.0064, Val Loss: 0.0035\n",
            "Epoch 33/300, Train Loss: 0.0083, Val Loss: 0.0038\n",
            "Epoch 34/300, Train Loss: 0.0060, Val Loss: 0.0038\n",
            "Epoch 35/300, Train Loss: 0.0056, Val Loss: 0.0036\n",
            "Epoch 00036: reducing learning rate of group 0 to 1.6777e-04.\n",
            "Epoch 36/300, Train Loss: 0.0065, Val Loss: 0.0033\n",
            "Epoch 37/300, Train Loss: 0.0063, Val Loss: 0.0036\n",
            "Early stopping triggered after 37 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2583, Val Loss: 0.0042\n",
            "Epoch 2/300, Train Loss: 1.2386, Val Loss: 0.0110\n",
            "Epoch 3/300, Train Loss: 1.2367, Val Loss: 0.0063\n",
            "Epoch 4/300, Train Loss: 1.2144, Val Loss: 0.0066\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.2542, Val Loss: 0.0112\n",
            "Epoch 6/300, Train Loss: 1.2468, Val Loss: 0.0107\n",
            "Epoch 7/300, Train Loss: 1.2300, Val Loss: 0.0094\n",
            "Epoch 8/300, Train Loss: 1.2551, Val Loss: 0.0135\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 1.2264, Val Loss: 0.0160\n",
            "Epoch 10/300, Train Loss: 1.2327, Val Loss: 0.0142\n",
            "Epoch 11/300, Train Loss: 1.2232, Val Loss: 0.0150\n",
            "Epoch 12/300, Train Loss: 1.2256, Val Loss: 0.0153\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 1.2306, Val Loss: 0.0160\n",
            "Epoch 14/300, Train Loss: 1.2265, Val Loss: 0.0147\n",
            "Epoch 15/300, Train Loss: 1.2386, Val Loss: 0.0152\n",
            "Epoch 16/300, Train Loss: 1.2412, Val Loss: 0.0165\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 1.2168, Val Loss: 0.0167\n",
            "Epoch 18/300, Train Loss: 1.2276, Val Loss: 0.0168\n",
            "Epoch 19/300, Train Loss: 1.2249, Val Loss: 0.0157\n",
            "Epoch 20/300, Train Loss: 1.2378, Val Loss: 0.0165\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 1.2282, Val Loss: 0.0165\n",
            "Epoch 22/300, Train Loss: 1.2254, Val Loss: 0.0155\n",
            "Epoch 23/300, Train Loss: 1.2031, Val Loss: 0.0172\n",
            "Epoch 24/300, Train Loss: 1.2200, Val Loss: 0.0172\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 1.2223, Val Loss: 0.0179\n",
            "Epoch 26/300, Train Loss: 1.2070, Val Loss: 0.0179\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2723, Val Loss: 0.0054\n",
            "Epoch 2/300, Train Loss: 1.2393, Val Loss: 0.0064\n",
            "Epoch 3/300, Train Loss: 1.2492, Val Loss: 0.0020\n",
            "Epoch 4/300, Train Loss: 1.2193, Val Loss: 0.0025\n",
            "Epoch 5/300, Train Loss: 1.2410, Val Loss: 0.0039\n",
            "Epoch 6/300, Train Loss: 1.2314, Val Loss: 0.0023\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2323, Val Loss: 0.0031\n",
            "Epoch 8/300, Train Loss: 1.2240, Val Loss: 0.0026\n",
            "Epoch 9/300, Train Loss: 1.2597, Val Loss: 0.0026\n",
            "Epoch 10/300, Train Loss: 1.2470, Val Loss: 0.0027\n",
            "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2292, Val Loss: 0.0037\n",
            "Epoch 12/300, Train Loss: 1.2433, Val Loss: 0.0039\n",
            "Epoch 13/300, Train Loss: 1.2246, Val Loss: 0.0037\n",
            "Epoch 14/300, Train Loss: 1.2166, Val Loss: 0.0041\n",
            "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 15/300, Train Loss: 1.2175, Val Loss: 0.0047\n",
            "Epoch 16/300, Train Loss: 1.2444, Val Loss: 0.0049\n",
            "Epoch 17/300, Train Loss: 1.2132, Val Loss: 0.0038\n",
            "Epoch 18/300, Train Loss: 1.2289, Val Loss: 0.0033\n",
            "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 19/300, Train Loss: 1.2150, Val Loss: 0.0047\n",
            "Epoch 20/300, Train Loss: 1.2265, Val Loss: 0.0050\n",
            "Epoch 21/300, Train Loss: 1.2280, Val Loss: 0.0045\n",
            "Epoch 22/300, Train Loss: 1.2335, Val Loss: 0.0033\n",
            "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 23/300, Train Loss: 1.2346, Val Loss: 0.0046\n",
            "Epoch 24/300, Train Loss: 1.2297, Val Loss: 0.0061\n",
            "Epoch 25/300, Train Loss: 1.2292, Val Loss: 0.0062\n",
            "Epoch 26/300, Train Loss: 1.2374, Val Loss: 0.0051\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 27/300, Train Loss: 1.2124, Val Loss: 0.0054\n",
            "Epoch 28/300, Train Loss: 1.2311, Val Loss: 0.0046\n",
            "Early stopping triggered after 28 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2427, Val Loss: 0.0055\n",
            "Epoch 2/300, Train Loss: 1.2450, Val Loss: 0.0324\n",
            "Epoch 3/300, Train Loss: 1.2469, Val Loss: 0.0102\n",
            "Epoch 4/300, Train Loss: 1.2624, Val Loss: 0.0244\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.2495, Val Loss: 0.0241\n",
            "Epoch 6/300, Train Loss: 1.2323, Val Loss: 0.0343\n",
            "Epoch 7/300, Train Loss: 1.2398, Val Loss: 0.0197\n",
            "Epoch 8/300, Train Loss: 1.2327, Val Loss: 0.0318\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 1.2167, Val Loss: 0.0336\n",
            "Epoch 10/300, Train Loss: 1.2241, Val Loss: 0.0220\n",
            "Epoch 11/300, Train Loss: 1.2243, Val Loss: 0.0318\n",
            "Epoch 12/300, Train Loss: 1.2162, Val Loss: 0.0270\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 1.2305, Val Loss: 0.0190\n",
            "Epoch 14/300, Train Loss: 1.2500, Val Loss: 0.0252\n",
            "Epoch 15/300, Train Loss: 1.2192, Val Loss: 0.0278\n",
            "Epoch 16/300, Train Loss: 1.2318, Val Loss: 0.0231\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 1.9544, Val Loss: 0.0185\n",
            "Epoch 18/300, Train Loss: 1.2300, Val Loss: 0.0261\n",
            "Epoch 19/300, Train Loss: 1.2174, Val Loss: 0.0287\n",
            "Epoch 20/300, Train Loss: 1.2201, Val Loss: 0.0185\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 1.2116, Val Loss: 0.0218\n",
            "Epoch 22/300, Train Loss: 1.2128, Val Loss: 0.0243\n",
            "Epoch 23/300, Train Loss: 1.2218, Val Loss: 0.0270\n",
            "Epoch 24/300, Train Loss: 1.2234, Val Loss: 0.0213\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 1.2243, Val Loss: 0.0228\n",
            "Epoch 26/300, Train Loss: 1.2114, Val Loss: 0.0233\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2879, Val Loss: 0.0166\n",
            "Epoch 2/300, Train Loss: 1.2370, Val Loss: 0.0189\n",
            "Epoch 3/300, Train Loss: 1.2589, Val Loss: 0.0144\n",
            "Epoch 4/300, Train Loss: 1.2494, Val Loss: 0.0305\n",
            "Epoch 5/300, Train Loss: 1.2283, Val Loss: 0.0158\n",
            "Epoch 6/300, Train Loss: 1.2304, Val Loss: 0.0213\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2464, Val Loss: 0.0181\n",
            "Epoch 8/300, Train Loss: 1.2328, Val Loss: 0.0176\n",
            "Epoch 9/300, Train Loss: 1.1970, Val Loss: 0.0140\n",
            "Epoch 10/300, Train Loss: 1.2416, Val Loss: 0.0238\n",
            "Epoch 11/300, Train Loss: 1.2531, Val Loss: 0.0225\n",
            "Epoch 12/300, Train Loss: 1.2425, Val Loss: 0.0209\n",
            "Epoch 00013: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 13/300, Train Loss: 1.2327, Val Loss: 0.0210\n",
            "Epoch 14/300, Train Loss: 1.2210, Val Loss: 0.0232\n",
            "Epoch 15/300, Train Loss: 1.2278, Val Loss: 0.0225\n",
            "Epoch 16/300, Train Loss: 1.2168, Val Loss: 0.0208\n",
            "Epoch 00017: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 17/300, Train Loss: 1.2445, Val Loss: 0.0160\n",
            "Epoch 18/300, Train Loss: 1.2244, Val Loss: 0.0224\n",
            "Epoch 19/300, Train Loss: 1.2367, Val Loss: 0.0284\n",
            "Epoch 20/300, Train Loss: 1.2259, Val Loss: 0.0261\n",
            "Epoch 00021: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 21/300, Train Loss: 1.2261, Val Loss: 0.0256\n",
            "Epoch 22/300, Train Loss: 1.2177, Val Loss: 0.0231\n",
            "Epoch 23/300, Train Loss: 1.2315, Val Loss: 0.0257\n",
            "Epoch 24/300, Train Loss: 1.2310, Val Loss: 0.0272\n",
            "Epoch 00025: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 25/300, Train Loss: 1.2201, Val Loss: 0.0217\n",
            "Epoch 26/300, Train Loss: 1.2112, Val Loss: 0.0271\n",
            "Epoch 27/300, Train Loss: 1.2230, Val Loss: 0.0263\n",
            "Epoch 28/300, Train Loss: 1.2286, Val Loss: 0.0260\n",
            "Epoch 00029: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 29/300, Train Loss: 1.2306, Val Loss: 0.0261\n",
            "Epoch 30/300, Train Loss: 1.2025, Val Loss: 0.0260\n",
            "Epoch 31/300, Train Loss: 1.2185, Val Loss: 0.0232\n",
            "Epoch 32/300, Train Loss: 1.9437, Val Loss: 0.0237\n",
            "Epoch 00033: reducing learning rate of group 0 to 2.0972e-04.\n",
            "Epoch 33/300, Train Loss: 1.2308, Val Loss: 0.0327\n",
            "Epoch 34/300, Train Loss: 1.2246, Val Loss: 0.0325\n",
            "Early stopping triggered after 34 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3952, Val Loss: 0.0027\n",
            "Epoch 2/300, Train Loss: 1.2683, Val Loss: 0.0014\n",
            "Epoch 3/300, Train Loss: 1.2262, Val Loss: 0.0052\n",
            "Epoch 4/300, Train Loss: 1.2630, Val Loss: 0.0089\n",
            "Epoch 5/300, Train Loss: 1.2363, Val Loss: 0.0064\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2483, Val Loss: 0.0040\n",
            "Epoch 7/300, Train Loss: 1.2419, Val Loss: 0.0052\n",
            "Epoch 8/300, Train Loss: 1.2340, Val Loss: 0.0043\n",
            "Epoch 9/300, Train Loss: 1.2378, Val Loss: 0.0083\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2296, Val Loss: 0.0098\n",
            "Epoch 11/300, Train Loss: 1.2547, Val Loss: 0.0077\n",
            "Epoch 12/300, Train Loss: 1.2402, Val Loss: 0.0055\n",
            "Epoch 13/300, Train Loss: 1.2142, Val Loss: 0.0078\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 14/300, Train Loss: 1.2299, Val Loss: 0.0069\n",
            "Epoch 15/300, Train Loss: 1.2280, Val Loss: 0.0091\n",
            "Epoch 16/300, Train Loss: 1.2200, Val Loss: 0.0099\n",
            "Epoch 17/300, Train Loss: 1.2210, Val Loss: 0.0085\n",
            "Epoch 00018: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 18/300, Train Loss: 1.2300, Val Loss: 0.0092\n",
            "Epoch 19/300, Train Loss: 1.2426, Val Loss: 0.0085\n",
            "Epoch 20/300, Train Loss: 1.2365, Val Loss: 0.0090\n",
            "Epoch 21/300, Train Loss: 1.2215, Val Loss: 0.0112\n",
            "Epoch 00022: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 22/300, Train Loss: 1.2193, Val Loss: 0.0099\n",
            "Epoch 23/300, Train Loss: 1.2274, Val Loss: 0.0103\n",
            "Epoch 24/300, Train Loss: 1.2277, Val Loss: 0.0101\n",
            "Epoch 25/300, Train Loss: 1.2307, Val Loss: 0.0104\n",
            "Epoch 00026: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 26/300, Train Loss: 1.2379, Val Loss: 0.0108\n",
            "Epoch 27/300, Train Loss: 1.2272, Val Loss: 0.0106\n",
            "Early stopping triggered after 27 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2545, Val Loss: 0.0068\n",
            "Epoch 2/300, Train Loss: 1.2657, Val Loss: 0.0019\n",
            "Epoch 3/300, Train Loss: 1.2265, Val Loss: 0.0073\n",
            "Epoch 4/300, Train Loss: 1.2580, Val Loss: 0.0046\n",
            "Epoch 5/300, Train Loss: 1.2557, Val Loss: 0.0141\n",
            "Epoch 00006: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 6/300, Train Loss: 1.2480, Val Loss: 0.0075\n",
            "Epoch 7/300, Train Loss: 1.2303, Val Loss: 0.0052\n",
            "Epoch 8/300, Train Loss: 1.2302, Val Loss: 0.0079\n",
            "Epoch 9/300, Train Loss: 1.2550, Val Loss: 0.0098\n",
            "Epoch 00010: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 10/300, Train Loss: 1.2395, Val Loss: 0.0029\n",
            "Epoch 11/300, Train Loss: 1.2377, Val Loss: 0.0190\n",
            "Epoch 12/300, Train Loss: 1.2211, Val Loss: 0.0111\n",
            "Epoch 13/300, Train Loss: 1.2381, Val Loss: 0.0139\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 14/300, Train Loss: 1.2257, Val Loss: 0.0118\n",
            "Epoch 15/300, Train Loss: 1.2319, Val Loss: 0.0102\n",
            "Epoch 16/300, Train Loss: 1.2214, Val Loss: 0.0114\n",
            "Epoch 17/300, Train Loss: 1.9589, Val Loss: 0.0036\n",
            "Epoch 00018: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 18/300, Train Loss: 1.2411, Val Loss: 0.0192\n",
            "Epoch 19/300, Train Loss: 1.2324, Val Loss: 0.0122\n",
            "Epoch 20/300, Train Loss: 1.2302, Val Loss: 0.0124\n",
            "Epoch 21/300, Train Loss: 1.2349, Val Loss: 0.0084\n",
            "Epoch 00022: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 22/300, Train Loss: 1.2239, Val Loss: 0.0156\n",
            "Epoch 23/300, Train Loss: 1.2275, Val Loss: 0.0136\n",
            "Epoch 24/300, Train Loss: 1.2262, Val Loss: 0.0135\n",
            "Epoch 25/300, Train Loss: 1.2159, Val Loss: 0.0122\n",
            "Epoch 00026: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 26/300, Train Loss: 1.2276, Val Loss: 0.0117\n",
            "Epoch 27/300, Train Loss: 1.2269, Val Loss: 0.0151\n",
            "Early stopping triggered after 27 epochs!\n",
            "Epoch 1/300, Train Loss: 1.3028, Val Loss: 0.0063\n",
            "Epoch 2/300, Train Loss: 1.2529, Val Loss: 0.0108\n",
            "Epoch 3/300, Train Loss: 1.2730, Val Loss: 0.0056\n",
            "Epoch 4/300, Train Loss: 1.2685, Val Loss: 0.0071\n",
            "Epoch 5/300, Train Loss: 1.2321, Val Loss: 0.0169\n",
            "Epoch 6/300, Train Loss: 1.2521, Val Loss: 0.0086\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2256, Val Loss: 0.0152\n",
            "Epoch 8/300, Train Loss: 1.2265, Val Loss: 0.0203\n",
            "Epoch 9/300, Train Loss: 1.2416, Val Loss: 0.0139\n",
            "Epoch 10/300, Train Loss: 1.2271, Val Loss: 0.0115\n",
            "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2352, Val Loss: 0.0189\n",
            "Epoch 12/300, Train Loss: 1.2241, Val Loss: 0.0108\n",
            "Epoch 13/300, Train Loss: 1.2221, Val Loss: 0.0077\n",
            "Epoch 14/300, Train Loss: 1.2280, Val Loss: 0.0077\n",
            "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 15/300, Train Loss: 1.2336, Val Loss: 0.0136\n",
            "Epoch 16/300, Train Loss: 1.2241, Val Loss: 0.0118\n",
            "Epoch 17/300, Train Loss: 1.2438, Val Loss: 0.0130\n",
            "Epoch 18/300, Train Loss: 1.2279, Val Loss: 0.0068\n",
            "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 19/300, Train Loss: 1.2040, Val Loss: 0.0112\n",
            "Epoch 20/300, Train Loss: 1.2202, Val Loss: 0.0089\n",
            "Epoch 21/300, Train Loss: 1.2154, Val Loss: 0.0098\n",
            "Epoch 22/300, Train Loss: 1.2230, Val Loss: 0.0079\n",
            "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 23/300, Train Loss: 1.2142, Val Loss: 0.0105\n",
            "Epoch 24/300, Train Loss: 1.2077, Val Loss: 0.0089\n",
            "Epoch 25/300, Train Loss: 1.2202, Val Loss: 0.0090\n",
            "Epoch 26/300, Train Loss: 1.2354, Val Loss: 0.0087\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 27/300, Train Loss: 1.9656, Val Loss: 0.0073\n",
            "Epoch 28/300, Train Loss: 1.2077, Val Loss: 0.0100\n",
            "Early stopping triggered after 28 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2626, Val Loss: 0.0034\n",
            "Epoch 2/300, Train Loss: 1.2686, Val Loss: 0.0114\n",
            "Epoch 3/300, Train Loss: 1.2425, Val Loss: 0.0106\n",
            "Epoch 4/300, Train Loss: 1.2606, Val Loss: 0.0107\n",
            "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 5/300, Train Loss: 1.2651, Val Loss: 0.0207\n",
            "Epoch 6/300, Train Loss: 2.0013, Val Loss: 0.0101\n",
            "Epoch 7/300, Train Loss: 1.2495, Val Loss: 0.0318\n",
            "Epoch 8/300, Train Loss: 1.2337, Val Loss: 0.0249\n",
            "Epoch 00009: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 9/300, Train Loss: 1.2370, Val Loss: 0.0131\n",
            "Epoch 10/300, Train Loss: 1.2183, Val Loss: 0.0150\n",
            "Epoch 11/300, Train Loss: 1.2295, Val Loss: 0.0159\n",
            "Epoch 12/300, Train Loss: 1.2462, Val Loss: 0.0140\n",
            "Epoch 00013: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 13/300, Train Loss: 1.2286, Val Loss: 0.0145\n",
            "Epoch 14/300, Train Loss: 1.2386, Val Loss: 0.0149\n",
            "Epoch 15/300, Train Loss: 1.2205, Val Loss: 0.0103\n",
            "Epoch 16/300, Train Loss: 1.2231, Val Loss: 0.0146\n",
            "Epoch 00017: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 17/300, Train Loss: 1.2170, Val Loss: 0.0105\n",
            "Epoch 18/300, Train Loss: 1.2338, Val Loss: 0.0135\n",
            "Epoch 19/300, Train Loss: 1.2291, Val Loss: 0.0122\n",
            "Epoch 20/300, Train Loss: 1.2202, Val Loss: 0.0157\n",
            "Epoch 00021: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 21/300, Train Loss: 1.2280, Val Loss: 0.0176\n",
            "Epoch 22/300, Train Loss: 1.2252, Val Loss: 0.0150\n",
            "Epoch 23/300, Train Loss: 1.2174, Val Loss: 0.0127\n",
            "Epoch 24/300, Train Loss: 1.2439, Val Loss: 0.0204\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 25/300, Train Loss: 1.2276, Val Loss: 0.0210\n",
            "Epoch 26/300, Train Loss: 1.2268, Val Loss: 0.0219\n",
            "Early stopping triggered after 26 epochs!\n",
            "Epoch 1/300, Train Loss: 1.2829, Val Loss: 0.0132\n",
            "Epoch 2/300, Train Loss: 1.2577, Val Loss: 0.0063\n",
            "Epoch 3/300, Train Loss: 1.2582, Val Loss: 0.0027\n",
            "Epoch 4/300, Train Loss: 1.2511, Val Loss: 0.0064\n",
            "Epoch 5/300, Train Loss: 1.2463, Val Loss: 0.0119\n",
            "Epoch 6/300, Train Loss: 1.2514, Val Loss: 0.0146\n",
            "Epoch 00007: reducing learning rate of group 0 to 8.0000e-04.\n",
            "Epoch 7/300, Train Loss: 1.2527, Val Loss: 0.0136\n",
            "Epoch 8/300, Train Loss: 1.2231, Val Loss: 0.0105\n",
            "Epoch 9/300, Train Loss: 1.2149, Val Loss: 0.0066\n",
            "Epoch 10/300, Train Loss: 1.2324, Val Loss: 0.0100\n",
            "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
            "Epoch 11/300, Train Loss: 1.2199, Val Loss: 0.0086\n",
            "Epoch 12/300, Train Loss: 1.2245, Val Loss: 0.0114\n",
            "Epoch 13/300, Train Loss: 1.2233, Val Loss: 0.0079\n",
            "Epoch 14/300, Train Loss: 1.2288, Val Loss: 0.0113\n",
            "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
            "Epoch 15/300, Train Loss: 1.2252, Val Loss: 0.0103\n",
            "Epoch 16/300, Train Loss: 1.2296, Val Loss: 0.0107\n",
            "Epoch 17/300, Train Loss: 1.2297, Val Loss: 0.0100\n",
            "Epoch 18/300, Train Loss: 1.2125, Val Loss: 0.0108\n",
            "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
            "Epoch 19/300, Train Loss: 1.2213, Val Loss: 0.0127\n",
            "Epoch 20/300, Train Loss: 1.2276, Val Loss: 0.0122\n",
            "Epoch 21/300, Train Loss: 1.2215, Val Loss: 0.0113\n",
            "Epoch 22/300, Train Loss: 1.2231, Val Loss: 0.0101\n",
            "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
            "Epoch 23/300, Train Loss: 1.2288, Val Loss: 0.0100\n",
            "Epoch 24/300, Train Loss: 1.9621, Val Loss: 0.0086\n",
            "Epoch 25/300, Train Loss: 1.2413, Val Loss: 0.0109\n",
            "Epoch 26/300, Train Loss: 1.2160, Val Loss: 0.0107\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
            "Epoch 27/300, Train Loss: 1.2205, Val Loss: 0.0085\n",
            "Epoch 28/300, Train Loss: 1.2303, Val Loss: 0.0098\n",
            "Early stopping triggered after 28 epochs!\n",
            "Metrics saved to model_metrics.csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "num_models = 50\n",
        "val_metrics = {'mae': [], 'mse': [], 'r2': []}\n",
        "test_metrics = {'mae': [], 'mse': [], 'r2': []}\n",
        "\n",
        "for i in range(num_models):\n",
        "    # Split the dataset differently each time\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(pyg_graphs, labels_scaled, train_size=0.8, random_state=i)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5, random_state=i)\n",
        "\n",
        "    # Create DataLoaders for each split\n",
        "    train_loader = DataLoader(X_train, batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(X_val, batch_size=8, shuffle=False)\n",
        "    test_loader = DataLoader(X_test, batch_size=8, shuffle=False)\n",
        "\n",
        "    # Instantiate and train the model\n",
        "    model = GCN(num_node_features)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_func = torch.nn.MSELoss()\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)\n",
        "\n",
        "    trained_model = train(model, train_loader, val_loader, optimizer, loss_func, scheduler, epochs, early_stopping_patience)\n",
        "\n",
        "    # Evaluate the model\n",
        "    val_mae, val_mse, val_r2 = evaluate_model(trained_model, val_loader)\n",
        "    test_mae, test_mse, test_r2 = evaluate_model(trained_model, test_loader)\n",
        "\n",
        "    # Save the metrics\n",
        "    val_metrics['mae'].append(val_mae)\n",
        "    val_metrics['mse'].append(val_mse)\n",
        "    val_metrics['r2'].append(val_r2)\n",
        "\n",
        "    test_metrics['mae'].append(test_mae)\n",
        "    test_metrics['mse'].append(test_mse)\n",
        "    test_metrics['r2'].append(test_r2)\n",
        "\n",
        "# Calculate average of the metrics\n",
        "avg_val_metrics = {metric: np.mean(values) for metric, values in val_metrics.items()}\n",
        "avg_test_metrics = {metric: np.mean(values) for metric, values in test_metrics.items()}\n",
        "\n",
        "# Prepare data for DataFrame\n",
        "data = {\n",
        "    'MAE Validation': [avg_val_metrics['mae']],\n",
        "    'MSE Validation': [avg_val_metrics['mse']],\n",
        "    'R² Validation': [avg_val_metrics['r2']],\n",
        "    'MAE Test': [avg_test_metrics['mae']],\n",
        "    'MSE Test': [avg_test_metrics['mse']],\n",
        "    'R² Test': [avg_test_metrics['r2']]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file in the current directory\n",
        "csv_file_path = 'model_metrics.csv'\n",
        "results_df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Print out the path to the CSV file\n",
        "print(f'Metrics saved to {csv_file_path}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "85D_BWkemnuE"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0Qt5DrpQsxsk"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import random\n",
        "# import torch\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import rdchem, rdmolops\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from torch_geometric.data import Data\n",
        "# from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "# from torch_geometric.loader import DataLoader\n",
        "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# import torch.nn.functional as F\n",
        "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# # Define your atom_features, bond_features, smiles_to_graph functions here\n",
        "# def atom_features(atom):\n",
        "#     \"\"\"Create an atom feature vector.\"\"\"\n",
        "#     return [\n",
        "#         atom.GetAtomicNum(),\n",
        "#         atom.GetDegree(),\n",
        "#         atom.GetFormalCharge(),\n",
        "#         atom.GetHybridization().real,\n",
        "#         atom.GetIsAromatic(),\n",
        "#         # Add more atom-level features here.\n",
        "#     ]\n",
        "\n",
        "# def bond_features(bond):\n",
        "#     \"\"\"Create a bond feature vector.\"\"\"\n",
        "#     bt = bond.GetBondType()\n",
        "#     return [\n",
        "#         bt == rdchem.BondType.SINGLE,\n",
        "#         bt == rdchem.BondType.DOUBLE,\n",
        "#         bt == rdchem.BondType.TRIPLE,\n",
        "#         bt == rdchem.BondType.AROMATIC,\n",
        "#         bond.GetIsConjugated(),\n",
        "#         bond.IsInRing(),\n",
        "#         # Add more bond-level features here.\n",
        "#     ]\n",
        "\n",
        "# def smiles_to_graph(smiles):\n",
        "#     \"\"\"\n",
        "#     Convert a SMILES string to a graph with nodes as atoms and edges as bonds,\n",
        "#     including additional features for atoms and bonds.\n",
        "#     \"\"\"\n",
        "#     mol = Chem.MolFromSmiles(smiles)\n",
        "#     mol = Chem.AddHs(mol)  # Add hydrogens to the molecule.\n",
        "\n",
        "#     # Get atom features\n",
        "#     atom_features_list = [atom_features(atom) for atom in mol.GetAtoms()]\n",
        "\n",
        "#     # Get bond features (as a matrix)\n",
        "#     num_atoms = mol.GetNumAtoms()\n",
        "#     bond_features_matrix = np.zeros((num_atoms, num_atoms, len(bond_features(mol.GetBonds()[0]))))\n",
        "\n",
        "#     for bond in mol.GetBonds():\n",
        "#         idx = bond.GetBeginAtomIdx()\n",
        "#         jdx = bond.GetEndAtomIdx()\n",
        "#         bond_feat = bond_features(bond)\n",
        "\n",
        "#         bond_features_matrix[idx, jdx, :] = bond_feat\n",
        "#         bond_features_matrix[jdx, idx, :] = bond_feat  # Bond is undirected\n",
        "\n",
        "#     # Create adjacency matrix where entries are 1 for bonded atom pairs\n",
        "#     adjacency_matrix = rdmolops.GetAdjacencyMatrix(mol)\n",
        "\n",
        "#     graph = {\n",
        "#         \"atom_features\": atom_features_list,\n",
        "#         \"bond_features\": bond_features_matrix,\n",
        "#         \"adjacency_matrix\": adjacency_matrix\n",
        "#         # Add more graph-level features if needed\n",
        "#     }\n",
        "\n",
        "#     return graph\n",
        "\n",
        "# def read_and_process_dataset(file_path, target_columns):\n",
        "#     df = pd.read_csv(file_path)\n",
        "\n",
        "#     smiles_column = 'Smiles'\n",
        "#     graphs = []\n",
        "#     labels = []\n",
        "\n",
        "#     for _, row in df.iterrows():\n",
        "#         graph = smiles_to_graph(row[smiles_column])\n",
        "#         labels.append([row[col] for col in target_columns])\n",
        "#         graphs.append(graph)\n",
        "\n",
        "#     return graphs, np.array(labels)\n",
        "\n",
        "# # GCN Model Definition\n",
        "# class GCN(torch.nn.Module):\n",
        "#     def __init__(self, num_node_features):\n",
        "#         super(GCN, self).__init__()\n",
        "#         self.conv1 = GCNConv(num_node_features, 64)\n",
        "#         self.conv2 = GCNConv(64, 32)\n",
        "#         self.fc = torch.nn.Linear(32, 1)  # Output one value per graph\n",
        "\n",
        "#     def forward(self, data):\n",
        "#         x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "#         x = self.conv1(x, edge_index)\n",
        "#         x = torch.relu(x)\n",
        "#         x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "#         x = self.conv2(x, edge_index)\n",
        "#         x = torch.relu(x)\n",
        "#         x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "#         x = global_mean_pool(x, batch)  # Pool node features to get graph-level features\n",
        "#         x = self.fc(x)  # Output one value per graph\n",
        "\n",
        "#         return x\n",
        "\n",
        "# # Training Function\n",
        "# def train(model, train_loader, val_loader, optimizer, criterion, scheduler, epochs, early_stopping_patience):\n",
        "#     best_val_loss = float('inf')\n",
        "#     epochs_no_improve = 0\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         total_loss = 0\n",
        "#         for data in train_loader:\n",
        "#             optimizer.zero_grad()\n",
        "#             output = model(data)\n",
        "#             loss = criterion(output, data.y)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#         avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "#         # Validation step\n",
        "#         model.eval()\n",
        "#         total_val_loss = 0\n",
        "#         with torch.no_grad():\n",
        "#             for data in val_loader:\n",
        "#                 output = model(data)\n",
        "#                 loss = criterion(output, data.y)\n",
        "#                 total_val_loss += loss.item()\n",
        "\n",
        "#         avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "#         # Scheduler step (for learning rate decay)\n",
        "#         scheduler.step(avg_val_loss)\n",
        "\n",
        "#         print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "#         # Early stopping check\n",
        "#         if avg_val_loss < best_val_loss:\n",
        "#             best_val_loss = avg_val_loss\n",
        "#             epochs_no_improve = 0\n",
        "#         else:\n",
        "#             epochs_no_improve += 1\n",
        "#             if epochs_no_improve == early_stopping_patience:\n",
        "#                 print(f'Early stopping triggered after {epoch+1} epochs!')\n",
        "#                 break\n",
        "\n",
        "#     return model\n",
        "\n",
        "# # Evaluation Function\n",
        "# def evaluate_model(model, data_loader):\n",
        "#     model.eval()\n",
        "#     predictions, actuals = [], []\n",
        "#     with torch.no_grad():\n",
        "#         for data in data_loader:\n",
        "#             output = model(data)\n",
        "#             predictions.append(output.numpy())\n",
        "#             actuals.append(data.y.numpy())\n",
        "\n",
        "#     predictions = np.vstack(predictions)\n",
        "#     actuals = np.vstack(actuals)\n",
        "\n",
        "#     mse = mean_squared_error(actuals, predictions)\n",
        "#     mae = mean_absolute_error(actuals, predictions)\n",
        "#     r2 = r2_score(actuals, predictions)\n",
        "\n",
        "#     return mae, mse, r2\n",
        "\n",
        "# # Ensemble method across all targets\n",
        "# file_path = '/content/LCIA_DATASET_CLEANED_V3.csv'\n",
        "# target_columns = [\"GWP\", \"HTP\", \"MDP\", \"FETP\", \"PMFP\", \"TAP\"]  # All target variables\n",
        "# num_models = 100\n",
        "# all_results = pd.DataFrame()\n",
        "\n",
        "# for target in target_columns:\n",
        "#     print(f\"Processing target: {target}\")\n",
        "\n",
        "#     # Read and process dataset\n",
        "#     graphs, labels = read_and_process_dataset(file_path, [target])\n",
        "#     scaler = StandardScaler()\n",
        "#     labels_scaled = scaler.fit_transform(labels)\n",
        "\n",
        "#     # Convert graphs to PyG data objects\n",
        "#     pyg_graphs = [\n",
        "#         Data(\n",
        "#             x=torch.tensor(graph['atom_features'], dtype=torch.float),\n",
        "#             edge_index=torch.tensor(np.nonzero(graph['adjacency_matrix'])).type(torch.long),\n",
        "#             y=torch.tensor([labels_scaled[i]], dtype=torch.float)\n",
        "#         )\n",
        "#         for i, graph in enumerate(graphs)\n",
        "#     ]\n",
        "\n",
        "#     val_metrics = {'mae': [], 'mse': [], 'r2': []}\n",
        "#     test_metrics = {'mae': [], 'mse': [], 'r2': []}\n",
        "\n",
        "#     for i in range(num_models):\n",
        "#         # Split and create DataLoaders\n",
        "#         X_train, X_temp, y_train, y_temp = train_test_split(pyg_graphs, labels_scaled, train_size=0.8, random_state=i)\n",
        "#         X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5, random_state=i)\n",
        "#         train_loader = DataLoader(X_train, batch_size=8, shuffle=True)\n",
        "#         val_loader = DataLoader(X_val, batch_size=8, shuffle=False)\n",
        "#         test_loader = DataLoader(X_test, batch_size=8, shuffle=False)\n",
        "\n",
        "#         # Train the model\n",
        "#         model = GCN(num_node_features)\n",
        "#         optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#         loss_func = torch.nn.MSELoss()\n",
        "#         scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)\n",
        "#         epochs = 300\n",
        "#         early_stopping_patience = 25\n",
        "#         trained_model = train(model, train_loader, val_loader, optimizer, loss_func, scheduler, epochs, early_stopping_patience)\n",
        "\n",
        "#         # Evaluate the model\n",
        "#         val_mae, val_mse, val_r2 = evaluate_model(trained_model, val_loader)\n",
        "#         test_mae, test_mse, test_r2 = evaluate_model(trained_model, test_loader)\n",
        "\n",
        "#         # Save the metrics\n",
        "#         val_metrics['mae'].append(val_mae)\n",
        "#         val_metrics['mse'].append(val_mse)\n",
        "#         val_metrics['r2'].append(val_r2)\n",
        "#         test_metrics['mae'].append(test_mae)\n",
        "#         test_metrics['mse'].append(test_mse)\n",
        "#         test_metrics['r2'].append(test_r2)\n",
        "\n",
        "#     # Calculate and save the average metrics\n",
        "#     avg_val_metrics = {metric: np.mean(values) for metric, values in val_metrics.items()}\n",
        "#     avg_test_metrics = {metric: np.mean(values) for metric, values in test_metrics.items()}\n",
        "\n",
        "#     result = {\n",
        "#         'Metric': target,\n",
        "#         'Validation MAE': avg_val_metrics['mae'],\n",
        "#         'Validation MSE': avg_val_metrics['mse'],\n",
        "#         'Validation R2': avg_val_metrics['r2'],\n",
        "#         'Test MAE': avg_test_metrics['mae'],\n",
        "#         'Test MSE': avg_test_metrics['mse'],\n",
        "#         'Test R2': avg_test_metrics['r2']\n",
        "#     }\n",
        "#     all_results = all_results.append(result, ignore_index=True)\n",
        "\n",
        "# # Save the results to a CSV file\n",
        "# csv_file_path = '/content/all_metrics_gcn_ensemble_results.csv'\n",
        "# all_results.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# print(f'Results for all metrics have been saved to {csv_file_path}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}